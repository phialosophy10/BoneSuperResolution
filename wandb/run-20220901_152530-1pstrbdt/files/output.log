
wandb: WARNING Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.
/work3/soeba/HALOS/halos/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.
  warnings.warn(
/work3/soeba/HALOS/halos/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Training Epoch 0
Trained batch 0 in epoch 0, gen_loss = 0.5102766156196594, disc_loss = 0.765160858631134
Trained batch 1 in epoch 0, gen_loss = 0.533453106880188, disc_loss = 0.8339563012123108
Trained batch 2 in epoch 0, gen_loss = 0.5445717374483744, disc_loss = 0.7312368353207906
Trained batch 3 in epoch 0, gen_loss = 0.5268607512116432, disc_loss = 0.6436694040894508
Trained batch 4 in epoch 0, gen_loss = 0.5196315467357635, disc_loss = 0.5832755327224731
Trained batch 5 in epoch 0, gen_loss = 0.49493056039015454, disc_loss = 0.5265055571993192
Trained batch 6 in epoch 0, gen_loss = 0.4851835455213274, disc_loss = 0.4776242481810706
Trained batch 7 in epoch 0, gen_loss = 0.48859653621912, disc_loss = 0.4405597560107708
Trained batch 8 in epoch 0, gen_loss = 0.47874805993503994, disc_loss = 0.4078402452998691
Trained batch 9 in epoch 0, gen_loss = 0.46776839196681974, disc_loss = 0.38527353703975675
Trained batch 10 in epoch 0, gen_loss = 0.46698950095610187, disc_loss = 0.36920804462649603
Trained batch 11 in epoch 0, gen_loss = 0.46597027281920117, disc_loss = 0.34990446269512177
Trained batch 12 in epoch 0, gen_loss = 0.4688623318305382, disc_loss = 0.3324904843018605
Trained batch 13 in epoch 0, gen_loss = 0.4682834872177669, disc_loss = 0.3165588474699429
Trained batch 14 in epoch 0, gen_loss = 0.46640793681144715, disc_loss = 0.30398242076237997
Trained batch 15 in epoch 0, gen_loss = 0.4685352686792612, disc_loss = 0.29368406254798174
Trained batch 16 in epoch 0, gen_loss = 0.46561463966089134, disc_loss = 0.28140205933767204
Trained batch 17 in epoch 0, gen_loss = 0.46475684808360207, disc_loss = 0.2695586739314927
Trained batch 18 in epoch 0, gen_loss = 0.46568969519514786, disc_loss = 0.25848382397701863
Trained batch 19 in epoch 0, gen_loss = 0.46563395112752914, disc_loss = 0.24962817057967185
Trained batch 20 in epoch 0, gen_loss = 0.46680722208250136, disc_loss = 0.2415352783032826
Trained batch 21 in epoch 0, gen_loss = 0.4659816785292192, disc_loss = 0.23398749394850296
Trained batch 22 in epoch 0, gen_loss = 0.46380660585735156, disc_loss = 0.22879272861325223
Trained batch 23 in epoch 0, gen_loss = 0.4632095918059349, disc_loss = 0.22689524696518978
Trained batch 24 in epoch 0, gen_loss = 0.46476571559906005, disc_loss = 0.22482388526201247
Trained batch 25 in epoch 0, gen_loss = 0.46872923924372745, disc_loss = 0.2288796308522041
Trained batch 26 in epoch 0, gen_loss = 0.46921644056284867, disc_loss = 0.23547492452241756
Trained batch 27 in epoch 0, gen_loss = 0.4693496823310852, disc_loss = 0.23625243056033338
Trained batch 28 in epoch 0, gen_loss = 0.46944781212971126, disc_loss = 0.23738020786951328
Trained batch 29 in epoch 0, gen_loss = 0.46864817241827644, disc_loss = 0.2346448374291261
Trained batch 30 in epoch 0, gen_loss = 0.4671022382474715, disc_loss = 0.2325734219724132
Trained batch 31 in epoch 0, gen_loss = 0.46722214203327894, disc_loss = 0.23004452674649656
Trained batch 32 in epoch 0, gen_loss = 0.4660432356776613, disc_loss = 0.2257121575601173
Trained batch 33 in epoch 0, gen_loss = 0.4667162071256077, disc_loss = 0.22177087143063545
Trained batch 34 in epoch 0, gen_loss = 0.46606729541506087, disc_loss = 0.21895934960671834
Trained batch 35 in epoch 0, gen_loss = 0.46477466573317844, disc_loss = 0.2188280135807064
Trained batch 36 in epoch 0, gen_loss = 0.46742725291767634, disc_loss = 0.2178286062301816
Trained batch 37 in epoch 0, gen_loss = 0.4691354044173893, disc_loss = 0.21405990049242973
Trained batch 38 in epoch 0, gen_loss = 0.46717974161490416, disc_loss = 0.21117517829705507
Trained batch 39 in epoch 0, gen_loss = 0.4674009270966053, disc_loss = 0.20886328499764203
Trained batch 40 in epoch 0, gen_loss = 0.46895186857479376, disc_loss = 0.20786110820566736
Trained batch 41 in epoch 0, gen_loss = 0.46805427755628315, disc_loss = 0.20522567931385266
Trained batch 42 in epoch 0, gen_loss = 0.46851435580918954, disc_loss = 0.20384262762097424
Trained batch 43 in epoch 0, gen_loss = 0.4690441963347522, disc_loss = 0.20235703225162896
Trained batch 44 in epoch 0, gen_loss = 0.4704186002413432, disc_loss = 0.20128930624988345
Trained batch 45 in epoch 0, gen_loss = 0.4713530799616938, disc_loss = 0.2028129763253357
Trained batch 46 in epoch 0, gen_loss = 0.47191139041109287, disc_loss = 0.2079493005542045
Trained batch 47 in epoch 0, gen_loss = 0.474561320617795, disc_loss = 0.20573919902866086
Trained batch 48 in epoch 0, gen_loss = 0.47614268441589513, disc_loss = 0.20588203458761684
Trained batch 49 in epoch 0, gen_loss = 0.474373117685318, disc_loss = 0.20481396958231926
Trained batch 50 in epoch 0, gen_loss = 0.4739199818349352, disc_loss = 0.20249863451018052
Trained batch 51 in epoch 0, gen_loss = 0.4744225087074133, disc_loss = 0.20009698159992695
Trained batch 52 in epoch 0, gen_loss = 0.47379126863659554, disc_loss = 0.19806100667085288
Trained batch 53 in epoch 0, gen_loss = 0.4732495651200966, disc_loss = 0.19765026356886933
Trained batch 54 in epoch 0, gen_loss = 0.47462835040959445, disc_loss = 0.1968607706102458
Trained batch 55 in epoch 0, gen_loss = 0.4732679879026754, disc_loss = 0.1980933966115117
Trained batch 56 in epoch 0, gen_loss = 0.4738851769974357, disc_loss = 0.20773185279808545
Trained batch 57 in epoch 0, gen_loss = 0.474044897432985, disc_loss = 0.20854065066267705
Trained batch 58 in epoch 0, gen_loss = 0.4744896171456676, disc_loss = 0.2085177424853131
Trained batch 59 in epoch 0, gen_loss = 0.47529437839984895, disc_loss = 0.20885065756738186
Trained batch 60 in epoch 0, gen_loss = 0.4751035009251266, disc_loss = 0.20843387492856041
Trained batch 61 in epoch 0, gen_loss = 0.47473965344890473, disc_loss = 0.20793539489949903
Trained batch 62 in epoch 0, gen_loss = 0.47401627614384606, disc_loss = 0.20681472583895638
Trained batch 63 in epoch 0, gen_loss = 0.4745222101919353, disc_loss = 0.205329789663665
Trained batch 64 in epoch 0, gen_loss = 0.47465703991743236, disc_loss = 0.20395553627839455
Trained batch 65 in epoch 0, gen_loss = 0.4760032336820256, disc_loss = 0.20274446419242656
Trained batch 66 in epoch 0, gen_loss = 0.47664078981129093, disc_loss = 0.20202106115088533
Trained batch 67 in epoch 0, gen_loss = 0.47645751912804213, disc_loss = 0.2017703386133208
Trained batch 68 in epoch 0, gen_loss = 0.4768559945666272, disc_loss = 0.2022565768464752
Trained batch 69 in epoch 0, gen_loss = 0.47699030935764314, disc_loss = 0.20619530134967395
Trained batch 70 in epoch 0, gen_loss = 0.4761672767115311, disc_loss = 0.20569251446237027
Trained batch 71 in epoch 0, gen_loss = 0.47581644480427104, disc_loss = 0.20678174816485909
Trained batch 72 in epoch 0, gen_loss = 0.4753588658489593, disc_loss = 0.20663297064092062
Trained batch 73 in epoch 0, gen_loss = 0.4754822769680539, disc_loss = 0.20599452416236336
Trained batch 74 in epoch 0, gen_loss = 0.4746432828903198, disc_loss = 0.20505717506011328
Trained batch 75 in epoch 0, gen_loss = 0.47408878215049444, disc_loss = 0.20372437283788858
Trained batch 76 in epoch 0, gen_loss = 0.4732056097550826, disc_loss = 0.2023154337491308
Trained batch 77 in epoch 0, gen_loss = 0.47186113550112796, disc_loss = 0.20137535580075705
Trained batch 78 in epoch 0, gen_loss = 0.4705859328372569, disc_loss = 0.20160646293359466
Trained batch 79 in epoch 0, gen_loss = 0.47096774391829965, disc_loss = 0.20222528567537665
Trained batch 80 in epoch 0, gen_loss = 0.47131956617037457, disc_loss = 0.20119361653004164
Trained batch 81 in epoch 0, gen_loss = 0.47078274335803055, disc_loss = 0.2004750281935785
Trained batch 82 in epoch 0, gen_loss = 0.470722803509379, disc_loss = 0.20083529134112668
Trained batch 83 in epoch 0, gen_loss = 0.4708168446308091, disc_loss = 0.20255540045244352
Trained batch 84 in epoch 0, gen_loss = 0.47001131001640767, disc_loss = 0.20371360024985144
Trained batch 85 in epoch 0, gen_loss = 0.46876355139322057, disc_loss = 0.20545496306447095
Trained batch 86 in epoch 0, gen_loss = 0.467974081121642, disc_loss = 0.20601608578501077
Trained batch 87 in epoch 0, gen_loss = 0.4679970636286519, disc_loss = 0.20610725761137225
Trained batch 88 in epoch 0, gen_loss = 0.46718878257140684, disc_loss = 0.20680974926171677
Trained batch 89 in epoch 0, gen_loss = 0.46683933635552727, disc_loss = 0.20683254649241764
Trained batch 90 in epoch 0, gen_loss = 0.4670189347240951, disc_loss = 0.20706281802811466
Trained batch 91 in epoch 0, gen_loss = 0.4663538537595583, disc_loss = 0.20682471618056297
Trained batch 92 in epoch 0, gen_loss = 0.4668426391898945, disc_loss = 0.20659366121856115
Trained batch 93 in epoch 0, gen_loss = 0.4664906386999374, disc_loss = 0.20660861534007052
Trained batch 94 in epoch 0, gen_loss = 0.4657675078040675, disc_loss = 0.20744545930310299
Trained batch 95 in epoch 0, gen_loss = 0.4660394126549363, disc_loss = 0.2079211495195826
Trained batch 96 in epoch 0, gen_loss = 0.46574327503283, disc_loss = 0.2082304450654492
Trained batch 97 in epoch 0, gen_loss = 0.46603644076658757, disc_loss = 0.2092984504237467
Trained batch 98 in epoch 0, gen_loss = 0.46682908559086345, disc_loss = 0.20947911432295135
Trained batch 99 in epoch 0, gen_loss = 0.4663896504044533, disc_loss = 0.20997240483760835
Trained batch 100 in epoch 0, gen_loss = 0.46648771367450753, disc_loss = 0.21065860691637095
Trained batch 101 in epoch 0, gen_loss = 0.4663444579231973, disc_loss = 0.21068998689160628
Trained batch 102 in epoch 0, gen_loss = 0.4667815719414683, disc_loss = 0.21092233018389026
Trained batch 103 in epoch 0, gen_loss = 0.46573274296063644, disc_loss = 0.21084675866250807
Trained batch 104 in epoch 0, gen_loss = 0.46550961818013875, disc_loss = 0.21073981452555884
Trained batch 105 in epoch 0, gen_loss = 0.46627084967100396, disc_loss = 0.21034770262128902
Trained batch 106 in epoch 0, gen_loss = 0.4662422696563685, disc_loss = 0.21023782405340782
Trained batch 107 in epoch 0, gen_loss = 0.4664232454918049, disc_loss = 0.21035027186627742
Trained batch 108 in epoch 0, gen_loss = 0.4664523267964704, disc_loss = 0.2109439609521026
Trained batch 109 in epoch 0, gen_loss = 0.466110065037554, disc_loss = 0.21186777312647212
Trained batch 110 in epoch 0, gen_loss = 0.46612429216101364, disc_loss = 0.21232392216050947
Trained batch 111 in epoch 0, gen_loss = 0.4662744626402855, disc_loss = 0.21262057378355945
Trained batch 112 in epoch 0, gen_loss = 0.46579986942552887, disc_loss = 0.21269997782939304
Trained batch 113 in epoch 0, gen_loss = 0.465392582510647, disc_loss = 0.21286160877922125
Trained batch 114 in epoch 0, gen_loss = 0.46500363583150117, disc_loss = 0.21289158100667208
Trained batch 115 in epoch 0, gen_loss = 0.46544440270497883, disc_loss = 0.21272323614564434
Trained batch 116 in epoch 0, gen_loss = 0.46531808707449174, disc_loss = 0.2125937174528073
Trained batch 117 in epoch 0, gen_loss = 0.46473620604660554, disc_loss = 0.2127575629343421
Trained batch 118 in epoch 0, gen_loss = 0.4643760095624363, disc_loss = 0.21316785672131708
Trained batch 119 in epoch 0, gen_loss = 0.46481533323725066, disc_loss = 0.2133797506491343
Trained batch 120 in epoch 0, gen_loss = 0.46480704068152373, disc_loss = 0.2136446664894908
Trained batch 121 in epoch 0, gen_loss = 0.46401110926612477, disc_loss = 0.21383116333211055
Trained batch 122 in epoch 0, gen_loss = 0.46443880282766453, disc_loss = 0.21551600704348184
Trained batch 123 in epoch 0, gen_loss = 0.4645100649326078, disc_loss = 0.21669108781122393
Trained batch 124 in epoch 0, gen_loss = 0.46441996908187866, disc_loss = 0.21692050397396087
Trained batch 125 in epoch 0, gen_loss = 0.4636501752667957, disc_loss = 0.21706335802399923
Trained batch 126 in epoch 0, gen_loss = 0.4627783195240291, disc_loss = 0.21685652751622236
Trained batch 127 in epoch 0, gen_loss = 0.46266856463626027, disc_loss = 0.21670389943756163
Trained batch 128 in epoch 0, gen_loss = 0.46251201421715493, disc_loss = 0.2162651052308637
Trained batch 129 in epoch 0, gen_loss = 0.4628050678051435, disc_loss = 0.21593857132471525
Trained batch 130 in epoch 0, gen_loss = 0.46287519922693265, disc_loss = 0.2157213453573125
Trained batch 131 in epoch 0, gen_loss = 0.4628751968795603, disc_loss = 0.21560779518701814
Trained batch 132 in epoch 0, gen_loss = 0.4629788595930974, disc_loss = 0.21571780640379826
Trained batch 133 in epoch 0, gen_loss = 0.4625532140037907, disc_loss = 0.2158806473016739
Trained batch 134 in epoch 0, gen_loss = 0.461861303779814, disc_loss = 0.21558673900586586
Trained batch 135 in epoch 0, gen_loss = 0.46229152490987496, disc_loss = 0.21564853520077817
Trained batch 136 in epoch 0, gen_loss = 0.46230938095245916, disc_loss = 0.21579457862968862
Trained batch 137 in epoch 0, gen_loss = 0.46174267128757807, disc_loss = 0.21558869640896286
Trained batch 138 in epoch 0, gen_loss = 0.4615459525756699, disc_loss = 0.2153738677072868
Trained batch 139 in epoch 0, gen_loss = 0.4611177806343351, disc_loss = 0.21528292926294462
Trained batch 140 in epoch 0, gen_loss = 0.4609626865556054, disc_loss = 0.21564562935778436
Trained batch 141 in epoch 0, gen_loss = 0.4610531579860499, disc_loss = 0.21581090734878056
Trained batch 142 in epoch 0, gen_loss = 0.4610812778656299, disc_loss = 0.21616644459170894
Trained batch 143 in epoch 0, gen_loss = 0.4605864507870542, disc_loss = 0.21627671333650747
Trained batch 144 in epoch 0, gen_loss = 0.46039615437902254, disc_loss = 0.21641853081768958
Trained batch 145 in epoch 0, gen_loss = 0.45962427372801795, disc_loss = 0.21641983530701023
Trained batch 146 in epoch 0, gen_loss = 0.45927811236608596, disc_loss = 0.21637840913672027
Trained batch 147 in epoch 0, gen_loss = 0.45941814901055517, disc_loss = 0.21657080898011052
Trained batch 148 in epoch 0, gen_loss = 0.45898230403861745, disc_loss = 0.21647711438220618
Trained batch 149 in epoch 0, gen_loss = 0.45816001415252683, disc_loss = 0.2165283907453219
Trained batch 150 in epoch 0, gen_loss = 0.45781733973926264, disc_loss = 0.21651345639434083
Trained batch 151 in epoch 0, gen_loss = 0.4575586718948264, disc_loss = 0.2163445407426671
Trained batch 152 in epoch 0, gen_loss = 0.45759692379072603, disc_loss = 0.2162257554679135
Trained batch 153 in epoch 0, gen_loss = 0.45695142815639445, disc_loss = 0.21661834118815212
Trained batch 154 in epoch 0, gen_loss = 0.4572212782598311, disc_loss = 0.21698115573775384
Trained batch 155 in epoch 0, gen_loss = 0.4572206240815994, disc_loss = 0.217012809159664
Trained batch 156 in epoch 0, gen_loss = 0.4566431698525787, disc_loss = 0.21711432677545364
Trained batch 157 in epoch 0, gen_loss = 0.4565734650138058, disc_loss = 0.21695863861071912
Trained batch 158 in epoch 0, gen_loss = 0.4556822602478963, disc_loss = 0.217157485631277
Trained batch 159 in epoch 0, gen_loss = 0.4557373056188226, disc_loss = 0.2172577204182744
Trained batch 160 in epoch 0, gen_loss = 0.4553495821375284, disc_loss = 0.21713987992416997
Trained batch 161 in epoch 0, gen_loss = 0.4553734720857055, disc_loss = 0.21708366918711014
Trained batch 162 in epoch 0, gen_loss = 0.45528523450248815, disc_loss = 0.21710755369780255
Trained batch 163 in epoch 0, gen_loss = 0.455081049807188, disc_loss = 0.21695124457885578
Trained batch 164 in epoch 0, gen_loss = 0.45434309460900046, disc_loss = 0.21678347524368402
Trained batch 165 in epoch 0, gen_loss = 0.4544662694974118, disc_loss = 0.21669661136994878
Trained batch 166 in epoch 0, gen_loss = 0.45446542226625775, disc_loss = 0.21658456950130578
Trained batch 167 in epoch 0, gen_loss = 0.4541626608087903, disc_loss = 0.21646014006719702
Trained batch 168 in epoch 0, gen_loss = 0.45394190775572196, disc_loss = 0.2162291656231739
Trained batch 169 in epoch 0, gen_loss = 0.4536151467000737, disc_loss = 0.216328624504454
Trained batch 170 in epoch 0, gen_loss = 0.4530923824909835, disc_loss = 0.2163330022006007
Trained batch 171 in epoch 0, gen_loss = 0.45259710505258205, disc_loss = 0.21612080325220906
Trained batch 172 in epoch 0, gen_loss = 0.4522091627465507, disc_loss = 0.21590147499059667
Trained batch 173 in epoch 0, gen_loss = 0.452259622428609, disc_loss = 0.21573644610999645
Trained batch 174 in epoch 0, gen_loss = 0.45232014468738013, disc_loss = 0.21587966842310768
Trained batch 175 in epoch 0, gen_loss = 0.45180899649858475, disc_loss = 0.21589563583785837
Trained batch 176 in epoch 0, gen_loss = 0.45168273984375645, disc_loss = 0.215879755084124
Trained batch 177 in epoch 0, gen_loss = 0.4516546496849381, disc_loss = 0.21579234963387586
Trained batch 178 in epoch 0, gen_loss = 0.45165160564736945, disc_loss = 0.2162995492446356
Trained batch 179 in epoch 0, gen_loss = 0.4517250465022193, disc_loss = 0.21657005639539825
Trained batch 180 in epoch 0, gen_loss = 0.4517860055299095, disc_loss = 0.21650353823248195
Trained batch 181 in epoch 0, gen_loss = 0.45143338104525765, disc_loss = 0.21655325894499874
Trained batch 182 in epoch 0, gen_loss = 0.45131118001182224, disc_loss = 0.21659547945515054
Trained batch 183 in epoch 0, gen_loss = 0.45122712089315703, disc_loss = 0.21647507494882398
Trained batch 184 in epoch 0, gen_loss = 0.4509278141163491, disc_loss = 0.21620491509502
Trained batch 185 in epoch 0, gen_loss = 0.4506253141869781, disc_loss = 0.21593630666373878
Trained batch 186 in epoch 0, gen_loss = 0.4503834209340141, disc_loss = 0.2158183477779123
Trained batch 187 in epoch 0, gen_loss = 0.4500112157869846, disc_loss = 0.21551442463347253
Trained batch 188 in epoch 0, gen_loss = 0.44928362192931, disc_loss = 0.215379341490685
Trained batch 189 in epoch 0, gen_loss = 0.4489344414911772, disc_loss = 0.21540510513280567
Trained batch 190 in epoch 0, gen_loss = 0.44888088949687815, disc_loss = 0.21499092720878063
Trained batch 191 in epoch 0, gen_loss = 0.44880405937631923, disc_loss = 0.2149442515025536
Trained batch 192 in epoch 0, gen_loss = 0.4489597715244392, disc_loss = 0.21490493880034728
Trained batch 193 in epoch 0, gen_loss = 0.4487396838124265, disc_loss = 0.21506242154492544
Trained batch 194 in epoch 0, gen_loss = 0.448546901727334, disc_loss = 0.21488395585463596
Trained batch 195 in epoch 0, gen_loss = 0.4484332536860388, disc_loss = 0.21476123200691477
Trained batch 196 in epoch 0, gen_loss = 0.44891558216913097, disc_loss = 0.2144914494099351
Trained batch 197 in epoch 0, gen_loss = 0.4490912862197317, disc_loss = 0.2142380889919069
Trained batch 198 in epoch 0, gen_loss = 0.4489998378645835, disc_loss = 0.21380043673754937
Trained batch 199 in epoch 0, gen_loss = 0.44857619270682336, disc_loss = 0.21361321747303008
Trained batch 200 in epoch 0, gen_loss = 0.4486210792515408, disc_loss = 0.21373832848534655
Trained batch 201 in epoch 0, gen_loss = 0.4485642176748502, disc_loss = 0.21350457312742083
Trained batch 202 in epoch 0, gen_loss = 0.448359964663172, disc_loss = 0.213137954326686
Trained batch 203 in epoch 0, gen_loss = 0.44802428299889846, disc_loss = 0.21335784214384415
Trained batch 204 in epoch 0, gen_loss = 0.4481951116061792, disc_loss = 0.2139228121536534
Trained batch 205 in epoch 0, gen_loss = 0.4480495762477801, disc_loss = 0.21377482764350558
Trained batch 206 in epoch 0, gen_loss = 0.448227202978687, disc_loss = 0.2140293868555539
Trained batch 207 in epoch 0, gen_loss = 0.4481643998565582, disc_loss = 0.2138186776294158
Trained batch 208 in epoch 0, gen_loss = 0.4482237890576632, disc_loss = 0.2134864449358443
Trained batch 209 in epoch 0, gen_loss = 0.4479471816903069, disc_loss = 0.21333025615839732
Trained batch 210 in epoch 0, gen_loss = 0.44784319499657615, disc_loss = 0.21312311355254096
Trained batch 211 in epoch 0, gen_loss = 0.4477886678475254, disc_loss = 0.21293170799343092
Trained batch 212 in epoch 0, gen_loss = 0.4475775828383898, disc_loss = 0.21272457457484215
Trained batch 213 in epoch 0, gen_loss = 0.4474735655517222, disc_loss = 0.21248176394500465
Trained batch 214 in epoch 0, gen_loss = 0.4474601959073266, disc_loss = 0.21247428773447524
Trained batch 215 in epoch 0, gen_loss = 0.44747091098516073, disc_loss = 0.21264878998476047
Trained batch 216 in epoch 0, gen_loss = 0.4475352705074345, disc_loss = 0.2128731493713669
Trained batch 217 in epoch 0, gen_loss = 0.44733947734220314, disc_loss = 0.21286990124424662
Trained batch 218 in epoch 0, gen_loss = 0.44704156093401454, disc_loss = 0.21280320165636332
Trained batch 219 in epoch 0, gen_loss = 0.44679597656835207, disc_loss = 0.21272893100976945
Trained batch 220 in epoch 0, gen_loss = 0.44641493928378523, disc_loss = 0.21283687280314
Trained batch 221 in epoch 0, gen_loss = 0.4465591016116443, disc_loss = 0.21261717231424004
Trained batch 222 in epoch 0, gen_loss = 0.44642151230653837, disc_loss = 0.21239384280459228
Trained batch 223 in epoch 0, gen_loss = 0.44653450910534176, disc_loss = 0.21220659690776042
Trained batch 224 in epoch 0, gen_loss = 0.4461675911479526, disc_loss = 0.21206002765231663
Trained batch 225 in epoch 0, gen_loss = 0.44603127307068985, disc_loss = 0.21193572008504277
Trained batch 226 in epoch 0, gen_loss = 0.4460491863641445, disc_loss = 0.21194129959077038
Trained batch 227 in epoch 0, gen_loss = 0.4459289717569686, disc_loss = 0.2118740229491602
Trained batch 228 in epoch 0, gen_loss = 0.4456791508145728, disc_loss = 0.21181997734906893
Trained batch 229 in epoch 0, gen_loss = 0.4453715780506963, disc_loss = 0.21184173291144165
Trained batch 230 in epoch 0, gen_loss = 0.44547980843168317, disc_loss = 0.2117389745010442
Trained batch 231 in epoch 0, gen_loss = 0.44539107510755804, disc_loss = 0.21163577136808429
Trained batch 232 in epoch 0, gen_loss = 0.4452208313819165, disc_loss = 0.21154440216752082
Trained batch 233 in epoch 0, gen_loss = 0.445322772121837, disc_loss = 0.21159148088887206
Trained batch 234 in epoch 0, gen_loss = 0.44500710964202883, disc_loss = 0.21126434054780513
Trained batch 235 in epoch 0, gen_loss = 0.4445214078335439, disc_loss = 0.211334811807689
Trained batch 236 in epoch 0, gen_loss = 0.4447210544272314, disc_loss = 0.2109072006578687
Trained batch 237 in epoch 0, gen_loss = 0.44475122196834627, disc_loss = 0.21063831279508205
Trained batch 238 in epoch 0, gen_loss = 0.4446231851268513, disc_loss = 0.21049237974518015
Trained batch 239 in epoch 0, gen_loss = 0.4447637127091487, disc_loss = 0.21030835633476574
Trained batch 240 in epoch 0, gen_loss = 0.4448406520720834, disc_loss = 0.2099803813512889
Trained batch 241 in epoch 0, gen_loss = 0.44490044102195864, disc_loss = 0.20975971646791647
Trained batch 242 in epoch 0, gen_loss = 0.44473871355684697, disc_loss = 0.20961582832375672
Trained batch 243 in epoch 0, gen_loss = 0.4449664669447258, disc_loss = 0.20925546492465208
Trained batch 244 in epoch 0, gen_loss = 0.4451015035716855, disc_loss = 0.20998014606991594
Trained batch 245 in epoch 0, gen_loss = 0.44512845176022225, disc_loss = 0.21050694542444817
Trained batch 246 in epoch 0, gen_loss = 0.4452825603697464, disc_loss = 0.2103699163989982
Trained batch 247 in epoch 0, gen_loss = 0.44540385793774356, disc_loss = 0.21008460121529718
Trained batch 248 in epoch 0, gen_loss = 0.44539744451821567, disc_loss = 0.20974911240688768
Trained batch 249 in epoch 0, gen_loss = 0.44536833655834196, disc_loss = 0.20957443594932557
Trained batch 250 in epoch 0, gen_loss = 0.44520266348147297, disc_loss = 0.20940502753770684
Trained batch 251 in epoch 0, gen_loss = 0.4451181885032427, disc_loss = 0.2091840940217177
Trained batch 252 in epoch 0, gen_loss = 0.4448586664652165, disc_loss = 0.20911198660083438
Trained batch 253 in epoch 0, gen_loss = 0.44464876919280827, disc_loss = 0.2087180552872147
Trained batch 254 in epoch 0, gen_loss = 0.4444386091886782, disc_loss = 0.20840055907473845
Trained batch 255 in epoch 0, gen_loss = 0.4444644127506763, disc_loss = 0.20813277835259214
Trained batch 256 in epoch 0, gen_loss = 0.44424866134090646, disc_loss = 0.20769823423735362
Trained batch 257 in epoch 0, gen_loss = 0.44392866889635724, disc_loss = 0.20753174976900565
Trained batch 258 in epoch 0, gen_loss = 0.443736320181703, disc_loss = 0.2076399583786611
Trained batch 259 in epoch 0, gen_loss = 0.44348364713100286, disc_loss = 0.2075524407797135
Trained batch 260 in epoch 0, gen_loss = 0.4429918592688681, disc_loss = 0.20736211438402818
Trained batch 261 in epoch 0, gen_loss = 0.4431318197086567, disc_loss = 0.20733328223342204
Trained batch 262 in epoch 0, gen_loss = 0.44281124796704197, disc_loss = 0.2072541889072371
Trained batch 263 in epoch 0, gen_loss = 0.4427771367358439, disc_loss = 0.20688508916646242
Trained batch 264 in epoch 0, gen_loss = 0.4424873453266216, disc_loss = 0.20666110495913703
Trained batch 265 in epoch 0, gen_loss = 0.44267384360607404, disc_loss = 0.20629048400691577
Trained batch 266 in epoch 0, gen_loss = 0.4423136734560634, disc_loss = 0.20613373227222134
Trained batch 267 in epoch 0, gen_loss = 0.4425335814481351, disc_loss = 0.20596373867966347
Trained batch 268 in epoch 0, gen_loss = 0.44243617582941586, disc_loss = 0.20576808058750232
Trained batch 269 in epoch 0, gen_loss = 0.4425909003725758, disc_loss = 0.20557077883018388
Trained batch 270 in epoch 0, gen_loss = 0.4426775482307941, disc_loss = 0.20529601437029363
Trained batch 271 in epoch 0, gen_loss = 0.44259147499414053, disc_loss = 0.2053254382301341
Trained batch 272 in epoch 0, gen_loss = 0.4425936681883676, disc_loss = 0.20561613680133017
Trained batch 273 in epoch 0, gen_loss = 0.4425700724559979, disc_loss = 0.2059711009862214
Trained batch 274 in epoch 0, gen_loss = 0.4424690350619229, disc_loss = 0.20546861643140965
Trained batch 275 in epoch 0, gen_loss = 0.44227149337530136, disc_loss = 0.2051880941118883
Trained batch 276 in epoch 0, gen_loss = 0.4423584857357108, disc_loss = 0.20502233015716292
Trained batch 277 in epoch 0, gen_loss = 0.442480005163083, disc_loss = 0.20489110706521452
Trained batch 278 in epoch 0, gen_loss = 0.44214332189183936, disc_loss = 0.20473333258569026
Trained batch 279 in epoch 0, gen_loss = 0.4420918092131615, disc_loss = 0.20461084565946033
Trained batch 280 in epoch 0, gen_loss = 0.44204700406760083, disc_loss = 0.2044401669417412
Trained batch 281 in epoch 0, gen_loss = 0.4419629158491784, disc_loss = 0.20415081720508582
Trained batch 282 in epoch 0, gen_loss = 0.4417594241590466, disc_loss = 0.20451319957474517
Trained batch 283 in epoch 0, gen_loss = 0.4420089083658138, disc_loss = 0.2044247626261392
Trained batch 284 in epoch 0, gen_loss = 0.4418047836998053, disc_loss = 0.2041568831655017
Trained batch 285 in epoch 0, gen_loss = 0.4418896916029337, disc_loss = 0.20395344232658405
Trained batch 286 in epoch 0, gen_loss = 0.4419364669597107, disc_loss = 0.20365966856479645
Trained batch 287 in epoch 0, gen_loss = 0.4417879705627759, disc_loss = 0.20352732638518015
Trained batch 288 in epoch 0, gen_loss = 0.44165055902358985, disc_loss = 0.20313474340009854
Trained batch 289 in epoch 0, gen_loss = 0.4414996667154904, disc_loss = 0.2032344958905516
Trained batch 290 in epoch 0, gen_loss = 0.44178812856117067, disc_loss = 0.20351628626335116
Trained batch 291 in epoch 0, gen_loss = 0.44161389717092253, disc_loss = 0.20327888541433908
Trained batch 292 in epoch 0, gen_loss = 0.44158344647176434, disc_loss = 0.2032390197185933
Trained batch 293 in epoch 0, gen_loss = 0.44149881641880995, disc_loss = 0.20292263711188116
Trained batch 294 in epoch 0, gen_loss = 0.4413712371203859, disc_loss = 0.20299210715091834
Trained batch 295 in epoch 0, gen_loss = 0.44118829433982437, disc_loss = 0.20279655973049435
Trained batch 296 in epoch 0, gen_loss = 0.4408683875193098, disc_loss = 0.2026705098673952
Trained batch 297 in epoch 0, gen_loss = 0.44095797146726773, disc_loss = 0.20236708503841555
Trained batch 298 in epoch 0, gen_loss = 0.4412026387393275, disc_loss = 0.2023793950826428
Trained batch 299 in epoch 0, gen_loss = 0.44111584186553954, disc_loss = 0.20205559802552064
Trained batch 300 in epoch 0, gen_loss = 0.4411065620045329, disc_loss = 0.20170615927820587
Trained batch 301 in epoch 0, gen_loss = 0.4410530863613482, disc_loss = 0.20135440428229356
Trained batch 302 in epoch 0, gen_loss = 0.44113311387918175, disc_loss = 0.20096353441476822
Trained batch 303 in epoch 0, gen_loss = 0.44105633838396324, disc_loss = 0.20082531259150097
Trained batch 304 in epoch 0, gen_loss = 0.44138587947751656, disc_loss = 0.20079234025028886
Trained batch 305 in epoch 0, gen_loss = 0.44132213586685703, disc_loss = 0.2004201189900925
Trained batch 306 in epoch 0, gen_loss = 0.44114695989347824, disc_loss = 0.20004831174394594
Trained batch 307 in epoch 0, gen_loss = 0.4412113144606739, disc_loss = 0.19972856633074873
Trained batch 308 in epoch 0, gen_loss = 0.44089328210716494, disc_loss = 0.19973664260605006
Trained batch 309 in epoch 0, gen_loss = 0.4406321561144244, disc_loss = 0.20042896847571096
Trained batch 310 in epoch 0, gen_loss = 0.44063292285637073, disc_loss = 0.20070904982051666
Trained batch 311 in epoch 0, gen_loss = 0.44049238967589843, disc_loss = 0.2005558515397402
Trained batch 312 in epoch 0, gen_loss = 0.440546688085166, disc_loss = 0.20040457688581448
Trained batch 313 in epoch 0, gen_loss = 0.44052540904777066, disc_loss = 0.20019492022930438
Trained batch 314 in epoch 0, gen_loss = 0.44046122233072915, disc_loss = 0.2001575340827306
Trained batch 315 in epoch 0, gen_loss = 0.44039194483923005, disc_loss = 0.19990879122780847
Trained batch 316 in epoch 0, gen_loss = 0.44004886599745285, disc_loss = 0.20006900739030506
Trained batch 317 in epoch 0, gen_loss = 0.4399969015481337, disc_loss = 0.1997933384476218
Trained batch 318 in epoch 0, gen_loss = 0.4397990677423985, disc_loss = 0.19961092838299313
Trained batch 319 in epoch 0, gen_loss = 0.4399110763333738, disc_loss = 0.19935161070898175
Trained batch 320 in epoch 0, gen_loss = 0.43997975533996414, disc_loss = 0.1997676889101664
Trained batch 321 in epoch 0, gen_loss = 0.43978970678326507, disc_loss = 0.1997101679907082
Trained batch 322 in epoch 0, gen_loss = 0.4398431109939197, disc_loss = 0.19953340854807167
Trained batch 323 in epoch 0, gen_loss = 0.4398819714048762, disc_loss = 0.19956863225426202
Trained batch 324 in epoch 0, gen_loss = 0.4395787858963013, disc_loss = 0.19977465643332554
Trained batch 325 in epoch 0, gen_loss = 0.4395743904669592, disc_loss = 0.1998673106995097
Trained batch 326 in epoch 0, gen_loss = 0.4394055690604977, disc_loss = 0.19998894262751307
Trained batch 327 in epoch 0, gen_loss = 0.43948512905981485, disc_loss = 0.19996483087903116
Trained batch 328 in epoch 0, gen_loss = 0.4394976005728122, disc_loss = 0.19993438397316224
Trained batch 329 in epoch 0, gen_loss = 0.43937764926390216, disc_loss = 0.1999217686328021
Trained batch 330 in epoch 0, gen_loss = 0.43932601980572383, disc_loss = 0.1998292147843139
Trained batch 331 in epoch 0, gen_loss = 0.43938891921775886, disc_loss = 0.19973682778247867
Trained batch 332 in epoch 0, gen_loss = 0.43947949012120563, disc_loss = 0.19963730979073155
Trained batch 333 in epoch 0, gen_loss = 0.43958233368253996, disc_loss = 0.1993123831193961
Trained batch 334 in epoch 0, gen_loss = 0.4395240397595648, disc_loss = 0.1991890948432595
Trained batch 335 in epoch 0, gen_loss = 0.4394694911759524, disc_loss = 0.1993442582454355
Trained batch 336 in epoch 0, gen_loss = 0.4392719803825684, disc_loss = 0.19934575659850232
Trained batch 337 in epoch 0, gen_loss = 0.4391569496964562, disc_loss = 0.19914458182877337
Trained batch 338 in epoch 0, gen_loss = 0.43935379869466684, disc_loss = 0.19900127364031334
Trained batch 339 in epoch 0, gen_loss = 0.4392139602233382, disc_loss = 0.19862626521464657
Trained batch 340 in epoch 0, gen_loss = 0.4390217727405235, disc_loss = 0.1985738674872432
Trained batch 341 in epoch 0, gen_loss = 0.4390625649551202, disc_loss = 0.1985146793487825
Trained batch 342 in epoch 0, gen_loss = 0.4388978549933642, disc_loss = 0.1984559841692969
Trained batch 343 in epoch 0, gen_loss = 0.43888634451946545, disc_loss = 0.19871459149777196
Trained batch 344 in epoch 0, gen_loss = 0.4388983271260192, disc_loss = 0.19861056284196135
Trained batch 345 in epoch 0, gen_loss = 0.43879420883049164, disc_loss = 0.19847490054937456
Trained batch 346 in epoch 0, gen_loss = 0.43870644576267825, disc_loss = 0.19852708196348004
Trained batch 347 in epoch 0, gen_loss = 0.4386549102163863, disc_loss = 0.19873968295581723
Trained batch 348 in epoch 0, gen_loss = 0.43841842687915594, disc_loss = 0.19876315717966986
Trained batch 349 in epoch 0, gen_loss = 0.4384474859918867, disc_loss = 0.19857856884598732
Trained batch 350 in epoch 0, gen_loss = 0.4383352526232728, disc_loss = 0.1983782400050734
Trained batch 351 in epoch 0, gen_loss = 0.4384703162041577, disc_loss = 0.19813578425567935
Trained batch 352 in epoch 0, gen_loss = 0.4383266324193214, disc_loss = 0.19806457150505893
Trained batch 353 in epoch 0, gen_loss = 0.4381913068772709, disc_loss = 0.19822504967030158
Trained batch 354 in epoch 0, gen_loss = 0.43827515362014235, disc_loss = 0.19818734113179462
Trained batch 355 in epoch 0, gen_loss = 0.4385403539524989, disc_loss = 0.19802739628161606
Trained batch 356 in epoch 0, gen_loss = 0.43849101128364476, disc_loss = 0.1980799346202228
Trained batch 357 in epoch 0, gen_loss = 0.4384205558613026, disc_loss = 0.1980041780992926
Trained batch 358 in epoch 0, gen_loss = 0.43838006390834583, disc_loss = 0.19765659994484655
Trained batch 359 in epoch 0, gen_loss = 0.43818902323643366, disc_loss = 0.19755630735307933
Trained batch 360 in epoch 0, gen_loss = 0.4381603262596183, disc_loss = 0.19741709775954402
Trained batch 361 in epoch 0, gen_loss = 0.4380811214117714, disc_loss = 0.19715618893751125
Trained batch 362 in epoch 0, gen_loss = 0.4380354629239432, disc_loss = 0.19698957951108287
Trained batch 363 in epoch 0, gen_loss = 0.43810177610798196, disc_loss = 0.19720352396041482
Trained batch 364 in epoch 0, gen_loss = 0.43789654725218474, disc_loss = 0.1973143063587685
Trained batch 365 in epoch 0, gen_loss = 0.43796920043523196, disc_loss = 0.19734199072331027
Trained batch 366 in epoch 0, gen_loss = 0.4379073033865531, disc_loss = 0.1970260189567015
Trained batch 367 in epoch 0, gen_loss = 0.43763650605536025, disc_loss = 0.19686025713125002
Trained batch 368 in epoch 0, gen_loss = 0.4376570290020165, disc_loss = 0.1965719385319932
Trained batch 369 in epoch 0, gen_loss = 0.43779989368206745, disc_loss = 0.1962859356121437
Trained batch 370 in epoch 0, gen_loss = 0.4379845078422053, disc_loss = 0.19595632043408576
Trained batch 371 in epoch 0, gen_loss = 0.43804913470821993, disc_loss = 0.19576129483519703
Trained batch 372 in epoch 0, gen_loss = 0.4382338121173848, disc_loss = 0.1956464730342975
Trained batch 373 in epoch 0, gen_loss = 0.4380832177432463, disc_loss = 0.19592007657981172
Trained batch 374 in epoch 0, gen_loss = 0.43817339579264325, disc_loss = 0.1959371803800265
Trained batch 375 in epoch 0, gen_loss = 0.43815667610219183, disc_loss = 0.1956439287936751
Trained batch 376 in epoch 0, gen_loss = 0.4382141839604163, disc_loss = 0.1952241095924251
Trained batch 377 in epoch 0, gen_loss = 0.4384976707438312, disc_loss = 0.19494827016793861
Trained batch 378 in epoch 0, gen_loss = 0.4384908065946876, disc_loss = 0.1946418652197931
Trained batch 379 in epoch 0, gen_loss = 0.43835383002695283, disc_loss = 0.19439530776519526
Trained batch 380 in epoch 0, gen_loss = 0.438625628006427, disc_loss = 0.19399331306691558
Trained batch 381 in epoch 0, gen_loss = 0.43881540593364476, disc_loss = 0.19364531546909147
Trained batch 382 in epoch 0, gen_loss = 0.4389349221405099, disc_loss = 0.1932745551782862
Trained batch 383 in epoch 0, gen_loss = 0.43910321694177884, disc_loss = 0.19280495254982574
Trained batch 384 in epoch 0, gen_loss = 0.43884145354295706, disc_loss = 0.19246107613692037
Trained batch 385 in epoch 0, gen_loss = 0.43903210519817826, disc_loss = 0.19202352514963383
Trained batch 386 in epoch 0, gen_loss = 0.43913881399834803, disc_loss = 0.19157154866775802
Trained batch 387 in epoch 0, gen_loss = 0.43941469167925645, disc_loss = 0.19111791417271512
Trained batch 388 in epoch 0, gen_loss = 0.4393865790189385, disc_loss = 0.19072268664798578
Trained batch 389 in epoch 0, gen_loss = 0.43923599826983917, disc_loss = 0.19085323729385167
Trained batch 390 in epoch 0, gen_loss = 0.43934628420778554, disc_loss = 0.19062070398951125
Trained batch 391 in epoch 0, gen_loss = 0.4393318696897857, disc_loss = 0.1905076139492496
Trained batch 392 in epoch 0, gen_loss = 0.43925023283667236, disc_loss = 0.19046458927085078
Trained batch 393 in epoch 0, gen_loss = 0.43931687513583806, disc_loss = 0.19019345096714302
Trained batch 394 in epoch 0, gen_loss = 0.4394045976898338, disc_loss = 0.19020131472733956
Trained batch 395 in epoch 0, gen_loss = 0.4393739626564161, disc_loss = 0.1899349136992988
Trained batch 396 in epoch 0, gen_loss = 0.4391027714503502, disc_loss = 0.18997948369954334
Trained batch 397 in epoch 0, gen_loss = 0.43936184872334927, disc_loss = 0.19071347179387382
Trained batch 398 in epoch 0, gen_loss = 0.4391912407146062, disc_loss = 0.19039042879428184
Trained batch 399 in epoch 0, gen_loss = 0.43911353141069415, disc_loss = 0.19082217170856894
Trained batch 400 in epoch 0, gen_loss = 0.4390698564320134, disc_loss = 0.1906203704414671
Trained batch 401 in epoch 0, gen_loss = 0.4390130544183266, disc_loss = 0.1904825431697849
Trained batch 402 in epoch 0, gen_loss = 0.4390887669091189, disc_loss = 0.1903285656965548
Trained batch 403 in epoch 0, gen_loss = 0.4391699701842695, disc_loss = 0.1901728061147698
Trained batch 404 in epoch 0, gen_loss = 0.43907947716889556, disc_loss = 0.18999668623746177
Trained batch 405 in epoch 0, gen_loss = 0.4391354185166617, disc_loss = 0.18984149681780432
Trained batch 406 in epoch 0, gen_loss = 0.4390042312168665, disc_loss = 0.18962788866507918
Trained batch 407 in epoch 0, gen_loss = 0.4389663601622862, disc_loss = 0.18953041979750873
Trained batch 408 in epoch 0, gen_loss = 0.4387605717100549, disc_loss = 0.18934854725717332
Trained batch 409 in epoch 0, gen_loss = 0.4387187428590728, disc_loss = 0.18925937849392252
Trained batch 410 in epoch 0, gen_loss = 0.4388259325874403, disc_loss = 0.18963263337013206
Trained batch 411 in epoch 0, gen_loss = 0.43887852479531925, disc_loss = 0.18965225123339197
Trained batch 412 in epoch 0, gen_loss = 0.43904371108616236, disc_loss = 0.18942430785904496
Trained batch 413 in epoch 0, gen_loss = 0.4388475793859233, disc_loss = 0.18951262862102133
Trained batch 414 in epoch 0, gen_loss = 0.4388400403131922, disc_loss = 0.18935834773513208
Trained batch 415 in epoch 0, gen_loss = 0.43862790836451143, disc_loss = 0.1892826149616247
Trained batch 416 in epoch 0, gen_loss = 0.4386691437493697, disc_loss = 0.18937745412512363
Trained batch 417 in epoch 0, gen_loss = 0.43871427787262857, disc_loss = 0.1891252408026936
Trained batch 418 in epoch 0, gen_loss = 0.43865863512979203, disc_loss = 0.18890903202865744
Trained batch 419 in epoch 0, gen_loss = 0.43852512254601433, disc_loss = 0.18873282446570339
Trained batch 420 in epoch 0, gen_loss = 0.4381693080598555, disc_loss = 0.1889969175880567
Trained batch 421 in epoch 0, gen_loss = 0.4380488420408484, disc_loss = 0.1889519798723866
Trained batch 422 in epoch 0, gen_loss = 0.43804345098511266, disc_loss = 0.189490872908639
Trained batch 423 in epoch 0, gen_loss = 0.4379772113741569, disc_loss = 0.18940258559437012
Trained batch 424 in epoch 0, gen_loss = 0.4380808474737055, disc_loss = 0.18944647297263145
Trained batch 425 in epoch 0, gen_loss = 0.4380350154890141, disc_loss = 0.1893620121909279
Trained batch 426 in epoch 0, gen_loss = 0.43794805216286725, disc_loss = 0.1893159692247662
Trained batch 427 in epoch 0, gen_loss = 0.43781951278726633, disc_loss = 0.18911774669747763
Trained batch 428 in epoch 0, gen_loss = 0.43777215557220656, disc_loss = 0.18894534317520392
Trained batch 429 in epoch 0, gen_loss = 0.4376641306073167, disc_loss = 0.18887460520274418
Trained batch 430 in epoch 0, gen_loss = 0.43779682414714255, disc_loss = 0.18901437525302517
Trained batch 431 in epoch 0, gen_loss = 0.43761813999326143, disc_loss = 0.1888421609290634
Trained batch 432 in epoch 0, gen_loss = 0.4374483421823444, disc_loss = 0.1887446779835169
Trained batch 433 in epoch 0, gen_loss = 0.4374690723309319, disc_loss = 0.18859509916864506
Trained batch 434 in epoch 0, gen_loss = 0.43753162656707323, disc_loss = 0.18838960859796097
Trained batch 435 in epoch 0, gen_loss = 0.43760350708841184, disc_loss = 0.18818135627058394
Trained batch 436 in epoch 0, gen_loss = 0.4376726988660389, disc_loss = 0.18793913010226507
Trained batch 437 in epoch 0, gen_loss = 0.4375531336624328, disc_loss = 0.18798389820781744
Trained batch 438 in epoch 0, gen_loss = 0.4374017237530754, disc_loss = 0.18824936657431185
Trained batch 439 in epoch 0, gen_loss = 0.4373719822276722, disc_loss = 0.18830596751279452
Trained batch 440 in epoch 0, gen_loss = 0.43737531417892095, disc_loss = 0.18839947755138078
Trained batch 441 in epoch 0, gen_loss = 0.43725967892694256, disc_loss = 0.18856051200958668
Trained batch 442 in epoch 0, gen_loss = 0.4372571024614855, disc_loss = 0.18852868734209316
Trained batch 443 in epoch 0, gen_loss = 0.4372511496683499, disc_loss = 0.1884120916276499
Trained batch 444 in epoch 0, gen_loss = 0.43711142720801105, disc_loss = 0.18849727736597652
Trained batch 445 in epoch 0, gen_loss = 0.43707242053453166, disc_loss = 0.1885319708725396
Trained batch 446 in epoch 0, gen_loss = 0.4367847149937478, disc_loss = 0.18858783001234333
Trained batch 447 in epoch 0, gen_loss = 0.4366826078455363, disc_loss = 0.18870881044339122
Trained batch 448 in epoch 0, gen_loss = 0.4368402311424901, disc_loss = 0.18854986374289262
Trained batch 449 in epoch 0, gen_loss = 0.4368162986967299, disc_loss = 0.18838228064278761
Trained batch 450 in epoch 0, gen_loss = 0.4368660719590282, disc_loss = 0.1881787467277103
Trained batch 451 in epoch 0, gen_loss = 0.43686223478443853, disc_loss = 0.1880079451206643
Trained batch 452 in epoch 0, gen_loss = 0.4367844721602551, disc_loss = 0.18774161556073635
Trained batch 453 in epoch 0, gen_loss = 0.43682365624915137, disc_loss = 0.18770457650760458
Trained batch 454 in epoch 0, gen_loss = 0.43668486783792687, disc_loss = 0.18772412781695744
Trained batch 455 in epoch 0, gen_loss = 0.4368178584988703, disc_loss = 0.1876503731483561
Trained batch 456 in epoch 0, gen_loss = 0.43681211238095224, disc_loss = 0.18756062292067957
Trained batch 457 in epoch 0, gen_loss = 0.4368135837358158, disc_loss = 0.18749943431710833
Trained batch 458 in epoch 0, gen_loss = 0.4367777677273179, disc_loss = 0.18736043000240732
Trained batch 459 in epoch 0, gen_loss = 0.4367099158141924, disc_loss = 0.18725694517564515
Trained batch 460 in epoch 0, gen_loss = 0.4368083140855239, disc_loss = 0.1870662291451416
Trained batch 461 in epoch 0, gen_loss = 0.43676727255443476, disc_loss = 0.186859352256119
Trained batch 462 in epoch 0, gen_loss = 0.4367215839369508, disc_loss = 0.18664277765747023
Trained batch 463 in epoch 0, gen_loss = 0.43680027088728446, disc_loss = 0.1864173566392655
Trained batch 464 in epoch 0, gen_loss = 0.4366231717089171, disc_loss = 0.18627966840260773
Trained batch 465 in epoch 0, gen_loss = 0.43660604781091467, disc_loss = 0.18638876546573996
Trained batch 466 in epoch 0, gen_loss = 0.43650029730490497, disc_loss = 0.1862475227413009
Trained batch 467 in epoch 0, gen_loss = 0.4365075227414441, disc_loss = 0.18605066077290183
Trained batch 468 in epoch 0, gen_loss = 0.4366539106058922, disc_loss = 0.18605734839209362
Trained batch 469 in epoch 0, gen_loss = 0.43688130055336244, disc_loss = 0.18638703425989506
Trained batch 470 in epoch 0, gen_loss = 0.4367339561699302, disc_loss = 0.18637987989497287
Trained batch 471 in epoch 0, gen_loss = 0.4368517148797795, disc_loss = 0.18626934727969563
Trained batch 472 in epoch 0, gen_loss = 0.43691175167958785, disc_loss = 0.18619009331813827
Trained batch 473 in epoch 0, gen_loss = 0.4367133912783635, disc_loss = 0.18628001391478982
Trained batch 474 in epoch 0, gen_loss = 0.4369144204415773, disc_loss = 0.1865456523001194
Trained batch 475 in epoch 0, gen_loss = 0.4368559447031061, disc_loss = 0.1866080745225068
Trained batch 476 in epoch 0, gen_loss = 0.43686567368747303, disc_loss = 0.18645482086254367
Trained batch 477 in epoch 0, gen_loss = 0.4369994357664715, disc_loss = 0.18630405273054684
Trained batch 478 in epoch 0, gen_loss = 0.43706356975131344, disc_loss = 0.18620208332738947
Trained batch 479 in epoch 0, gen_loss = 0.43696181358148656, disc_loss = 0.1861481164038802
Trained batch 480 in epoch 0, gen_loss = 0.4368412358969016, disc_loss = 0.18594659955517664
Trained batch 481 in epoch 0, gen_loss = 0.43697773785759303, disc_loss = 0.18586947009183807
Trained batch 482 in epoch 0, gen_loss = 0.43690272122930046, disc_loss = 0.1856225036917636
Trained batch 483 in epoch 0, gen_loss = 0.43699745218123287, disc_loss = 0.18539971675846942
Trained batch 484 in epoch 0, gen_loss = 0.43703395564531544, disc_loss = 0.18524384166008417
Trained batch 485 in epoch 0, gen_loss = 0.4370020028862934, disc_loss = 0.1850543775108984
Trained batch 486 in epoch 0, gen_loss = 0.43693289399391816, disc_loss = 0.1853552956891378
Trained batch 487 in epoch 0, gen_loss = 0.43714274588178414, disc_loss = 0.1854946904540917
Trained batch 488 in epoch 0, gen_loss = 0.4371341525411313, disc_loss = 0.18538590673745775
Trained batch 489 in epoch 0, gen_loss = 0.43709483049353776, disc_loss = 0.1853728560996907
Trained batch 490 in epoch 0, gen_loss = 0.43704752159701343, disc_loss = 0.18539934278353898
Trained batch 491 in epoch 0, gen_loss = 0.43697716622817806, disc_loss = 0.18531285131698458
Trained batch 492 in epoch 0, gen_loss = 0.43699952972104533, disc_loss = 0.18517443931398964
Trained batch 493 in epoch 0, gen_loss = 0.4368896263572369, disc_loss = 0.18504395562899018
Trained batch 494 in epoch 0, gen_loss = 0.43668300178315905, disc_loss = 0.1849732063409656
Trained batch 495 in epoch 0, gen_loss = 0.4367665992749314, disc_loss = 0.1849165961161376
Trained batch 496 in epoch 0, gen_loss = 0.4366835128493472, disc_loss = 0.18494725207117962
Trained batch 497 in epoch 0, gen_loss = 0.43670983151738424, disc_loss = 0.1849643258400351
Trained batch 498 in epoch 0, gen_loss = 0.43657347315060113, disc_loss = 0.18485756693986471
Trained batch 499 in epoch 0, gen_loss = 0.43656917548179625, disc_loss = 0.18478064335137606
Trained batch 500 in epoch 0, gen_loss = 0.43660017419717984, disc_loss = 0.18478080351463336
Trained batch 501 in epoch 0, gen_loss = 0.43645455631839325, disc_loss = 0.18468509009664513
Trained batch 502 in epoch 0, gen_loss = 0.4366355580435122, disc_loss = 0.18474292498133765
Trained batch 503 in epoch 0, gen_loss = 0.4365976866157282, disc_loss = 0.18474574242940262
Trained batch 504 in epoch 0, gen_loss = 0.4366919175823136, disc_loss = 0.1846742340907602
Trained batch 505 in epoch 0, gen_loss = 0.43680360766031995, disc_loss = 0.18442162350114627
Trained batch 506 in epoch 0, gen_loss = 0.4366710464629901, disc_loss = 0.18455495063722485
Trained batch 507 in epoch 0, gen_loss = 0.4367112929778775, disc_loss = 0.18461312306678201
Trained batch 508 in epoch 0, gen_loss = 0.4365512886192569, disc_loss = 0.1844865339853909
Trained batch 509 in epoch 0, gen_loss = 0.43646671999903286, disc_loss = 0.18434101398669037
Trained batch 510 in epoch 0, gen_loss = 0.43648775187257216, disc_loss = 0.1844821309565098
Trained batch 511 in epoch 0, gen_loss = 0.43637366325128824, disc_loss = 0.18430272747355048
Trained batch 512 in epoch 0, gen_loss = 0.4363408400068971, disc_loss = 0.18413691481186864
Trained batch 513 in epoch 0, gen_loss = 0.43649967727957995, disc_loss = 0.1840122050899244
Trained batch 514 in epoch 0, gen_loss = 0.4365128651984687, disc_loss = 0.1838557277344963
Trained batch 515 in epoch 0, gen_loss = 0.43647304494944655, disc_loss = 0.18372902893569582
Trained batch 516 in epoch 0, gen_loss = 0.43638811506893005, disc_loss = 0.18359605692744946
Trained batch 517 in epoch 0, gen_loss = 0.4361308366635591, disc_loss = 0.18362559664375994
Trained batch 518 in epoch 0, gen_loss = 0.4361963499488169, disc_loss = 0.18346963669236224
Trained batch 519 in epoch 0, gen_loss = 0.43622796810590303, disc_loss = 0.1834822296952972
Trained batch 520 in epoch 0, gen_loss = 0.43604472897331914, disc_loss = 0.1837835443311598
Trained batch 521 in epoch 0, gen_loss = 0.43616570783529246, disc_loss = 0.1837188195137457
Trained batch 522 in epoch 0, gen_loss = 0.43612534525745456, disc_loss = 0.18365107367018213
Trained batch 523 in epoch 0, gen_loss = 0.43604431466291876, disc_loss = 0.18352534334736925
Trained batch 524 in epoch 0, gen_loss = 0.4360745636622111, disc_loss = 0.18348752254531497
Trained batch 525 in epoch 0, gen_loss = 0.4361203550272568, disc_loss = 0.18332675456320832
Trained batch 526 in epoch 0, gen_loss = 0.4361107081344503, disc_loss = 0.18334340257481788
Trained batch 527 in epoch 0, gen_loss = 0.4360157446653554, disc_loss = 0.18331102228187252
Trained batch 528 in epoch 0, gen_loss = 0.43593260324744965, disc_loss = 0.18311183392607197
Trained batch 529 in epoch 0, gen_loss = 0.4361084315012086, disc_loss = 0.1829971861726833
Trained batch 530 in epoch 0, gen_loss = 0.43605849023368176, disc_loss = 0.18318425116116943
Trained batch 531 in epoch 0, gen_loss = 0.435991326108911, disc_loss = 0.18307369203775897
Trained batch 532 in epoch 0, gen_loss = 0.4358451646555208, disc_loss = 0.1829034128063839
Trained batch 533 in epoch 0, gen_loss = 0.43602376601445986, disc_loss = 0.18299563058864757
Trained batch 534 in epoch 0, gen_loss = 0.4360015140515622, disc_loss = 0.18285653722063402
Trained batch 535 in epoch 0, gen_loss = 0.4359842086794661, disc_loss = 0.18262570343022025
Trained batch 536 in epoch 0, gen_loss = 0.4359500451460897, disc_loss = 0.1825113846214568
Trained batch 537 in epoch 0, gen_loss = 0.4359959721232879, disc_loss = 0.18242549976562478
Trained batch 538 in epoch 0, gen_loss = 0.4360491267382988, disc_loss = 0.18224020459877538
Trained batch 539 in epoch 0, gen_loss = 0.4359874743554327, disc_loss = 0.1820431548136252
Trained batch 540 in epoch 0, gen_loss = 0.43599541212626614, disc_loss = 0.1818816813325265
Trained batch 541 in epoch 0, gen_loss = 0.4359758904829236, disc_loss = 0.18183988778133675
Trained batch 542 in epoch 0, gen_loss = 0.43600788811293756, disc_loss = 0.1817431426289771
Trained batch 543 in epoch 0, gen_loss = 0.43590779060169177, disc_loss = 0.181646204471369
Trained batch 544 in epoch 0, gen_loss = 0.4360446220691051, disc_loss = 0.18190304705309213
Trained batch 545 in epoch 0, gen_loss = 0.4359774167611922, disc_loss = 0.1820562346241413
Trained batch 546 in epoch 0, gen_loss = 0.4361041463380322, disc_loss = 0.18208443391148743
Trained batch 547 in epoch 0, gen_loss = 0.4361860905468029, disc_loss = 0.1822388842429993
Trained batch 548 in epoch 0, gen_loss = 0.43607890334502813, disc_loss = 0.18211971121570453
Trained batch 549 in epoch 0, gen_loss = 0.43608987759460105, disc_loss = 0.18211790647019038
Trained batch 550 in epoch 0, gen_loss = 0.43603946806731114, disc_loss = 0.18196749389712477
Trained batch 551 in epoch 0, gen_loss = 0.4360874357322852, disc_loss = 0.1817760443638848
Trained batch 552 in epoch 0, gen_loss = 0.43619897126289214, disc_loss = 0.18158399327161298
Trained batch 553 in epoch 0, gen_loss = 0.4363740084403689, disc_loss = 0.1815739057366383
Trained batch 554 in epoch 0, gen_loss = 0.43633965382704865, disc_loss = 0.1814860400181633
Trained batch 555 in epoch 0, gen_loss = 0.4363579828318932, disc_loss = 0.181269012217363
Trained batch 556 in epoch 0, gen_loss = 0.4363858002092509, disc_loss = 0.18121974546081074
Trained batch 557 in epoch 0, gen_loss = 0.43638771852498415, disc_loss = 0.18105992835055115
Trained batch 558 in epoch 0, gen_loss = 0.43637949484192196, disc_loss = 0.18088506844581134
Trained batch 559 in epoch 0, gen_loss = 0.4365292907825538, disc_loss = 0.18071190174669027
Trained batch 560 in epoch 0, gen_loss = 0.436612879634966, disc_loss = 0.18050048487682607
Trained batch 561 in epoch 0, gen_loss = 0.4364426296164557, disc_loss = 0.18027432930400575
Trained batch 562 in epoch 0, gen_loss = 0.43633235924832564, disc_loss = 0.180440537294052
Trained batch 563 in epoch 0, gen_loss = 0.43645510040487806, disc_loss = 0.18026653988326483
Trained batch 564 in epoch 0, gen_loss = 0.43632512514570115, disc_loss = 0.180088095096624
Trained batch 565 in epoch 0, gen_loss = 0.4363047404761028, disc_loss = 0.17997570955210987
Trained batch 566 in epoch 0, gen_loss = 0.4363879968775223, disc_loss = 0.17984191091103738
Trained batch 567 in epoch 0, gen_loss = 0.43639687333308474, disc_loss = 0.17959633353494212
Trained batch 568 in epoch 0, gen_loss = 0.4363168147515329, disc_loss = 0.17964595306815287
Trained batch 569 in epoch 0, gen_loss = 0.43659296365160694, disc_loss = 0.17964954372952904
Trained batch 570 in epoch 0, gen_loss = 0.4367513806410721, disc_loss = 0.17937327955642327
Trained batch 571 in epoch 0, gen_loss = 0.4366525150575004, disc_loss = 0.17913339809039153
Trained batch 572 in epoch 0, gen_loss = 0.43664012305815597, disc_loss = 0.17895016872224903
Trained batch 573 in epoch 0, gen_loss = 0.4366383890553219, disc_loss = 0.17873105255218438
Trained batch 574 in epoch 0, gen_loss = 0.43659911347472147, disc_loss = 0.1786055869673905
Trained batch 575 in epoch 0, gen_loss = 0.4366647035607861, disc_loss = 0.17840770240420373
Trained batch 576 in epoch 0, gen_loss = 0.4368183942870003, disc_loss = 0.17819612602698431
Trained batch 577 in epoch 0, gen_loss = 0.43670350911295536, disc_loss = 0.17817604888849295
Trained batch 578 in epoch 0, gen_loss = 0.4366007665484269, disc_loss = 0.17824194042755223
Trained batch 579 in epoch 0, gen_loss = 0.4367255337279418, disc_loss = 0.17802158689511746
Trained batch 580 in epoch 0, gen_loss = 0.43685701125254933, disc_loss = 0.1778319507946297
Trained batch 581 in epoch 0, gen_loss = 0.4369565009865974, disc_loss = 0.17756238861067086
Trained batch 582 in epoch 0, gen_loss = 0.43705806708663036, disc_loss = 0.1773436982631939
Trained batch 583 in epoch 0, gen_loss = 0.4370572849189582, disc_loss = 0.17723727609311574
Trained batch 584 in epoch 0, gen_loss = 0.4368861970738468, disc_loss = 0.17718899061855597
Trained batch 585 in epoch 0, gen_loss = 0.43695300644575125, disc_loss = 0.17703624962552517
Trained batch 586 in epoch 0, gen_loss = 0.43704656167574596, disc_loss = 0.17678907535513275
Trained batch 587 in epoch 0, gen_loss = 0.4370008192094816, disc_loss = 0.1766885123469037
Trained batch 588 in epoch 0, gen_loss = 0.4370958879969521, disc_loss = 0.1765647623182001
Trained batch 589 in epoch 0, gen_loss = 0.4370582369424529, disc_loss = 0.1763369492334077
Trained batch 590 in epoch 0, gen_loss = 0.4370231597153265, disc_loss = 0.17645123974434312
Trained batch 591 in epoch 0, gen_loss = 0.437024911865592, disc_loss = 0.17672862883065701
Trained batch 592 in epoch 0, gen_loss = 0.43705065108672364, disc_loss = 0.17653309987641205
Trained batch 593 in epoch 0, gen_loss = 0.4369591894474896, disc_loss = 0.1765544699780919
Trained batch 594 in epoch 0, gen_loss = 0.4371551546229034, disc_loss = 0.17696962289261717
Trained batch 595 in epoch 0, gen_loss = 0.43709326625470346, disc_loss = 0.17707534532231473
Trained batch 596 in epoch 0, gen_loss = 0.43714767484808686, disc_loss = 0.17740090196668304
Trained batch 597 in epoch 0, gen_loss = 0.4372126269699339, disc_loss = 0.1773688449838937
Trained batch 598 in epoch 0, gen_loss = 0.4372837360394817, disc_loss = 0.17745302000917954
Testing Epoch 0
Traceback (most recent call last):
  File "srgan_bones.py", line 333, in <module>
    img_grid1, img_grid2 = utils.make_thickness_images_dif2(imgs_hr[:5], imgs_lr[:5], gen_hr[:5])
  File "/work3/soeba/HALOS/utils.py", line 102, in make_thickness_images_dif2
    img_grid = torch.cat((imgs_hr, imgs_lr, imgs_sr, imgs_dif), -1)
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument tensors in method wrapper_cat)