wandb: WARNING Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.
/work3/soeba/HALOS/halos/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.
  warnings.warn(
/work3/soeba/HALOS/halos/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Training Epoch 0
Trained batch 0 in epoch 0, gen_loss = 0.5563786029815674, disc_loss = 0.7390609383583069
Trained batch 1 in epoch 0, gen_loss = 0.5446664094924927, disc_loss = 0.7806582748889923
Trained batch 2 in epoch 0, gen_loss = 0.5109145243962606, disc_loss = 0.7113451361656189
Trained batch 3 in epoch 0, gen_loss = 0.5234076380729675, disc_loss = 0.6182924956083298
Trained batch 4 in epoch 0, gen_loss = 0.496449863910675, disc_loss = 0.5532835185527801
Trained batch 5 in epoch 0, gen_loss = 0.48672746618588764, disc_loss = 0.5050262461105982
Trained batch 6 in epoch 0, gen_loss = 0.46989273599215914, disc_loss = 0.4778876006603241
Trained batch 7 in epoch 0, gen_loss = 0.467810545116663, disc_loss = 0.43912702798843384
Trained batch 8 in epoch 0, gen_loss = 0.4674016303486294, disc_loss = 0.4120604594548543
Trained batch 9 in epoch 0, gen_loss = 0.46998355388641355, disc_loss = 0.3860507279634476
Trained batch 10 in epoch 0, gen_loss = 0.4687965999950062, disc_loss = 0.360097272829576
Trained batch 11 in epoch 0, gen_loss = 0.46654297411441803, disc_loss = 0.33738767852385837
Trained batch 12 in epoch 0, gen_loss = 0.46335705427023083, disc_loss = 0.31837158707471996
Trained batch 13 in epoch 0, gen_loss = 0.4627660108464105, disc_loss = 0.302440239914826
Trained batch 14 in epoch 0, gen_loss = 0.4613028029600779, disc_loss = 0.28845255772272743
Trained batch 15 in epoch 0, gen_loss = 0.46451047249138355, disc_loss = 0.27667515352368355
Trained batch 16 in epoch 0, gen_loss = 0.46701430748490724, disc_loss = 0.2650366416748832
Trained batch 17 in epoch 0, gen_loss = 0.4630872640344832, disc_loss = 0.2546786094705264
Trained batch 18 in epoch 0, gen_loss = 0.46280316302650853, disc_loss = 0.24748315897427106
Trained batch 19 in epoch 0, gen_loss = 0.46436749398708344, disc_loss = 0.2398317947983742
Trained batch 20 in epoch 0, gen_loss = 0.46410817617461797, disc_loss = 0.2325592544816789
Trained batch 21 in epoch 0, gen_loss = 0.4593235823241147, disc_loss = 0.2275558351115747
Trained batch 22 in epoch 0, gen_loss = 0.45980424466340436, disc_loss = 0.22285279890765314
Trained batch 23 in epoch 0, gen_loss = 0.4586757818857829, disc_loss = 0.22464223702748617
Trained batch 24 in epoch 0, gen_loss = 0.45741663098335267, disc_loss = 0.227794873714447
Trained batch 25 in epoch 0, gen_loss = 0.4544558742871651, disc_loss = 0.23046111372800973
Trained batch 26 in epoch 0, gen_loss = 0.4538333471174593, disc_loss = 0.23338365996325458
Trained batch 27 in epoch 0, gen_loss = 0.45229250511952807, disc_loss = 0.22944419219025544
Trained batch 28 in epoch 0, gen_loss = 0.4522928359179661, disc_loss = 0.22722292177636047
Trained batch 29 in epoch 0, gen_loss = 0.45209989448388416, disc_loss = 0.22372718552748364
Trained batch 30 in epoch 0, gen_loss = 0.4553137661949281, disc_loss = 0.22002347174190706
Trained batch 31 in epoch 0, gen_loss = 0.4572939435020089, disc_loss = 0.2155164044816047
Trained batch 32 in epoch 0, gen_loss = 0.45802203994808777, disc_loss = 0.2109591064579559
Trained batch 33 in epoch 0, gen_loss = 0.4564897242714377, disc_loss = 0.2067147992989596
Trained batch 34 in epoch 0, gen_loss = 0.4554062443120139, disc_loss = 0.2023207343050412
Trained batch 35 in epoch 0, gen_loss = 0.45615508572922814, disc_loss = 0.19810289392868677
Trained batch 36 in epoch 0, gen_loss = 0.45738632856188594, disc_loss = 0.19400655095641678
Trained batch 37 in epoch 0, gen_loss = 0.45831537168276937, disc_loss = 0.19006742537021637
Trained batch 38 in epoch 0, gen_loss = 0.45898436659421676, disc_loss = 0.18616265965959963
Trained batch 39 in epoch 0, gen_loss = 0.45864233672618865, disc_loss = 0.18244018154218794
Trained batch 40 in epoch 0, gen_loss = 0.45746548510179286, disc_loss = 0.17927960942431195
Trained batch 41 in epoch 0, gen_loss = 0.4587525547969909, disc_loss = 0.1763743098293032
Trained batch 42 in epoch 0, gen_loss = 0.46131660841232125, disc_loss = 0.1732121154146139
Trained batch 43 in epoch 0, gen_loss = 0.46095429360866547, disc_loss = 0.17032308300787752
Trained batch 44 in epoch 0, gen_loss = 0.45986618995666506, disc_loss = 0.168721690442827
Trained batch 45 in epoch 0, gen_loss = 0.46191270973371423, disc_loss = 0.16634528104053892
Trained batch 46 in epoch 0, gen_loss = 0.46378771675393937, disc_loss = 0.16372312605381012
Trained batch 47 in epoch 0, gen_loss = 0.46361105392376584, disc_loss = 0.16197569916645685
Trained batch 48 in epoch 0, gen_loss = 0.46291972362265293, disc_loss = 0.16152467958781183
Trained batch 49 in epoch 0, gen_loss = 0.4622130846977234, disc_loss = 0.1628292778134346
Trained batch 50 in epoch 0, gen_loss = 0.46230976312768224, disc_loss = 0.16262895686953677
Trained batch 51 in epoch 0, gen_loss = 0.46336355060338974, disc_loss = 0.1603742345737723
Trained batch 52 in epoch 0, gen_loss = 0.4637236679500004, disc_loss = 0.15818997226514905
Trained batch 53 in epoch 0, gen_loss = 0.4646121931296808, disc_loss = 0.1561070637156566
Trained batch 54 in epoch 0, gen_loss = 0.46464246240529145, disc_loss = 0.15399752171202138
Trained batch 55 in epoch 0, gen_loss = 0.46544369576232775, disc_loss = 0.1519534174086792
Trained batch 56 in epoch 0, gen_loss = 0.464109035960415, disc_loss = 0.15035594313552506
Trained batch 57 in epoch 0, gen_loss = 0.46382210840438975, disc_loss = 0.1485626470811408
Trained batch 58 in epoch 0, gen_loss = 0.4639078186730207, disc_loss = 0.1470716684046438
Trained batch 59 in epoch 0, gen_loss = 0.464421055217584, disc_loss = 0.14529831980665525
Trained batch 60 in epoch 0, gen_loss = 0.4655001393107117, disc_loss = 0.14350362476266798
Trained batch 61 in epoch 0, gen_loss = 0.4664587085285494, disc_loss = 0.14169523528506678
Trained batch 62 in epoch 0, gen_loss = 0.4660052652396853, disc_loss = 0.13999783602498828
Trained batch 63 in epoch 0, gen_loss = 0.46603235229849815, disc_loss = 0.13828572275815532
Trained batch 64 in epoch 0, gen_loss = 0.46576390679066, disc_loss = 0.13662145653596292
Trained batch 65 in epoch 0, gen_loss = 0.4655822601282235, disc_loss = 0.13505453949399066
Trained batch 66 in epoch 0, gen_loss = 0.4657906075911735, disc_loss = 0.13376846270107512
Trained batch 67 in epoch 0, gen_loss = 0.4652952573755208, disc_loss = 0.13235722454812596
Trained batch 68 in epoch 0, gen_loss = 0.4650340814521347, disc_loss = 0.13114271107791126
Trained batch 69 in epoch 0, gen_loss = 0.465647497347423, disc_loss = 0.1297520909458399
Trained batch 70 in epoch 0, gen_loss = 0.46657848526054707, disc_loss = 0.12834954697271467
Trained batch 71 in epoch 0, gen_loss = 0.4675341670711835, disc_loss = 0.12696931099829575
Trained batch 72 in epoch 0, gen_loss = 0.46800118194867485, disc_loss = 0.12583970862810742
Trained batch 73 in epoch 0, gen_loss = 0.4694026926079312, disc_loss = 0.12469928593349618
Trained batch 74 in epoch 0, gen_loss = 0.47082990090052285, disc_loss = 0.12353452481329441
Trained batch 75 in epoch 0, gen_loss = 0.4701949397200032, disc_loss = 0.12297940016479085
Trained batch 76 in epoch 0, gen_loss = 0.47063171012060984, disc_loss = 0.12301372318202025
Trained batch 77 in epoch 0, gen_loss = 0.4704761321728046, disc_loss = 0.12540451301118502
Trained batch 78 in epoch 0, gen_loss = 0.4717784244802934, disc_loss = 0.12499516285201416
Trained batch 79 in epoch 0, gen_loss = 0.471526262909174, disc_loss = 0.12442392844241112
Trained batch 80 in epoch 0, gen_loss = 0.4713180591294795, disc_loss = 0.12421922374194787
Trained batch 81 in epoch 0, gen_loss = 0.47082479370803365, disc_loss = 0.1233672315436529
Trained batch 82 in epoch 0, gen_loss = 0.47123807273715373, disc_loss = 0.1224928359355194
Trained batch 83 in epoch 0, gen_loss = 0.4721193742893991, disc_loss = 0.12237192346670088
Trained batch 84 in epoch 0, gen_loss = 0.4728983349659864, disc_loss = 0.12195125855505466
Trained batch 85 in epoch 0, gen_loss = 0.4748326411773992, disc_loss = 0.12120064506090658
Trained batch 86 in epoch 0, gen_loss = 0.4744339044066681, disc_loss = 0.12167590858693096
Trained batch 87 in epoch 0, gen_loss = 0.47532224926081573, disc_loss = 0.12166004596193405
Trained batch 88 in epoch 0, gen_loss = 0.4754118962904041, disc_loss = 0.12259122095164958
Trained batch 89 in epoch 0, gen_loss = 0.4767778022421731, disc_loss = 0.12862236568083366
Trained batch 90 in epoch 0, gen_loss = 0.47737961123277856, disc_loss = 0.13110725370819096
Trained batch 91 in epoch 0, gen_loss = 0.47732190528641577, disc_loss = 0.13189155999166163
Trained batch 92 in epoch 0, gen_loss = 0.4773393821331762, disc_loss = 0.13278477136245978
Trained batch 93 in epoch 0, gen_loss = 0.47755982489027876, disc_loss = 0.1338726645335555
Trained batch 94 in epoch 0, gen_loss = 0.47677269201529654, disc_loss = 0.13499738921068216
Trained batch 95 in epoch 0, gen_loss = 0.4765263417114814, disc_loss = 0.13572755799395964
Trained batch 96 in epoch 0, gen_loss = 0.47587297902893777, disc_loss = 0.1363968001390548
Trained batch 97 in epoch 0, gen_loss = 0.4749183469280905, disc_loss = 0.13717450020948843
Trained batch 98 in epoch 0, gen_loss = 0.47424353854824797, disc_loss = 0.13978732055561108
Trained batch 99 in epoch 0, gen_loss = 0.4745170450210571, disc_loss = 0.1410432914085686
Trained batch 100 in epoch 0, gen_loss = 0.4743510071593936, disc_loss = 0.14098735524359907
Trained batch 101 in epoch 0, gen_loss = 0.47454659582353104, disc_loss = 0.14072210160905824
Trained batch 102 in epoch 0, gen_loss = 0.47483050562803025, disc_loss = 0.14071427598523284
Trained batch 103 in epoch 0, gen_loss = 0.47509437885421973, disc_loss = 0.14064711971709934
Trained batch 104 in epoch 0, gen_loss = 0.4749490936597188, disc_loss = 0.1405630106904677
Trained batch 105 in epoch 0, gen_loss = 0.4747007477958247, disc_loss = 0.1412510082723116
Trained batch 106 in epoch 0, gen_loss = 0.4753304937175501, disc_loss = 0.14254760639480898
Trained batch 107 in epoch 0, gen_loss = 0.4749254622945079, disc_loss = 0.14296723662496166
Trained batch 108 in epoch 0, gen_loss = 0.4742188863798019, disc_loss = 0.14506363740549721
Trained batch 109 in epoch 0, gen_loss = 0.47415786493908274, disc_loss = 0.14554518320682375
Trained batch 110 in epoch 0, gen_loss = 0.4735541475248766, disc_loss = 0.14565278987425404
Trained batch 111 in epoch 0, gen_loss = 0.4726105446794203, disc_loss = 0.14579316503035702
Trained batch 112 in epoch 0, gen_loss = 0.47236953240580265, disc_loss = 0.14549940305275727
Trained batch 113 in epoch 0, gen_loss = 0.4716302156448364, disc_loss = 0.1450376528429619
Trained batch 114 in epoch 0, gen_loss = 0.47123816039251243, disc_loss = 0.1442405258021925
Trained batch 115 in epoch 0, gen_loss = 0.4711679099448796, disc_loss = 0.14330556177823195
Trained batch 116 in epoch 0, gen_loss = 0.470253935481748, disc_loss = 0.14255898205451986
Trained batch 117 in epoch 0, gen_loss = 0.4707004677946285, disc_loss = 0.14205210575423502
Trained batch 118 in epoch 0, gen_loss = 0.47049292245832813, disc_loss = 0.1419849414065355
Trained batch 119 in epoch 0, gen_loss = 0.47094760040442146, disc_loss = 0.14355322105499604
Trained batch 120 in epoch 0, gen_loss = 0.4705195653537088, disc_loss = 0.1440088140108615
Trained batch 121 in epoch 0, gen_loss = 0.47001762004172215, disc_loss = 0.1440405984424421
Trained batch 122 in epoch 0, gen_loss = 0.4701860961390705, disc_loss = 0.1438887493185154
Trained batch 123 in epoch 0, gen_loss = 0.4702144536760546, disc_loss = 0.14368503199770086
Trained batch 124 in epoch 0, gen_loss = 0.47032483768463135, disc_loss = 0.14333394069969654
Trained batch 125 in epoch 0, gen_loss = 0.4696125019164312, disc_loss = 0.14544200632602922
Trained batch 126 in epoch 0, gen_loss = 0.469861541676709, disc_loss = 0.1474641873171245
Trained batch 127 in epoch 0, gen_loss = 0.470543394330889, disc_loss = 0.1476071914221393
Trained batch 128 in epoch 0, gen_loss = 0.4699515716512074, disc_loss = 0.14817535699626735
Trained batch 129 in epoch 0, gen_loss = 0.46904963392477766, disc_loss = 0.14800077248364688
Trained batch 130 in epoch 0, gen_loss = 0.46836419733426043, disc_loss = 0.14786587810527732
Trained batch 131 in epoch 0, gen_loss = 0.4678765813058073, disc_loss = 0.14742499675996829
Trained batch 132 in epoch 0, gen_loss = 0.4673120587840116, disc_loss = 0.14703360257348172
Trained batch 133 in epoch 0, gen_loss = 0.46591519975840157, disc_loss = 0.14681601047571471
Trained batch 134 in epoch 0, gen_loss = 0.46505955634293733, disc_loss = 0.14711375701482649
Trained batch 135 in epoch 0, gen_loss = 0.4656017869710922, disc_loss = 0.14666727164705448
Trained batch 136 in epoch 0, gen_loss = 0.46559564736637754, disc_loss = 0.14661777749603247
Trained batch 137 in epoch 0, gen_loss = 0.4652576554512632, disc_loss = 0.14672712324376125
Trained batch 138 in epoch 0, gen_loss = 0.4653210224007531, disc_loss = 0.14645691908788766
Trained batch 139 in epoch 0, gen_loss = 0.4655756196805409, disc_loss = 0.1464519084165139
Trained batch 140 in epoch 0, gen_loss = 0.46522921957868213, disc_loss = 0.1464480361009532
Trained batch 141 in epoch 0, gen_loss = 0.46481986994474705, disc_loss = 0.14624518094400704
Trained batch 142 in epoch 0, gen_loss = 0.46423259856817606, disc_loss = 0.14622400608565006
Trained batch 143 in epoch 0, gen_loss = 0.4650867349571652, disc_loss = 0.14649160756056923
Trained batch 144 in epoch 0, gen_loss = 0.46587797773295436, disc_loss = 0.1479310026585028
Trained batch 145 in epoch 0, gen_loss = 0.46546636399340957, disc_loss = 0.14777618433565717
Trained batch 146 in epoch 0, gen_loss = 0.46495234702720123, disc_loss = 0.14749300699098175
Trained batch 147 in epoch 0, gen_loss = 0.4650717343430261, disc_loss = 0.1473917866100532
Trained batch 148 in epoch 0, gen_loss = 0.4650905578328459, disc_loss = 0.14717986080920537
Trained batch 149 in epoch 0, gen_loss = 0.4654374676942825, disc_loss = 0.1470639511073629
Trained batch 150 in epoch 0, gen_loss = 0.4647611010548295, disc_loss = 0.14705410758835197
Trained batch 151 in epoch 0, gen_loss = 0.4646994546055794, disc_loss = 0.14683788256278554
Trained batch 152 in epoch 0, gen_loss = 0.46418581776369633, disc_loss = 0.1467090493153318
Trained batch 153 in epoch 0, gen_loss = 0.4641752645566866, disc_loss = 0.14625254560068443
Trained batch 154 in epoch 0, gen_loss = 0.46394005898506413, disc_loss = 0.14645912807074285
Trained batch 155 in epoch 0, gen_loss = 0.46372591971586913, disc_loss = 0.14767130241036797
Trained batch 156 in epoch 0, gen_loss = 0.4638220128739715, disc_loss = 0.14800365540869298
Trained batch 157 in epoch 0, gen_loss = 0.4633361702855629, disc_loss = 0.14796924219714314
Trained batch 158 in epoch 0, gen_loss = 0.4629291142307737, disc_loss = 0.14788402444770876
Trained batch 159 in epoch 0, gen_loss = 0.4631343038752675, disc_loss = 0.14773931285599246
Trained batch 160 in epoch 0, gen_loss = 0.4629121698577952, disc_loss = 0.14752632283507297
Trained batch 161 in epoch 0, gen_loss = 0.4633111845196029, disc_loss = 0.14732774665556203
Trained batch 162 in epoch 0, gen_loss = 0.46343353774649965, disc_loss = 0.14742262895716116
Trained batch 163 in epoch 0, gen_loss = 0.46288349788363387, disc_loss = 0.14768902045424756
Trained batch 164 in epoch 0, gen_loss = 0.46282422361951886, disc_loss = 0.14732910924111353
Trained batch 165 in epoch 0, gen_loss = 0.4627532962575016, disc_loss = 0.14706042208091682
Trained batch 166 in epoch 0, gen_loss = 0.46294612174262545, disc_loss = 0.14699838561435302
Trained batch 167 in epoch 0, gen_loss = 0.4629603076194014, disc_loss = 0.1473212683506842
Trained batch 168 in epoch 0, gen_loss = 0.4625892873699143, disc_loss = 0.14807570382908603
Trained batch 169 in epoch 0, gen_loss = 0.46252355417784524, disc_loss = 0.14812016326057562
Trained batch 170 in epoch 0, gen_loss = 0.46217498584100375, disc_loss = 0.14807753549202493
Trained batch 171 in epoch 0, gen_loss = 0.4619477122675541, disc_loss = 0.1482558730058372
Trained batch 172 in epoch 0, gen_loss = 0.4620482184294331, disc_loss = 0.14899747905918972
Trained batch 173 in epoch 0, gen_loss = 0.4621756410804288, disc_loss = 0.15046563741333527
Trained batch 174 in epoch 0, gen_loss = 0.4618693540777479, disc_loss = 0.15054179801472595
Trained batch 175 in epoch 0, gen_loss = 0.4618905960497531, disc_loss = 0.15046668422027407
Trained batch 176 in epoch 0, gen_loss = 0.462074886607585, disc_loss = 0.15034373859101433
Trained batch 177 in epoch 0, gen_loss = 0.46247068281923787, disc_loss = 0.1501765414925941
Trained batch 178 in epoch 0, gen_loss = 0.46238300404069144, disc_loss = 0.15037932755768965
Trained batch 179 in epoch 0, gen_loss = 0.46199004484547507, disc_loss = 0.15037191093174948
Trained batch 180 in epoch 0, gen_loss = 0.46144659841916835, disc_loss = 0.15038605352338835
Trained batch 181 in epoch 0, gen_loss = 0.4615457955297533, disc_loss = 0.14984165612035072
Trained batch 182 in epoch 0, gen_loss = 0.4614248883203079, disc_loss = 0.14931754601987007
Trained batch 183 in epoch 0, gen_loss = 0.4610447561287362, disc_loss = 0.14886292945796056
Trained batch 184 in epoch 0, gen_loss = 0.46103622317314147, disc_loss = 0.14829377053758583
Trained batch 185 in epoch 0, gen_loss = 0.46092230802582157, disc_loss = 0.14786334801465273
Trained batch 186 in epoch 0, gen_loss = 0.46109258570773076, disc_loss = 0.14770782026736176
Trained batch 187 in epoch 0, gen_loss = 0.46084556696896856, disc_loss = 0.1495510270085899
Trained batch 188 in epoch 0, gen_loss = 0.4605771989103348, disc_loss = 0.1493660198652713
Trained batch 189 in epoch 0, gen_loss = 0.46046166121959686, disc_loss = 0.14923313602216934
Trained batch 190 in epoch 0, gen_loss = 0.46043988859466234, disc_loss = 0.14906840971354102
Trained batch 191 in epoch 0, gen_loss = 0.46054333153491217, disc_loss = 0.14888035503099672
Trained batch 192 in epoch 0, gen_loss = 0.4605608187191227, disc_loss = 0.1485743184569123
Trained batch 193 in epoch 0, gen_loss = 0.4611188087266745, disc_loss = 0.14816988583116494
Trained batch 194 in epoch 0, gen_loss = 0.4607967708355341, disc_loss = 0.1478884646239189
Trained batch 195 in epoch 0, gen_loss = 0.46031700667678094, disc_loss = 0.14765523652527102
Trained batch 196 in epoch 0, gen_loss = 0.45994340390118243, disc_loss = 0.14886777357288122
Trained batch 197 in epoch 0, gen_loss = 0.46059036149521065, disc_loss = 0.1494034223653602
Trained batch 198 in epoch 0, gen_loss = 0.4603616261302526, disc_loss = 0.14970321635991785
Trained batch 199 in epoch 0, gen_loss = 0.460139996856451, disc_loss = 0.149537052558735
Trained batch 200 in epoch 0, gen_loss = 0.46007402278297577, disc_loss = 0.1492591356182128
Trained batch 201 in epoch 0, gen_loss = 0.4593631440755164, disc_loss = 0.14944638618922765
Trained batch 202 in epoch 0, gen_loss = 0.45890744448882603, disc_loss = 0.15008392692382994
Trained batch 203 in epoch 0, gen_loss = 0.45857616879192054, disc_loss = 0.15001711407311114
Trained batch 204 in epoch 0, gen_loss = 0.45876037115004004, disc_loss = 0.14984894272212576
Trained batch 205 in epoch 0, gen_loss = 0.45860223211709733, disc_loss = 0.14977039701477127
Trained batch 206 in epoch 0, gen_loss = 0.4580827466243707, disc_loss = 0.14985555692031477
Trained batch 207 in epoch 0, gen_loss = 0.45766326025701487, disc_loss = 0.14994004391169605
Trained batch 208 in epoch 0, gen_loss = 0.4577321018043317, disc_loss = 0.15035662455172344
Trained batch 209 in epoch 0, gen_loss = 0.45707603891690574, disc_loss = 0.1505911876137058
Trained batch 210 in epoch 0, gen_loss = 0.45654997497938254, disc_loss = 0.1507119120526794
Trained batch 211 in epoch 0, gen_loss = 0.45650088238828584, disc_loss = 0.15051656357838578
Trained batch 212 in epoch 0, gen_loss = 0.4563087262737919, disc_loss = 0.15033246329307837
Trained batch 213 in epoch 0, gen_loss = 0.4560040448592088, disc_loss = 0.15052046519747683
Trained batch 214 in epoch 0, gen_loss = 0.45626485084378443, disc_loss = 0.15062729907763558
Trained batch 215 in epoch 0, gen_loss = 0.45669459786128114, disc_loss = 0.15065468438142152
Trained batch 216 in epoch 0, gen_loss = 0.4563469605237108, disc_loss = 0.15167463173411683
Trained batch 217 in epoch 0, gen_loss = 0.45610421991676364, disc_loss = 0.15172870160687132
Trained batch 218 in epoch 0, gen_loss = 0.45634911090271657, disc_loss = 0.15172361828318742
Trained batch 219 in epoch 0, gen_loss = 0.4561917218295011, disc_loss = 0.15261269066990776
Trained batch 220 in epoch 0, gen_loss = 0.4557496709791244, disc_loss = 0.15347159425496246
Trained batch 221 in epoch 0, gen_loss = 0.45528530161659997, disc_loss = 0.15381268664422604
Trained batch 222 in epoch 0, gen_loss = 0.45502287389986185, disc_loss = 0.1544581308643513
Trained batch 223 in epoch 0, gen_loss = 0.4548483320644924, disc_loss = 0.1548262216965668
Trained batch 224 in epoch 0, gen_loss = 0.45499535746044584, disc_loss = 0.15507926223178706
Trained batch 225 in epoch 0, gen_loss = 0.45493716858657063, disc_loss = 0.15529198005828446
Trained batch 226 in epoch 0, gen_loss = 0.4544848068432661, disc_loss = 0.15584818615951465
Trained batch 227 in epoch 0, gen_loss = 0.45400509303599074, disc_loss = 0.1562879914009388
Trained batch 228 in epoch 0, gen_loss = 0.4535496747649913, disc_loss = 0.15678563724601374
Trained batch 229 in epoch 0, gen_loss = 0.4531136186226555, disc_loss = 0.1573856388502147
Trained batch 230 in epoch 0, gen_loss = 0.45302920772399735, disc_loss = 0.1576548782278062
Trained batch 231 in epoch 0, gen_loss = 0.45308590192219306, disc_loss = 0.15788676516666753
Trained batch 232 in epoch 0, gen_loss = 0.4531415703470615, disc_loss = 0.15787284866838752
Trained batch 233 in epoch 0, gen_loss = 0.45277716372257626, disc_loss = 0.15808063259141314
Trained batch 234 in epoch 0, gen_loss = 0.4524350180270824, disc_loss = 0.1583409097996798
Trained batch 235 in epoch 0, gen_loss = 0.4520980076769651, disc_loss = 0.1585482167379962
Trained batch 236 in epoch 0, gen_loss = 0.4516323563418811, disc_loss = 0.15870310728465706
Trained batch 237 in epoch 0, gen_loss = 0.4514343106195706, disc_loss = 0.15894796045794457
Trained batch 238 in epoch 0, gen_loss = 0.45131194429417537, disc_loss = 0.15906319979999106
Trained batch 239 in epoch 0, gen_loss = 0.45091657936573026, disc_loss = 0.15916455950743208
Trained batch 240 in epoch 0, gen_loss = 0.4508089083111632, disc_loss = 0.15924097830236453
Trained batch 241 in epoch 0, gen_loss = 0.45079869130426203, disc_loss = 0.15918383769660202
Trained batch 242 in epoch 0, gen_loss = 0.45047061053323156, disc_loss = 0.1590963387823522
Trained batch 243 in epoch 0, gen_loss = 0.4500965925513721, disc_loss = 0.1594566695895962
Trained batch 244 in epoch 0, gen_loss = 0.4499734923547628, disc_loss = 0.1602450300616269
Trained batch 245 in epoch 0, gen_loss = 0.44956568489229776, disc_loss = 0.16061927515225924
Trained batch 246 in epoch 0, gen_loss = 0.4497112552164055, disc_loss = 0.16048655424432956
Trained batch 247 in epoch 0, gen_loss = 0.4495761071241671, disc_loss = 0.16077167640680506
Trained batch 248 in epoch 0, gen_loss = 0.4494223820875926, disc_loss = 0.16082572240576926
Trained batch 249 in epoch 0, gen_loss = 0.44934463715553286, disc_loss = 0.16082997313886882
Trained batch 250 in epoch 0, gen_loss = 0.4490604414882888, disc_loss = 0.16099987233213456
Trained batch 251 in epoch 0, gen_loss = 0.44883779696528875, disc_loss = 0.16108094703494794
Trained batch 252 in epoch 0, gen_loss = 0.44868315479500964, disc_loss = 0.161628741086295
Trained batch 253 in epoch 0, gen_loss = 0.44834916692549787, disc_loss = 0.16206093758021986
Trained batch 254 in epoch 0, gen_loss = 0.4479583867624694, disc_loss = 0.1620415962575113
Trained batch 255 in epoch 0, gen_loss = 0.4479687550337985, disc_loss = 0.1622045807671384
Trained batch 256 in epoch 0, gen_loss = 0.44798412385617714, disc_loss = 0.1621007799073068
Trained batch 257 in epoch 0, gen_loss = 0.44797178121038184, disc_loss = 0.1619812513290103
Trained batch 258 in epoch 0, gen_loss = 0.44758634079377163, disc_loss = 0.16221226375679482
Trained batch 259 in epoch 0, gen_loss = 0.4475107870422877, disc_loss = 0.16312710789677043
Trained batch 260 in epoch 0, gen_loss = 0.4471215030242657, disc_loss = 0.1638073218750885
Trained batch 261 in epoch 0, gen_loss = 0.44683235724463716, disc_loss = 0.16387230565216931
Trained batch 262 in epoch 0, gen_loss = 0.4463123944322419, disc_loss = 0.1640227450404897
Trained batch 263 in epoch 0, gen_loss = 0.4458100295653849, disc_loss = 0.16419349411135595
Trained batch 264 in epoch 0, gen_loss = 0.4452857326786473, disc_loss = 0.1641979690219434
Trained batch 265 in epoch 0, gen_loss = 0.44501386683686334, disc_loss = 0.16413961309603506
Trained batch 266 in epoch 0, gen_loss = 0.4448221100403575, disc_loss = 0.1641682798063822
Trained batch 267 in epoch 0, gen_loss = 0.44459547569502644, disc_loss = 0.16428882381948295
Trained batch 268 in epoch 0, gen_loss = 0.4442426780564191, disc_loss = 0.16430307108368794
Trained batch 269 in epoch 0, gen_loss = 0.4439866195122401, disc_loss = 0.16427496890364973
Trained batch 270 in epoch 0, gen_loss = 0.4436630606651306, disc_loss = 0.16431744048213826
Trained batch 271 in epoch 0, gen_loss = 0.44360009657547755, disc_loss = 0.1642840256270788
Trained batch 272 in epoch 0, gen_loss = 0.4433308047252697, disc_loss = 0.16484153885462563
Trained batch 273 in epoch 0, gen_loss = 0.4431472673033276, disc_loss = 0.164931192103583
Trained batch 274 in epoch 0, gen_loss = 0.44286809075962413, disc_loss = 0.16493625183674424
Trained batch 275 in epoch 0, gen_loss = 0.4426922366239022, disc_loss = 0.16512616581377992
Trained batch 276 in epoch 0, gen_loss = 0.4424567682002856, disc_loss = 0.16522895443229685
Trained batch 277 in epoch 0, gen_loss = 0.44208447174202625, disc_loss = 0.16532547641277529
Trained batch 278 in epoch 0, gen_loss = 0.44176181071975323, disc_loss = 0.16545669150315093
Trained batch 279 in epoch 0, gen_loss = 0.44157619327306746, disc_loss = 0.16546295144861298
Trained batch 280 in epoch 0, gen_loss = 0.44118083721802326, disc_loss = 0.16547423821883905
Trained batch 281 in epoch 0, gen_loss = 0.4412299247709572, disc_loss = 0.16536470949781915
Trained batch 282 in epoch 0, gen_loss = 0.4409505700685953, disc_loss = 0.16534069167983195
Trained batch 283 in epoch 0, gen_loss = 0.44057966380471913, disc_loss = 0.16542722067703874
Trained batch 284 in epoch 0, gen_loss = 0.4402999824599216, disc_loss = 0.16563218845740746
Trained batch 285 in epoch 0, gen_loss = 0.440001165846011, disc_loss = 0.16554074888801448
Trained batch 286 in epoch 0, gen_loss = 0.4395288222758197, disc_loss = 0.1654509690489607
Trained batch 287 in epoch 0, gen_loss = 0.4394247771965133, disc_loss = 0.16550082908037844
Trained batch 288 in epoch 0, gen_loss = 0.4396480410569267, disc_loss = 0.1661340684845889
Trained batch 289 in epoch 0, gen_loss = 0.4393282056882464, disc_loss = 0.16624663161948838
Trained batch 290 in epoch 0, gen_loss = 0.4392515213014334, disc_loss = 0.1665721667604041
Trained batch 291 in epoch 0, gen_loss = 0.43907474512106753, disc_loss = 0.16641394228176914
Trained batch 292 in epoch 0, gen_loss = 0.4389570081803579, disc_loss = 0.16661619082881526
Trained batch 293 in epoch 0, gen_loss = 0.4388933894382853, disc_loss = 0.16682608948298255
Trained batch 294 in epoch 0, gen_loss = 0.43868444834725334, disc_loss = 0.16689549892256825
Trained batch 295 in epoch 0, gen_loss = 0.43850764773181966, disc_loss = 0.16707627254387214
Trained batch 296 in epoch 0, gen_loss = 0.43814479220997204, disc_loss = 0.16735055260611104
Trained batch 297 in epoch 0, gen_loss = 0.437984013517431, disc_loss = 0.1675977183930926
Trained batch 298 in epoch 0, gen_loss = 0.4374722249332479, disc_loss = 0.16776224169070306
Trained batch 299 in epoch 0, gen_loss = 0.4372172948718071, disc_loss = 0.16797307988628746
Trained batch 300 in epoch 0, gen_loss = 0.43727448394528257, disc_loss = 0.1680060068201088
Trained batch 301 in epoch 0, gen_loss = 0.43695078336245174, disc_loss = 0.16797223860484284
Trained batch 302 in epoch 0, gen_loss = 0.43681763845308386, disc_loss = 0.1682878553240311
Trained batch 303 in epoch 0, gen_loss = 0.4370097334643728, disc_loss = 0.16821370372475175
Trained batch 304 in epoch 0, gen_loss = 0.4370856785383381, disc_loss = 0.16807898176131678
Trained batch 305 in epoch 0, gen_loss = 0.43690094147242753, disc_loss = 0.16812148700571722
Trained batch 306 in epoch 0, gen_loss = 0.43698279370314136, disc_loss = 0.16810596004395414
Trained batch 307 in epoch 0, gen_loss = 0.43665729675974163, disc_loss = 0.16836235803467306
Trained batch 308 in epoch 0, gen_loss = 0.4365291171089345, disc_loss = 0.16924536638685223
Trained batch 309 in epoch 0, gen_loss = 0.4363803990425602, disc_loss = 0.1691549965510926
Trained batch 310 in epoch 0, gen_loss = 0.4362353481280459, disc_loss = 0.16948231106618017
Trained batch 311 in epoch 0, gen_loss = 0.43610824988438535, disc_loss = 0.16973106539808214
Trained batch 312 in epoch 0, gen_loss = 0.4360252045594846, disc_loss = 0.1695943008275173
Trained batch 313 in epoch 0, gen_loss = 0.43614871458263155, disc_loss = 0.16968946548617758
Trained batch 314 in epoch 0, gen_loss = 0.4360087620833563, disc_loss = 0.16974245721976908
Trained batch 315 in epoch 0, gen_loss = 0.43588267681719384, disc_loss = 0.16973702185728315
Trained batch 316 in epoch 0, gen_loss = 0.4356775443636657, disc_loss = 0.16964727781760017
Trained batch 317 in epoch 0, gen_loss = 0.4354536789205839, disc_loss = 0.16965876257855375
Trained batch 318 in epoch 0, gen_loss = 0.43525868579511734, disc_loss = 0.16958189590434108
Trained batch 319 in epoch 0, gen_loss = 0.43514678087085484, disc_loss = 0.16937985486001708
Trained batch 320 in epoch 0, gen_loss = 0.4348503297363115, disc_loss = 0.1694038498311121
Trained batch 321 in epoch 0, gen_loss = 0.43446414903824376, disc_loss = 0.16929309454787037
Trained batch 322 in epoch 0, gen_loss = 0.43440609063157354, disc_loss = 0.1690824410183234
Trained batch 323 in epoch 0, gen_loss = 0.43451899225697105, disc_loss = 0.1690065540706762
Trained batch 324 in epoch 0, gen_loss = 0.4342049321761498, disc_loss = 0.16898374466368785
Trained batch 325 in epoch 0, gen_loss = 0.43371923834633974, disc_loss = 0.16908052335693244
Trained batch 326 in epoch 0, gen_loss = 0.43347033912982413, disc_loss = 0.16889796714193048
Trained batch 327 in epoch 0, gen_loss = 0.4333653193784923, disc_loss = 0.16878783998715624
Trained batch 328 in epoch 0, gen_loss = 0.4332738739741247, disc_loss = 0.1687558202814639
Trained batch 329 in epoch 0, gen_loss = 0.4331255960645098, disc_loss = 0.1685859451296203
Trained batch 330 in epoch 0, gen_loss = 0.43282051677069994, disc_loss = 0.16877718211863516
Trained batch 331 in epoch 0, gen_loss = 0.43286396886210843, disc_loss = 0.16860229928742151
Trained batch 332 in epoch 0, gen_loss = 0.4328887905623462, disc_loss = 0.16838654424357521
Trained batch 333 in epoch 0, gen_loss = 0.43297388551834814, disc_loss = 0.16893245427573692
Trained batch 334 in epoch 0, gen_loss = 0.43282608283099844, disc_loss = 0.16885438023774482
Trained batch 335 in epoch 0, gen_loss = 0.43256854354625657, disc_loss = 0.16870415849899428
Trained batch 336 in epoch 0, gen_loss = 0.43239513968147936, disc_loss = 0.1683497382521364
Trained batch 337 in epoch 0, gen_loss = 0.4322631869619415, disc_loss = 0.16804844128331664
Trained batch 338 in epoch 0, gen_loss = 0.4321430587487235, disc_loss = 0.1679372818739453
Trained batch 339 in epoch 0, gen_loss = 0.4323582368738511, disc_loss = 0.16767391639706844
Trained batch 340 in epoch 0, gen_loss = 0.4321534582072339, disc_loss = 0.1676407086982493
Trained batch 341 in epoch 0, gen_loss = 0.43186937943536635, disc_loss = 0.1684682172565171
Trained batch 342 in epoch 0, gen_loss = 0.4315827174367432, disc_loss = 0.16852114771549798
Trained batch 343 in epoch 0, gen_loss = 0.4317558369664259, disc_loss = 0.16960721162451042
Trained batch 344 in epoch 0, gen_loss = 0.4314481972784236, disc_loss = 0.16973329223692418
Trained batch 345 in epoch 0, gen_loss = 0.4312963784602336, disc_loss = 0.1699471418837333
Trained batch 346 in epoch 0, gen_loss = 0.43125030072003345, disc_loss = 0.16975480518290392
Trained batch 347 in epoch 0, gen_loss = 0.4310573352308109, disc_loss = 0.16984800538904538
Trained batch 348 in epoch 0, gen_loss = 0.43096273266483515, disc_loss = 0.16975637218437256
Trained batch 349 in epoch 0, gen_loss = 0.4307755840676171, disc_loss = 0.1698061581434948
Trained batch 350 in epoch 0, gen_loss = 0.4305960862045614, disc_loss = 0.16989040640620595
Trained batch 351 in epoch 0, gen_loss = 0.4305092715234919, disc_loss = 0.16997823075243187
Trained batch 352 in epoch 0, gen_loss = 0.43052250864485503, disc_loss = 0.17001129681028151
Trained batch 353 in epoch 0, gen_loss = 0.43029641347416375, disc_loss = 0.1700038816306298
Trained batch 354 in epoch 0, gen_loss = 0.4302023235341193, disc_loss = 0.1701327977308505
Trained batch 355 in epoch 0, gen_loss = 0.4299349070767338, disc_loss = 0.16996681496103325
Trained batch 356 in epoch 0, gen_loss = 0.42988997138514906, disc_loss = 0.16989307671639264
Trained batch 357 in epoch 0, gen_loss = 0.42965982080171894, disc_loss = 0.16981499049521193
Trained batch 358 in epoch 0, gen_loss = 0.4295755850571444, disc_loss = 0.16962712947642405
Trained batch 359 in epoch 0, gen_loss = 0.429470773289601, disc_loss = 0.16988701425596245
Trained batch 360 in epoch 0, gen_loss = 0.42948523072985073, disc_loss = 0.16976725062938444
Trained batch 361 in epoch 0, gen_loss = 0.4293915845906537, disc_loss = 0.16954868351165925
Trained batch 362 in epoch 0, gen_loss = 0.4292360111537387, disc_loss = 0.16948161648263124
Trained batch 363 in epoch 0, gen_loss = 0.4293440952078327, disc_loss = 0.16945982706518128
Trained batch 364 in epoch 0, gen_loss = 0.4294098669535493, disc_loss = 0.1697105285086452
Trained batch 365 in epoch 0, gen_loss = 0.4292294103754023, disc_loss = 0.1697881717916802
Trained batch 366 in epoch 0, gen_loss = 0.42899726067316957, disc_loss = 0.16973401482752462
Trained batch 367 in epoch 0, gen_loss = 0.42892761074978375, disc_loss = 0.16954565609542085
Trained batch 368 in epoch 0, gen_loss = 0.42895341339473153, disc_loss = 0.16955892457487945
Trained batch 369 in epoch 0, gen_loss = 0.42870938914853174, disc_loss = 0.16954271409257843
Trained batch 370 in epoch 0, gen_loss = 0.4283772547932648, disc_loss = 0.1698337762272663
Trained batch 371 in epoch 0, gen_loss = 0.4285457735901238, disc_loss = 0.1697125765795429
Trained batch 372 in epoch 0, gen_loss = 0.4284793230387864, disc_loss = 0.16972900136566194
Trained batch 373 in epoch 0, gen_loss = 0.42853161254349875, disc_loss = 0.16951847720373284
Trained batch 374 in epoch 0, gen_loss = 0.4283883012930552, disc_loss = 0.16921580962836744
Trained batch 375 in epoch 0, gen_loss = 0.4281534852975227, disc_loss = 0.16912035061977804
Trained batch 376 in epoch 0, gen_loss = 0.42807191120218535, disc_loss = 0.16912565928258852
Trained batch 377 in epoch 0, gen_loss = 0.42809978032869006, disc_loss = 0.16898908880514601
Trained batch 378 in epoch 0, gen_loss = 0.4278373382494129, disc_loss = 0.1693633226309178
Trained batch 379 in epoch 0, gen_loss = 0.427838046378211, disc_loss = 0.16967264030030685
Trained batch 380 in epoch 0, gen_loss = 0.42763802432638454, disc_loss = 0.1696814588097528
Trained batch 381 in epoch 0, gen_loss = 0.42754066185489376, disc_loss = 0.1696769219155165
Trained batch 382 in epoch 0, gen_loss = 0.42767144587892775, disc_loss = 0.16985740234914865
Trained batch 383 in epoch 0, gen_loss = 0.42756675959875184, disc_loss = 0.16968255085036313
Trained batch 384 in epoch 0, gen_loss = 0.42726806146757945, disc_loss = 0.1697964103819875
Trained batch 385 in epoch 0, gen_loss = 0.42705012834751543, disc_loss = 0.16975144106277068
Trained batch 386 in epoch 0, gen_loss = 0.42720240462349984, disc_loss = 0.1695749608785376
Trained batch 387 in epoch 0, gen_loss = 0.4271283868661861, disc_loss = 0.16965828314135523
Trained batch 388 in epoch 0, gen_loss = 0.42692031705594, disc_loss = 0.16967160023442424
Trained batch 389 in epoch 0, gen_loss = 0.4268344269349025, disc_loss = 0.16956218825414396
Trained batch 390 in epoch 0, gen_loss = 0.42661193691556104, disc_loss = 0.16953523278407887
Trained batch 391 in epoch 0, gen_loss = 0.42640284707351606, disc_loss = 0.16951970555059306
Trained batch 392 in epoch 0, gen_loss = 0.4261722278048974, disc_loss = 0.16964516688716472
Trained batch 393 in epoch 0, gen_loss = 0.42609273630955496, disc_loss = 0.16989301939773832
Trained batch 394 in epoch 0, gen_loss = 0.4258241946938672, disc_loss = 0.16986659007453467
Trained batch 395 in epoch 0, gen_loss = 0.42573789996330186, disc_loss = 0.17019604098032973
Trained batch 396 in epoch 0, gen_loss = 0.425237818313786, disc_loss = 0.1703517365712738
Trained batch 397 in epoch 0, gen_loss = 0.42526144455725223, disc_loss = 0.17024583699434398
Trained batch 398 in epoch 0, gen_loss = 0.4250068149919199, disc_loss = 0.17023861460985248
Trained batch 399 in epoch 0, gen_loss = 0.4248065363615751, disc_loss = 0.17016872838605196
Trained batch 400 in epoch 0, gen_loss = 0.42483004979659195, disc_loss = 0.1700304517025439
Trained batch 401 in epoch 0, gen_loss = 0.42466144152541657, disc_loss = 0.1699524428463768
Trained batch 402 in epoch 0, gen_loss = 0.42434110771633554, disc_loss = 0.17010275461704055
Trained batch 403 in epoch 0, gen_loss = 0.42430368508442795, disc_loss = 0.1699482710469551
Trained batch 404 in epoch 0, gen_loss = 0.42443358184378827, disc_loss = 0.1699086486971305
Trained batch 405 in epoch 0, gen_loss = 0.4243091592941378, disc_loss = 0.16968959577730283
Trained batch 406 in epoch 0, gen_loss = 0.42427401299558637, disc_loss = 0.16949485098206293
Trained batch 407 in epoch 0, gen_loss = 0.4242400630724196, disc_loss = 0.16947220436151267
Trained batch 408 in epoch 0, gen_loss = 0.4243126870076056, disc_loss = 0.16966766835370245
Trained batch 409 in epoch 0, gen_loss = 0.42454886014868576, disc_loss = 0.16943631926778613
Trained batch 410 in epoch 0, gen_loss = 0.4244357054541001, disc_loss = 0.16919533190954195
Trained batch 411 in epoch 0, gen_loss = 0.4242939907804276, disc_loss = 0.1690748147169146
Trained batch 412 in epoch 0, gen_loss = 0.4241632176224891, disc_loss = 0.16879303373179916
Trained batch 413 in epoch 0, gen_loss = 0.42428984842150685, disc_loss = 0.1684849768076182
Trained batch 414 in epoch 0, gen_loss = 0.42415410352040483, disc_loss = 0.16852305834462125
Trained batch 415 in epoch 0, gen_loss = 0.4240769367808333, disc_loss = 0.16862437105289876
Trained batch 416 in epoch 0, gen_loss = 0.42425424110689325, disc_loss = 0.1688141568164102
Trained batch 417 in epoch 0, gen_loss = 0.4242372354537106, disc_loss = 0.16870492699881348
Trained batch 418 in epoch 0, gen_loss = 0.4241405785226025, disc_loss = 0.1686326198075367
Trained batch 419 in epoch 0, gen_loss = 0.42391190514678045, disc_loss = 0.16862526404272232
Trained batch 420 in epoch 0, gen_loss = 0.423957241190301, disc_loss = 0.16840108986191824
Trained batch 421 in epoch 0, gen_loss = 0.4240328427315888, disc_loss = 0.16824274861046348
Trained batch 422 in epoch 0, gen_loss = 0.4237689788741705, disc_loss = 0.16833593685347278
Trained batch 423 in epoch 0, gen_loss = 0.42381875589489937, disc_loss = 0.16845810892640278
Trained batch 424 in epoch 0, gen_loss = 0.4235529811943279, disc_loss = 0.16874400981647128
Trained batch 425 in epoch 0, gen_loss = 0.4232906147208012, disc_loss = 0.16973609852871285
Trained batch 426 in epoch 0, gen_loss = 0.4231989446512709, disc_loss = 0.1696807003822134
Trained batch 427 in epoch 0, gen_loss = 0.42317730745422505, disc_loss = 0.16942060437210113
Trained batch 428 in epoch 0, gen_loss = 0.4230735234586231, disc_loss = 0.16922377004333727
Trained batch 429 in epoch 0, gen_loss = 0.42301123641258065, disc_loss = 0.16910932808681284
Trained batch 430 in epoch 0, gen_loss = 0.4227687722015823, disc_loss = 0.16897336856446935
Trained batch 431 in epoch 0, gen_loss = 0.4225218231203379, disc_loss = 0.1688245288570653
Trained batch 432 in epoch 0, gen_loss = 0.42242140045892834, disc_loss = 0.16889100230576398
Trained batch 433 in epoch 0, gen_loss = 0.4223046026746249, disc_loss = 0.16875560131473338
Trained batch 434 in epoch 0, gen_loss = 0.422355188926061, disc_loss = 0.16866809660247686
Trained batch 435 in epoch 0, gen_loss = 0.4223242385671773, disc_loss = 0.1686896013582997
Trained batch 436 in epoch 0, gen_loss = 0.4222038658314493, disc_loss = 0.16856575768773424
Trained batch 437 in epoch 0, gen_loss = 0.42207143826571775, disc_loss = 0.16841576809327216
Trained batch 438 in epoch 0, gen_loss = 0.42202485938278583, disc_loss = 0.16832667653255826
Trained batch 439 in epoch 0, gen_loss = 0.42191370725631716, disc_loss = 0.16819320767694576
Trained batch 440 in epoch 0, gen_loss = 0.4218734904211395, disc_loss = 0.1679101520916975
Trained batch 441 in epoch 0, gen_loss = 0.4218200883444618, disc_loss = 0.16794898682505702
Trained batch 442 in epoch 0, gen_loss = 0.42188236393185824, disc_loss = 0.1680374568113826
Trained batch 443 in epoch 0, gen_loss = 0.42183978278357703, disc_loss = 0.16781811170733055
Trained batch 444 in epoch 0, gen_loss = 0.42169919221588736, disc_loss = 0.16793018160492518
Trained batch 445 in epoch 0, gen_loss = 0.42169867896980234, disc_loss = 0.16772148447036075
Trained batch 446 in epoch 0, gen_loss = 0.4215841832043607, disc_loss = 0.1675978798452573
Trained batch 447 in epoch 0, gen_loss = 0.4214152884669602, disc_loss = 0.16787116170910718
Trained batch 448 in epoch 0, gen_loss = 0.42161609541068895, disc_loss = 0.16767516594073156
Trained batch 449 in epoch 0, gen_loss = 0.4215147007173962, disc_loss = 0.1676735059875581
Trained batch 450 in epoch 0, gen_loss = 0.4212135459789944, disc_loss = 0.1675354402528832
Trained batch 451 in epoch 0, gen_loss = 0.4211393922435499, disc_loss = 0.1676887865178287
Trained batch 452 in epoch 0, gen_loss = 0.4213569425457624, disc_loss = 0.16778649984721186
Trained batch 453 in epoch 0, gen_loss = 0.4212954551769248, disc_loss = 0.16814812925533446
Trained batch 454 in epoch 0, gen_loss = 0.42135806404627285, disc_loss = 0.16867848365762075
Trained batch 455 in epoch 0, gen_loss = 0.4210678342925875, disc_loss = 0.1687019492148242
Trained batch 456 in epoch 0, gen_loss = 0.4211549561315866, disc_loss = 0.16872840760687377
Trained batch 457 in epoch 0, gen_loss = 0.4210413542767279, disc_loss = 0.16872894236468203
Trained batch 458 in epoch 0, gen_loss = 0.42104097324259143, disc_loss = 0.1687605863717062
Trained batch 459 in epoch 0, gen_loss = 0.4208174375088318, disc_loss = 0.1689642930443844
Trained batch 460 in epoch 0, gen_loss = 0.4206041926417072, disc_loss = 0.16913269883235857
Testing Epoch 0
Traceback (most recent call last):
  File "srgan_bones.py", line 330, in <module>
    img_grid = utils.make_thickness_images(imgs_hr[:5], imgs_lr[:5], gen_hr[:5])
  File "/work3/soeba/HALOS/utils.py", line 26, in make_thickness_images
    axs[i,0].imshow(lr_imgs[i][0],cmap='gray')
  File "/work3/soeba/HALOS/halos/lib/python3.8/site-packages/matplotlib/_api/deprecation.py", line 459, in wrapper
    return func(*args, **kwargs)
  File "/work3/soeba/HALOS/halos/lib/python3.8/site-packages/matplotlib/__init__.py", line 1412, in inner
    return func(ax, *map(sanitize_sequence, args), **kwargs)
  File "/work3/soeba/HALOS/halos/lib/python3.8/site-packages/matplotlib/axes/_axes.py", line 5481, in imshow
    im.set_data(X)
  File "/work3/soeba/HALOS/halos/lib/python3.8/site-packages/matplotlib/image.py", line 702, in set_data
    self._A = cbook.safe_masked_invalid(A, copy=True)
  File "/work3/soeba/HALOS/halos/lib/python3.8/site-packages/matplotlib/cbook/__init__.py", line 701, in safe_masked_invalid
    x = np.array(x, subok=True, copy=copy)
  File "/work3/soeba/HALOS/halos/lib/python3.8/site-packages/torch/_tensor.py", line 757, in __array__
    return self.numpy()
TypeError: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.