wandb: WARNING Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.
/work3/soeba/HALOS/halos/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.
  warnings.warn(
/work3/soeba/HALOS/halos/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Training Epoch 0
Trained batch 0 in epoch 0, gen_loss = 0.5143977999687195, disc_loss = 0.84085613489151
Trained batch 1 in epoch 0, gen_loss = 0.5337973833084106, disc_loss = 0.7852014601230621
Trained batch 2 in epoch 0, gen_loss = 0.49820785721143085, disc_loss = 0.700822631518046
Trained batch 3 in epoch 0, gen_loss = 0.4884335994720459, disc_loss = 0.7029827535152435
Trained batch 4 in epoch 0, gen_loss = 0.4633143603801727, disc_loss = 0.6305515229701996
Trained batch 5 in epoch 0, gen_loss = 0.45479487379391986, disc_loss = 0.5920469413201014
Trained batch 6 in epoch 0, gen_loss = 0.44486755558422636, disc_loss = 0.5450948689665113
Trained batch 7 in epoch 0, gen_loss = 0.45548010244965553, disc_loss = 0.5063937809318304
Trained batch 8 in epoch 0, gen_loss = 0.4473995135890113, disc_loss = 0.4662368181678984
Trained batch 9 in epoch 0, gen_loss = 0.44122628271579745, disc_loss = 0.4312004387378693
Trained batch 10 in epoch 0, gen_loss = 0.44175961071794684, disc_loss = 0.40179266848347406
Trained batch 11 in epoch 0, gen_loss = 0.44587310403585434, disc_loss = 0.3763017101834218
Trained batch 12 in epoch 0, gen_loss = 0.4512977943970607, disc_loss = 0.35448772689470875
Trained batch 13 in epoch 0, gen_loss = 0.448610531432288, disc_loss = 0.3353178320186479
Trained batch 14 in epoch 0, gen_loss = 0.44781322876612345, disc_loss = 0.3185335899392764
Trained batch 15 in epoch 0, gen_loss = 0.44477739930152893, disc_loss = 0.30304670380428433
Trained batch 16 in epoch 0, gen_loss = 0.4423433156574474, disc_loss = 0.2893744527417071
Trained batch 17 in epoch 0, gen_loss = 0.4395982142951753, disc_loss = 0.27685652756028706
Trained batch 18 in epoch 0, gen_loss = 0.44019097246621786, disc_loss = 0.2654254844314174
Trained batch 19 in epoch 0, gen_loss = 0.43664521872997286, disc_loss = 0.25447299517691135
Trained batch 20 in epoch 0, gen_loss = 0.4376148340247926, disc_loss = 0.2448755098240716
Trained batch 21 in epoch 0, gen_loss = 0.4387997077269988, disc_loss = 0.23585887693546034
Trained batch 22 in epoch 0, gen_loss = 0.43728964484256244, disc_loss = 0.22926769308421924
Trained batch 23 in epoch 0, gen_loss = 0.43766287590066594, disc_loss = 0.22254641385128102
Trained batch 24 in epoch 0, gen_loss = 0.4407919061183929, disc_loss = 0.21787504225969315
Trained batch 25 in epoch 0, gen_loss = 0.44371950970246243, disc_loss = 0.21751719111433396
Trained batch 26 in epoch 0, gen_loss = 0.44457389690257887, disc_loss = 0.22136053636118216
Trained batch 27 in epoch 0, gen_loss = 0.44471832471234457, disc_loss = 0.22563389129936695
Trained batch 28 in epoch 0, gen_loss = 0.4436552483459999, disc_loss = 0.22734856785371385
Trained batch 29 in epoch 0, gen_loss = 0.4434188594420751, disc_loss = 0.2250068463385105
Trained batch 30 in epoch 0, gen_loss = 0.4469664856310814, disc_loss = 0.2205888578487981
Trained batch 31 in epoch 0, gen_loss = 0.44678879901766777, disc_loss = 0.21755612385459244
Trained batch 32 in epoch 0, gen_loss = 0.44532574306834827, disc_loss = 0.21701106000127215
Trained batch 33 in epoch 0, gen_loss = 0.44614023320815144, disc_loss = 0.21306994053370812
Trained batch 34 in epoch 0, gen_loss = 0.4460958617074149, disc_loss = 0.20991433731147222
Trained batch 35 in epoch 0, gen_loss = 0.44544344809320235, disc_loss = 0.20658921988474
Trained batch 36 in epoch 0, gen_loss = 0.44397464475116216, disc_loss = 0.2025543244907985
Trained batch 37 in epoch 0, gen_loss = 0.44366802981025294, disc_loss = 0.1984343414047831
Trained batch 38 in epoch 0, gen_loss = 0.44584537010926467, disc_loss = 0.19436446109261268
Trained batch 39 in epoch 0, gen_loss = 0.44516102522611617, disc_loss = 0.19054376790300012
Trained batch 40 in epoch 0, gen_loss = 0.44406592119030835, disc_loss = 0.18675698730640294
Trained batch 41 in epoch 0, gen_loss = 0.4443044265111287, disc_loss = 0.1830220769736029
Trained batch 42 in epoch 0, gen_loss = 0.44390590800795443, disc_loss = 0.1794368361317834
Trained batch 43 in epoch 0, gen_loss = 0.4444610977714712, disc_loss = 0.17614121244034983
Trained batch 44 in epoch 0, gen_loss = 0.4441457596090105, disc_loss = 0.17288190383050178
Trained batch 45 in epoch 0, gen_loss = 0.4444152473107628, disc_loss = 0.16968812526244184
Trained batch 46 in epoch 0, gen_loss = 0.44431375125621225, disc_loss = 0.16670571457832417
Trained batch 47 in epoch 0, gen_loss = 0.4441293117900689, disc_loss = 0.16402377917741737
Trained batch 48 in epoch 0, gen_loss = 0.44670854174360936, disc_loss = 0.16127966218913087
Trained batch 49 in epoch 0, gen_loss = 0.4489177429676056, disc_loss = 0.15869429137557745
Trained batch 50 in epoch 0, gen_loss = 0.45015283425649005, disc_loss = 0.1569716472763057
Trained batch 51 in epoch 0, gen_loss = 0.4530563159630849, disc_loss = 0.15501849937181061
Trained batch 52 in epoch 0, gen_loss = 0.4531854587905812, disc_loss = 0.1528394093983016
Trained batch 53 in epoch 0, gen_loss = 0.4531671929138678, disc_loss = 0.1510062189772725
Trained batch 54 in epoch 0, gen_loss = 0.4537531310861761, disc_loss = 0.14887887926941568
Trained batch 55 in epoch 0, gen_loss = 0.45491363959653036, disc_loss = 0.14673870931645588
Trained batch 56 in epoch 0, gen_loss = 0.45666631794812385, disc_loss = 0.14470648278661988
Trained batch 57 in epoch 0, gen_loss = 0.4574903588870476, disc_loss = 0.14276529671945448
Trained batch 58 in epoch 0, gen_loss = 0.4584256976337756, disc_loss = 0.14077819950120934
Trained batch 59 in epoch 0, gen_loss = 0.4581471453110377, disc_loss = 0.1389656356535852
Trained batch 60 in epoch 0, gen_loss = 0.4586304944069659, disc_loss = 0.13715269050148668
Trained batch 61 in epoch 0, gen_loss = 0.4586459859724968, disc_loss = 0.13554202873379953
Trained batch 62 in epoch 0, gen_loss = 0.45898632586948457, disc_loss = 0.13387506549793576
Trained batch 63 in epoch 0, gen_loss = 0.45988192595541477, disc_loss = 0.1322314103017561
Trained batch 64 in epoch 0, gen_loss = 0.46104702766125016, disc_loss = 0.13060287873332316
Trained batch 65 in epoch 0, gen_loss = 0.46056707203388214, disc_loss = 0.1290532196448608
Trained batch 66 in epoch 0, gen_loss = 0.46058563287578413, disc_loss = 0.1277810628734418
Trained batch 67 in epoch 0, gen_loss = 0.461990323575104, disc_loss = 0.12638972195632317
Trained batch 68 in epoch 0, gen_loss = 0.461343800244124, disc_loss = 0.12499871148147444
Trained batch 69 in epoch 0, gen_loss = 0.4604228411402021, disc_loss = 0.1241579576262406
Trained batch 70 in epoch 0, gen_loss = 0.46199560417255886, disc_loss = 0.12288816110558913
Trained batch 71 in epoch 0, gen_loss = 0.4619692030052344, disc_loss = 0.12229921736030115
Trained batch 72 in epoch 0, gen_loss = 0.46106674614017956, disc_loss = 0.12419420628719134
Trained batch 73 in epoch 0, gen_loss = 0.4622600678656552, disc_loss = 0.12598290903544104
Trained batch 74 in epoch 0, gen_loss = 0.4616221006711324, disc_loss = 0.12538691823681195
Trained batch 75 in epoch 0, gen_loss = 0.4621685994298835, disc_loss = 0.12511895778343865
Trained batch 76 in epoch 0, gen_loss = 0.46178290093099916, disc_loss = 0.12398577713734144
Trained batch 77 in epoch 0, gen_loss = 0.46137805665150666, disc_loss = 0.12278893162520269
Trained batch 78 in epoch 0, gen_loss = 0.4623183672186695, disc_loss = 0.12158135461467731
Trained batch 79 in epoch 0, gen_loss = 0.4629286598414183, disc_loss = 0.12034124871715904
Trained batch 80 in epoch 0, gen_loss = 0.463089476764938, disc_loss = 0.11926440278321136
Trained batch 81 in epoch 0, gen_loss = 0.4622900202506926, disc_loss = 0.11986525493060671
Trained batch 82 in epoch 0, gen_loss = 0.4618177033332457, disc_loss = 0.12178408982883017
Trained batch 83 in epoch 0, gen_loss = 0.461930503093061, disc_loss = 0.12083461667810168
Trained batch 84 in epoch 0, gen_loss = 0.4626082430867588, disc_loss = 0.12002891078591346
Trained batch 85 in epoch 0, gen_loss = 0.46196723226891007, disc_loss = 0.11926167107425457
Trained batch 86 in epoch 0, gen_loss = 0.4613047303824589, disc_loss = 0.11860036190556383
Trained batch 87 in epoch 0, gen_loss = 0.46090836077928543, disc_loss = 0.11773168033158238
Trained batch 88 in epoch 0, gen_loss = 0.461165845059277, disc_loss = 0.11679464628857174
Trained batch 89 in epoch 0, gen_loss = 0.46212972700595856, disc_loss = 0.1158706706845098
Trained batch 90 in epoch 0, gen_loss = 0.463591252381985, disc_loss = 0.11491562911196725
Trained batch 91 in epoch 0, gen_loss = 0.4637400187228037, disc_loss = 0.11416763020679355
Trained batch 92 in epoch 0, gen_loss = 0.46372636351534113, disc_loss = 0.11355248143676147
Trained batch 93 in epoch 0, gen_loss = 0.46450576883681277, disc_loss = 0.11271526626846258
Trained batch 94 in epoch 0, gen_loss = 0.4652874193693462, disc_loss = 0.11234095004436216
Trained batch 95 in epoch 0, gen_loss = 0.46534098125994205, disc_loss = 0.11174028081586584
Trained batch 96 in epoch 0, gen_loss = 0.4650491076646392, disc_loss = 0.11188653766264006
Trained batch 97 in epoch 0, gen_loss = 0.4645588045217553, disc_loss = 0.11347049849145875
Trained batch 98 in epoch 0, gen_loss = 0.46477033152724756, disc_loss = 0.1142442197247286
Trained batch 99 in epoch 0, gen_loss = 0.465383785367012, disc_loss = 0.11507319191470743
Trained batch 100 in epoch 0, gen_loss = 0.4664154589766323, disc_loss = 0.11474758454847454
Trained batch 101 in epoch 0, gen_loss = 0.46586452862795663, disc_loss = 0.11468535524738185
Trained batch 102 in epoch 0, gen_loss = 0.46535132000747237, disc_loss = 0.1144514674302733
Trained batch 103 in epoch 0, gen_loss = 0.46599763803757155, disc_loss = 0.11412448007971622
Trained batch 104 in epoch 0, gen_loss = 0.4659739440395719, disc_loss = 0.11463295719808056
Trained batch 105 in epoch 0, gen_loss = 0.46474141304223043, disc_loss = 0.1154054993904143
Trained batch 106 in epoch 0, gen_loss = 0.465487950037573, disc_loss = 0.11507989147625794
Trained batch 107 in epoch 0, gen_loss = 0.4662256044922052, disc_loss = 0.1159277957243224
Trained batch 108 in epoch 0, gen_loss = 0.4660241543699842, disc_loss = 0.11598025783554676
Trained batch 109 in epoch 0, gen_loss = 0.46591063981706443, disc_loss = 0.11586888109079817
Trained batch 110 in epoch 0, gen_loss = 0.4657270827808896, disc_loss = 0.11582053192631081
Trained batch 111 in epoch 0, gen_loss = 0.46552795650703566, disc_loss = 0.11687887022604368
Trained batch 112 in epoch 0, gen_loss = 0.4667507758182762, disc_loss = 0.1210900327701748
Trained batch 113 in epoch 0, gen_loss = 0.4669729659431859, disc_loss = 0.12109168065025618
Trained batch 114 in epoch 0, gen_loss = 0.4661705628685329, disc_loss = 0.12538360935514387
Trained batch 115 in epoch 0, gen_loss = 0.4658686310566705, disc_loss = 0.12569973510208315
Trained batch 116 in epoch 0, gen_loss = 0.46563241242343545, disc_loss = 0.12588111159956863
Trained batch 117 in epoch 0, gen_loss = 0.46465823357388125, disc_loss = 0.12618217738818818
Trained batch 118 in epoch 0, gen_loss = 0.463778802577187, disc_loss = 0.1274905876733926
Trained batch 119 in epoch 0, gen_loss = 0.4635023795068264, disc_loss = 0.1272543076581011
Trained batch 120 in epoch 0, gen_loss = 0.46346478619851356, disc_loss = 0.12698916932337048
Trained batch 121 in epoch 0, gen_loss = 0.4631179365955415, disc_loss = 0.12675933126115896
Trained batch 122 in epoch 0, gen_loss = 0.4628465274969737, disc_loss = 0.1269580896459217
Trained batch 123 in epoch 0, gen_loss = 0.4625131869989057, disc_loss = 0.1278441744375854
Trained batch 124 in epoch 0, gen_loss = 0.46184943413734436, disc_loss = 0.12845968069136143
Trained batch 125 in epoch 0, gen_loss = 0.46159637944092824, disc_loss = 0.12850979799848228
Trained batch 126 in epoch 0, gen_loss = 0.46133001512429844, disc_loss = 0.1283979669477292
Trained batch 127 in epoch 0, gen_loss = 0.46152379969134927, disc_loss = 0.12812621846387628
Trained batch 128 in epoch 0, gen_loss = 0.4613236184268035, disc_loss = 0.12967698589371618
Trained batch 129 in epoch 0, gen_loss = 0.46092596650123596, disc_loss = 0.13180408731389504
Trained batch 130 in epoch 0, gen_loss = 0.46132963713798814, disc_loss = 0.13139450267127906
Trained batch 131 in epoch 0, gen_loss = 0.46262021588556695, disc_loss = 0.13148836431686173
Trained batch 132 in epoch 0, gen_loss = 0.46218856071171005, disc_loss = 0.13157690211402295
Trained batch 133 in epoch 0, gen_loss = 0.46157904569782426, disc_loss = 0.13172972370495104
Trained batch 134 in epoch 0, gen_loss = 0.46075040720127247, disc_loss = 0.1319084838722591
Trained batch 135 in epoch 0, gen_loss = 0.4603342229829115, disc_loss = 0.1328264365438372
Trained batch 136 in epoch 0, gen_loss = 0.4605870033702711, disc_loss = 0.132655447702447
Trained batch 137 in epoch 0, gen_loss = 0.46042364036691363, disc_loss = 0.13259315870000402
Trained batch 138 in epoch 0, gen_loss = 0.4601197356371571, disc_loss = 0.13264277878907516
Trained batch 139 in epoch 0, gen_loss = 0.46021067712988173, disc_loss = 0.1329232584285949
Trained batch 140 in epoch 0, gen_loss = 0.4599442257948801, disc_loss = 0.1334857139736414
Trained batch 141 in epoch 0, gen_loss = 0.4604303903982673, disc_loss = 0.13390582391965977
Trained batch 142 in epoch 0, gen_loss = 0.45999464371821264, disc_loss = 0.13366766858819898
Trained batch 143 in epoch 0, gen_loss = 0.4597173175877995, disc_loss = 0.13370693072728398
Trained batch 144 in epoch 0, gen_loss = 0.4596879644640561, disc_loss = 0.1341611338201268
Trained batch 145 in epoch 0, gen_loss = 0.458764935805373, disc_loss = 0.13428718312196944
Trained batch 146 in epoch 0, gen_loss = 0.45834272268678056, disc_loss = 0.1344309961233212
Trained batch 147 in epoch 0, gen_loss = 0.4582215130731866, disc_loss = 0.13470646750332937
Trained batch 148 in epoch 0, gen_loss = 0.4584153748598675, disc_loss = 0.13458902960105631
Trained batch 149 in epoch 0, gen_loss = 0.4583209119240443, disc_loss = 0.13508444553862015
Trained batch 150 in epoch 0, gen_loss = 0.45820006057126633, disc_loss = 0.1357183689929989
Trained batch 151 in epoch 0, gen_loss = 0.4582509812163679, disc_loss = 0.1358459138581039
Trained batch 152 in epoch 0, gen_loss = 0.4579733280964147, disc_loss = 0.13555359320556806
Trained batch 153 in epoch 0, gen_loss = 0.4573265149608835, disc_loss = 0.13540078168613956
Trained batch 154 in epoch 0, gen_loss = 0.4571077810179803, disc_loss = 0.13504706084007218
Trained batch 155 in epoch 0, gen_loss = 0.4572690954575172, disc_loss = 0.13464384469896173
Trained batch 156 in epoch 0, gen_loss = 0.4577671517232421, disc_loss = 0.13425483190377427
Trained batch 157 in epoch 0, gen_loss = 0.4577649018809765, disc_loss = 0.13404153364038543
Trained batch 158 in epoch 0, gen_loss = 0.4583523856393946, disc_loss = 0.13454410791069082
Trained batch 159 in epoch 0, gen_loss = 0.4577461427077651, disc_loss = 0.136057029676158
Trained batch 160 in epoch 0, gen_loss = 0.45743288479236344, disc_loss = 0.13618535221039507
Trained batch 161 in epoch 0, gen_loss = 0.4572117006705131, disc_loss = 0.1362442493001804
Trained batch 162 in epoch 0, gen_loss = 0.45679001830106863, disc_loss = 0.1365201097621691
Trained batch 163 in epoch 0, gen_loss = 0.4566750397405973, disc_loss = 0.1363131047862514
Trained batch 164 in epoch 0, gen_loss = 0.45633905656409984, disc_loss = 0.13654940906121876
Trained batch 165 in epoch 0, gen_loss = 0.4561142018401479, disc_loss = 0.13849860484745488
Trained batch 166 in epoch 0, gen_loss = 0.45569236442714395, disc_loss = 0.1383551775985314
Trained batch 167 in epoch 0, gen_loss = 0.4556902421727067, disc_loss = 0.13826006448029407
Trained batch 168 in epoch 0, gen_loss = 0.45563001128343433, disc_loss = 0.13879531032279047
Trained batch 169 in epoch 0, gen_loss = 0.4554076100096983, disc_loss = 0.13947869163444815
Trained batch 170 in epoch 0, gen_loss = 0.45535850368048014, disc_loss = 0.13917926726154764
Trained batch 171 in epoch 0, gen_loss = 0.4553623331147571, disc_loss = 0.1390502497575484
Trained batch 172 in epoch 0, gen_loss = 0.455147490163759, disc_loss = 0.1389765241264091
Trained batch 173 in epoch 0, gen_loss = 0.4557804398495576, disc_loss = 0.13858476975911307
Trained batch 174 in epoch 0, gen_loss = 0.4557234115259988, disc_loss = 0.13848089781190667
Trained batch 175 in epoch 0, gen_loss = 0.4557264782488346, disc_loss = 0.1396096091484651
Trained batch 176 in epoch 0, gen_loss = 0.455972987042982, disc_loss = 0.14024691613085863
Trained batch 177 in epoch 0, gen_loss = 0.45549011983898247, disc_loss = 0.14025154124861688
Trained batch 178 in epoch 0, gen_loss = 0.45519673474674116, disc_loss = 0.1404098014362864
Trained batch 179 in epoch 0, gen_loss = 0.4550655260682106, disc_loss = 0.1414189762642814
Trained batch 180 in epoch 0, gen_loss = 0.45518611560868955, disc_loss = 0.14137575352430673
Trained batch 181 in epoch 0, gen_loss = 0.4547398891095277, disc_loss = 0.1414226469696387
Trained batch 182 in epoch 0, gen_loss = 0.45454248606832953, disc_loss = 0.1416850111299688
Trained batch 183 in epoch 0, gen_loss = 0.4544134301983792, disc_loss = 0.1420735341962427
Trained batch 184 in epoch 0, gen_loss = 0.45417307534733337, disc_loss = 0.1434003147825196
Trained batch 185 in epoch 0, gen_loss = 0.45385668018171865, disc_loss = 0.14362097933127355
Trained batch 186 in epoch 0, gen_loss = 0.4534878545903905, disc_loss = 0.14396973274010708
Trained batch 187 in epoch 0, gen_loss = 0.45347526019557993, disc_loss = 0.14385532961286446
Trained batch 188 in epoch 0, gen_loss = 0.453383802894562, disc_loss = 0.1438439733708504
Trained batch 189 in epoch 0, gen_loss = 0.4533218364966543, disc_loss = 0.14405757628969457
Trained batch 190 in epoch 0, gen_loss = 0.453049979596862, disc_loss = 0.1448561424457746
Trained batch 191 in epoch 0, gen_loss = 0.4527927970824142, disc_loss = 0.14498289528031214
Trained batch 192 in epoch 0, gen_loss = 0.45247830581788573, disc_loss = 0.14504140075971733
Trained batch 193 in epoch 0, gen_loss = 0.4522156845970252, disc_loss = 0.14510858781904595
Trained batch 194 in epoch 0, gen_loss = 0.4519002974033356, disc_loss = 0.14504645949181838
Trained batch 195 in epoch 0, gen_loss = 0.45184397621422395, disc_loss = 0.14512391685869316
Trained batch 196 in epoch 0, gen_loss = 0.451356942732322, disc_loss = 0.1452951334549237
Trained batch 197 in epoch 0, gen_loss = 0.4513272125311572, disc_loss = 0.14631647986331672
Trained batch 198 in epoch 0, gen_loss = 0.45164699201008784, disc_loss = 0.14721014198084273
Trained batch 199 in epoch 0, gen_loss = 0.45123238816857336, disc_loss = 0.14771582231856883
Trained batch 200 in epoch 0, gen_loss = 0.4506990276462403, disc_loss = 0.14813181447486082
Trained batch 201 in epoch 0, gen_loss = 0.4504872495585149, disc_loss = 0.14836930115661112
Trained batch 202 in epoch 0, gen_loss = 0.4502708638830138, disc_loss = 0.14874424481663504
Trained batch 203 in epoch 0, gen_loss = 0.4498143545260616, disc_loss = 0.14898372186786113
Trained batch 204 in epoch 0, gen_loss = 0.449295018068174, disc_loss = 0.14921418363909897
Trained batch 205 in epoch 0, gen_loss = 0.44910897386884224, disc_loss = 0.14966141390026483
Trained batch 206 in epoch 0, gen_loss = 0.44883032308684456, disc_loss = 0.1501019426536013
Trained batch 207 in epoch 0, gen_loss = 0.4487424007115456, disc_loss = 0.1502595699410169
Trained batch 208 in epoch 0, gen_loss = 0.4484819064014836, disc_loss = 0.15041059534551138
Trained batch 209 in epoch 0, gen_loss = 0.44845814023699077, disc_loss = 0.15052054979439292
Trained batch 210 in epoch 0, gen_loss = 0.4483571647185285, disc_loss = 0.15053613830834486
Trained batch 211 in epoch 0, gen_loss = 0.4479805541769514, disc_loss = 0.15099965500698057
Trained batch 212 in epoch 0, gen_loss = 0.4480333196725084, disc_loss = 0.15108503073348967
Trained batch 213 in epoch 0, gen_loss = 0.4479428688499415, disc_loss = 0.15118784360831308
Trained batch 214 in epoch 0, gen_loss = 0.4478867443495019, disc_loss = 0.15118736909745736
Trained batch 215 in epoch 0, gen_loss = 0.44790214617495183, disc_loss = 0.15124573255889118
Trained batch 216 in epoch 0, gen_loss = 0.44780708471750885, disc_loss = 0.1511262903925598
Trained batch 217 in epoch 0, gen_loss = 0.4476972469769487, disc_loss = 0.15147456636977033
Trained batch 218 in epoch 0, gen_loss = 0.4475536229403596, disc_loss = 0.15196639776536047
Trained batch 219 in epoch 0, gen_loss = 0.44709319905801254, disc_loss = 0.15265058943663132
Trained batch 220 in epoch 0, gen_loss = 0.44718414558544417, disc_loss = 0.15300237735505826
Trained batch 221 in epoch 0, gen_loss = 0.4466489738709218, disc_loss = 0.15371208021266236
Trained batch 222 in epoch 0, gen_loss = 0.4464704811305743, disc_loss = 0.1542946250666551
Trained batch 223 in epoch 0, gen_loss = 0.4462560626811215, disc_loss = 0.15490826319936396
Trained batch 224 in epoch 0, gen_loss = 0.446272228691313, disc_loss = 0.15544637170930703
Trained batch 225 in epoch 0, gen_loss = 0.44597637244557914, disc_loss = 0.1556275353841154
Trained batch 226 in epoch 0, gen_loss = 0.44563392684323144, disc_loss = 0.15572382208567095
Trained batch 227 in epoch 0, gen_loss = 0.44506175175570606, disc_loss = 0.15568154600138465
Trained batch 228 in epoch 0, gen_loss = 0.4448135978009503, disc_loss = 0.15565298615617262
Trained batch 229 in epoch 0, gen_loss = 0.4446782660225163, disc_loss = 0.1556532804813722
Trained batch 230 in epoch 0, gen_loss = 0.4442845073355225, disc_loss = 0.1560563062644237
Trained batch 231 in epoch 0, gen_loss = 0.4440764258133954, disc_loss = 0.1562113878236891
Trained batch 232 in epoch 0, gen_loss = 0.44388229319977657, disc_loss = 0.15669457346616641
Trained batch 233 in epoch 0, gen_loss = 0.44360955734538216, disc_loss = 0.1570755194984058
Trained batch 234 in epoch 0, gen_loss = 0.4433908444769839, disc_loss = 0.1571652918499201
Trained batch 235 in epoch 0, gen_loss = 0.44328950686474977, disc_loss = 0.15726593895262833
Trained batch 236 in epoch 0, gen_loss = 0.4430255180672754, disc_loss = 0.1582815761634695
Trained batch 237 in epoch 0, gen_loss = 0.4428037014328131, disc_loss = 0.1586288656460262
Trained batch 238 in epoch 0, gen_loss = 0.44228130853325753, disc_loss = 0.15895242104501656
Trained batch 239 in epoch 0, gen_loss = 0.4421988027791182, disc_loss = 0.15919488576085616
Trained batch 240 in epoch 0, gen_loss = 0.44218582360081654, disc_loss = 0.15930535079316233
Trained batch 241 in epoch 0, gen_loss = 0.44193656865722875, disc_loss = 0.15931410632627316
Trained batch 242 in epoch 0, gen_loss = 0.4414984300548648, disc_loss = 0.15934529876788703
Trained batch 243 in epoch 0, gen_loss = 0.44127445655768033, disc_loss = 0.15931525937144142
Trained batch 244 in epoch 0, gen_loss = 0.4409976031099047, disc_loss = 0.15916263753814355
Trained batch 245 in epoch 0, gen_loss = 0.4408968620426287, disc_loss = 0.1590913342826856
Trained batch 246 in epoch 0, gen_loss = 0.4408332764619758, disc_loss = 0.1592768305905073
Trained batch 247 in epoch 0, gen_loss = 0.440464541796715, disc_loss = 0.1598288315393391
Trained batch 248 in epoch 0, gen_loss = 0.44010065178794555, disc_loss = 0.1600802615658945
Trained batch 249 in epoch 0, gen_loss = 0.43972889268398285, disc_loss = 0.16020780605822801
Trained batch 250 in epoch 0, gen_loss = 0.43942673913986086, disc_loss = 0.16061447359680892
Trained batch 251 in epoch 0, gen_loss = 0.4394071525524533, disc_loss = 0.1606797798950639
Trained batch 252 in epoch 0, gen_loss = 0.4391405684910273, disc_loss = 0.1609840850776481
Trained batch 253 in epoch 0, gen_loss = 0.43873639709836854, disc_loss = 0.1612213625403957
Trained batch 254 in epoch 0, gen_loss = 0.4385021781220156, disc_loss = 0.1613333873377711
Trained batch 255 in epoch 0, gen_loss = 0.43847262242343277, disc_loss = 0.1611555285708164
Trained batch 256 in epoch 0, gen_loss = 0.438219092583378, disc_loss = 0.1612814176131895
Trained batch 257 in epoch 0, gen_loss = 0.4378257152415061, disc_loss = 0.16154093428897534
Trained batch 258 in epoch 0, gen_loss = 0.43762070584941554, disc_loss = 0.1618169312613466
Trained batch 259 in epoch 0, gen_loss = 0.43772209332539486, disc_loss = 0.161644167230966
Trained batch 260 in epoch 0, gen_loss = 0.43704024368319017, disc_loss = 0.16186250695580495
Trained batch 261 in epoch 0, gen_loss = 0.43674761398147993, disc_loss = 0.16246972421454337
Trained batch 262 in epoch 0, gen_loss = 0.43669658842648845, disc_loss = 0.16354841039916182
Trained batch 263 in epoch 0, gen_loss = 0.4363427253609354, disc_loss = 0.16397260096291025
Trained batch 264 in epoch 0, gen_loss = 0.4357751369476318, disc_loss = 0.16443780942486141
Trained batch 265 in epoch 0, gen_loss = 0.4354203385966165, disc_loss = 0.16503993876250392
Trained batch 266 in epoch 0, gen_loss = 0.43532598666037514, disc_loss = 0.1669480757166942
Trained batch 267 in epoch 0, gen_loss = 0.43528530379729485, disc_loss = 0.1671032055503508
Trained batch 268 in epoch 0, gen_loss = 0.43496071428171323, disc_loss = 0.16756882915294083
Trained batch 269 in epoch 0, gen_loss = 0.4344170105678064, disc_loss = 0.16821889210216426
Trained batch 270 in epoch 0, gen_loss = 0.43440344588782953, disc_loss = 0.1684089623652811
Trained batch 271 in epoch 0, gen_loss = 0.4341045878827572, disc_loss = 0.16891145109719433
Trained batch 272 in epoch 0, gen_loss = 0.4339290369342972, disc_loss = 0.16932321223174476
Trained batch 273 in epoch 0, gen_loss = 0.43337707602194625, disc_loss = 0.16949197449415487
Trained batch 274 in epoch 0, gen_loss = 0.43338302850723265, disc_loss = 0.1696753373538906
Trained batch 275 in epoch 0, gen_loss = 0.4334601304237393, disc_loss = 0.16977645527652424
Trained batch 276 in epoch 0, gen_loss = 0.4332513197019212, disc_loss = 0.17018007377451722
Trained batch 277 in epoch 0, gen_loss = 0.432896315301065, disc_loss = 0.17043284777712694
Trained batch 278 in epoch 0, gen_loss = 0.43256711489838084, disc_loss = 0.17067206584878505
Trained batch 279 in epoch 0, gen_loss = 0.43217995965055056, disc_loss = 0.1709466555887567
Trained batch 280 in epoch 0, gen_loss = 0.43183692833707, disc_loss = 0.17120366748356097
Trained batch 281 in epoch 0, gen_loss = 0.43161722859169577, disc_loss = 0.17137637301777484
Trained batch 282 in epoch 0, gen_loss = 0.43136136681367987, disc_loss = 0.17155696936741524
Trained batch 283 in epoch 0, gen_loss = 0.4310752127162168, disc_loss = 0.17164853882228195
Trained batch 284 in epoch 0, gen_loss = 0.4308025279588867, disc_loss = 0.17184942945707263
Trained batch 285 in epoch 0, gen_loss = 0.43066579134731026, disc_loss = 0.17190829951680833
Trained batch 286 in epoch 0, gen_loss = 0.4306585428399076, disc_loss = 0.17203045982335294
Trained batch 287 in epoch 0, gen_loss = 0.4305499168112874, disc_loss = 0.17218967310489258
Trained batch 288 in epoch 0, gen_loss = 0.4305461941087122, disc_loss = 0.17215955840700845
Trained batch 289 in epoch 0, gen_loss = 0.4305259773443485, disc_loss = 0.1722106190141419
Trained batch 290 in epoch 0, gen_loss = 0.4301818374710804, disc_loss = 0.1723012489513126
Trained batch 291 in epoch 0, gen_loss = 0.4297689928175652, disc_loss = 0.1727643827014692
Trained batch 292 in epoch 0, gen_loss = 0.429472599826982, disc_loss = 0.1731432020829501
Trained batch 293 in epoch 0, gen_loss = 0.4290306169767769, disc_loss = 0.17329748406322026
Trained batch 294 in epoch 0, gen_loss = 0.4289708350674581, disc_loss = 0.17368428136963965
Trained batch 295 in epoch 0, gen_loss = 0.4287609460990171, disc_loss = 0.1739627033837039
Trained batch 296 in epoch 0, gen_loss = 0.4285511444714736, disc_loss = 0.17400087256247948
Trained batch 297 in epoch 0, gen_loss = 0.42823145413558755, disc_loss = 0.17410030726643416
Trained batch 298 in epoch 0, gen_loss = 0.42805399862818894, disc_loss = 0.1741958780893215
Trained batch 299 in epoch 0, gen_loss = 0.4278572509686152, disc_loss = 0.17417880426471433
Trained batch 300 in epoch 0, gen_loss = 0.42758458625042556, disc_loss = 0.17429161913989985
Trained batch 301 in epoch 0, gen_loss = 0.42733550259214365, disc_loss = 0.1743866316295716
Trained batch 302 in epoch 0, gen_loss = 0.42711124384757315, disc_loss = 0.17431312426309972
Trained batch 303 in epoch 0, gen_loss = 0.4269363883098489, disc_loss = 0.17435562052445389
Trained batch 304 in epoch 0, gen_loss = 0.42665497426126825, disc_loss = 0.17450694906540581
Trained batch 305 in epoch 0, gen_loss = 0.4265201760857713, disc_loss = 0.17461045247691323
Trained batch 306 in epoch 0, gen_loss = 0.4263597962716503, disc_loss = 0.1747638273086241
Trained batch 307 in epoch 0, gen_loss = 0.42646906728094275, disc_loss = 0.1747466787853121
Trained batch 308 in epoch 0, gen_loss = 0.42636241891623317, disc_loss = 0.1746514936830716
Trained batch 309 in epoch 0, gen_loss = 0.42619920757509044, disc_loss = 0.1746938598312197
Trained batch 310 in epoch 0, gen_loss = 0.4259362449791654, disc_loss = 0.1748648356837954
Trained batch 311 in epoch 0, gen_loss = 0.4259818010032177, disc_loss = 0.1747291507020306
Trained batch 312 in epoch 0, gen_loss = 0.42575861765934636, disc_loss = 0.17487829067289068
Trained batch 313 in epoch 0, gen_loss = 0.4256297155360507, disc_loss = 0.17525391252770736
Trained batch 314 in epoch 0, gen_loss = 0.42547625151891555, disc_loss = 0.17531963218417432
Trained batch 315 in epoch 0, gen_loss = 0.425182544355151, disc_loss = 0.175322326401225
Trained batch 316 in epoch 0, gen_loss = 0.4250576606120221, disc_loss = 0.17537597475767325
Trained batch 317 in epoch 0, gen_loss = 0.42520747985105095, disc_loss = 0.17541326123799356
Trained batch 318 in epoch 0, gen_loss = 0.4249511264895197, disc_loss = 0.1755654004265242
Trained batch 319 in epoch 0, gen_loss = 0.4248990583233535, disc_loss = 0.17540365427848884
Trained batch 320 in epoch 0, gen_loss = 0.42449776025204644, disc_loss = 0.17582711297148299
Trained batch 321 in epoch 0, gen_loss = 0.4243693009296559, disc_loss = 0.17641586072812354
Trained batch 322 in epoch 0, gen_loss = 0.4242059048668888, disc_loss = 0.17642589894356928
Trained batch 323 in epoch 0, gen_loss = 0.4241667684213615, disc_loss = 0.1766098354437193
Trained batch 324 in epoch 0, gen_loss = 0.42385500972087564, disc_loss = 0.1766726876508731
Trained batch 325 in epoch 0, gen_loss = 0.4238615488530668, disc_loss = 0.17671667165665722
Trained batch 326 in epoch 0, gen_loss = 0.42381360215513714, disc_loss = 0.1766405978983422
Trained batch 327 in epoch 0, gen_loss = 0.42356443259774185, disc_loss = 0.17662747665989872
Trained batch 328 in epoch 0, gen_loss = 0.42317633471228067, disc_loss = 0.17665083980963403
Trained batch 329 in epoch 0, gen_loss = 0.42291361806970657, disc_loss = 0.1766749876297333
Trained batch 330 in epoch 0, gen_loss = 0.42277200029335715, disc_loss = 0.17674744622697464
Trained batch 331 in epoch 0, gen_loss = 0.42244113317455156, disc_loss = 0.17675810081166138
Trained batch 332 in epoch 0, gen_loss = 0.42235983957399476, disc_loss = 0.17681784210195234
Trained batch 333 in epoch 0, gen_loss = 0.4222867859159401, disc_loss = 0.1768218962207972
Trained batch 334 in epoch 0, gen_loss = 0.42218209318260647, disc_loss = 0.17682735090936297
Trained batch 335 in epoch 0, gen_loss = 0.4220046591723249, disc_loss = 0.17675974511075765
Trained batch 336 in epoch 0, gen_loss = 0.42182781239999156, disc_loss = 0.17675434321010325
Trained batch 337 in epoch 0, gen_loss = 0.42182935545077693, disc_loss = 0.1767551855586425
Trained batch 338 in epoch 0, gen_loss = 0.4217093405822034, disc_loss = 0.1769027114189216
Trained batch 339 in epoch 0, gen_loss = 0.4215722229550867, disc_loss = 0.17680533842238433
Trained batch 340 in epoch 0, gen_loss = 0.42132927306935936, disc_loss = 0.17690550055780893
Trained batch 341 in epoch 0, gen_loss = 0.4208375553638614, disc_loss = 0.1773659091780496
Trained batch 342 in epoch 0, gen_loss = 0.4210051538819127, disc_loss = 0.1772710985338462
Trained batch 343 in epoch 0, gen_loss = 0.42118616064274034, disc_loss = 0.17707067457850761
Trained batch 344 in epoch 0, gen_loss = 0.42114541841589886, disc_loss = 0.1769682194713665
Trained batch 345 in epoch 0, gen_loss = 0.42123915763259623, disc_loss = 0.17675899882730894
Trained batch 346 in epoch 0, gen_loss = 0.4212235603964638, disc_loss = 0.17653897439561764
Trained batch 347 in epoch 0, gen_loss = 0.42137042595737284, disc_loss = 0.17677615326561633
Trained batch 348 in epoch 0, gen_loss = 0.421729731696383, disc_loss = 0.17793644551070337
Trained batch 349 in epoch 0, gen_loss = 0.42172961507524764, disc_loss = 0.178114331455103
Trained batch 350 in epoch 0, gen_loss = 0.4214277828690673, disc_loss = 0.17840600566158438
Trained batch 351 in epoch 0, gen_loss = 0.42127446533942764, disc_loss = 0.17852329183899035
Trained batch 352 in epoch 0, gen_loss = 0.4212141047962664, disc_loss = 0.1785441878127731
Trained batch 353 in epoch 0, gen_loss = 0.4210390127816443, disc_loss = 0.17854220803700765
Trained batch 354 in epoch 0, gen_loss = 0.42091240001396396, disc_loss = 0.17873569643518455
Trained batch 355 in epoch 0, gen_loss = 0.42086661531684105, disc_loss = 0.1789865972673039
Trained batch 356 in epoch 0, gen_loss = 0.4208223772983925, disc_loss = 0.17898215066005035
Trained batch 357 in epoch 0, gen_loss = 0.4205551869209918, disc_loss = 0.1791451231279936
Trained batch 358 in epoch 0, gen_loss = 0.4204202385665979, disc_loss = 0.17916583865984734
Trained batch 359 in epoch 0, gen_loss = 0.4204752879838149, disc_loss = 0.17909387168474494
Trained batch 360 in epoch 0, gen_loss = 0.42023475018234463, disc_loss = 0.17897892609713315
Trained batch 361 in epoch 0, gen_loss = 0.420281084038276, disc_loss = 0.17901068588831973
Trained batch 362 in epoch 0, gen_loss = 0.42032640343198435, disc_loss = 0.17929969116860678
Trained batch 363 in epoch 0, gen_loss = 0.42003835860516997, disc_loss = 0.17925816864941965
Trained batch 364 in epoch 0, gen_loss = 0.41983216323264655, disc_loss = 0.17930442936310212
Trained batch 365 in epoch 0, gen_loss = 0.4197974826305942, disc_loss = 0.17924953883646128
Trained batch 366 in epoch 0, gen_loss = 0.4197834966455558, disc_loss = 0.17918765642845175
Trained batch 367 in epoch 0, gen_loss = 0.41988912078997365, disc_loss = 0.17916274706974789
Trained batch 368 in epoch 0, gen_loss = 0.41971114544364496, disc_loss = 0.17950783000103018
Trained batch 369 in epoch 0, gen_loss = 0.41953024115111376, disc_loss = 0.17947916219866758
Trained batch 370 in epoch 0, gen_loss = 0.419343826182769, disc_loss = 0.1797126625636919
Trained batch 371 in epoch 0, gen_loss = 0.4192489570667667, disc_loss = 0.17980129300286213
Trained batch 372 in epoch 0, gen_loss = 0.41911169926538544, disc_loss = 0.1798567119586324
Trained batch 373 in epoch 0, gen_loss = 0.4187885677431994, disc_loss = 0.1798104702440255
Trained batch 374 in epoch 0, gen_loss = 0.4185626351038615, disc_loss = 0.1798529074639082
Trained batch 375 in epoch 0, gen_loss = 0.4184321264954323, disc_loss = 0.1799234299712438
Trained batch 376 in epoch 0, gen_loss = 0.4183724884645376, disc_loss = 0.17985943628262618
Trained batch 377 in epoch 0, gen_loss = 0.4184211138537321, disc_loss = 0.17973605618767796
Trained batch 378 in epoch 0, gen_loss = 0.41835711986848734, disc_loss = 0.179523117668278
Trained batch 379 in epoch 0, gen_loss = 0.4183889906657369, disc_loss = 0.17940665562392066
Trained batch 380 in epoch 0, gen_loss = 0.41818873384806116, disc_loss = 0.1794761654791322
Trained batch 381 in epoch 0, gen_loss = 0.418063206510394, disc_loss = 0.1796720925162678
Trained batch 382 in epoch 0, gen_loss = 0.4179776116388585, disc_loss = 0.18003713820547876
Trained batch 383 in epoch 0, gen_loss = 0.41788571847913164, disc_loss = 0.18009003593761008
Trained batch 384 in epoch 0, gen_loss = 0.41770302730721315, disc_loss = 0.18020587695593185
Trained batch 385 in epoch 0, gen_loss = 0.4177260469124107, disc_loss = 0.1800921695268293
Trained batch 386 in epoch 0, gen_loss = 0.4176266338936118, disc_loss = 0.1801005919976521
Trained batch 387 in epoch 0, gen_loss = 0.4175944286830646, disc_loss = 0.18015435211763708
Trained batch 388 in epoch 0, gen_loss = 0.4174341598482549, disc_loss = 0.17999028662042477
Trained batch 389 in epoch 0, gen_loss = 0.417343907784193, disc_loss = 0.17999427617073824
Trained batch 390 in epoch 0, gen_loss = 0.41722362768619564, disc_loss = 0.1799450915764131
Trained batch 391 in epoch 0, gen_loss = 0.41723773430804817, disc_loss = 0.18015001315566503
Trained batch 392 in epoch 0, gen_loss = 0.4169989232798569, disc_loss = 0.18017452773721013
Trained batch 393 in epoch 0, gen_loss = 0.416857604221039, disc_loss = 0.18007526585009648
Trained batch 394 in epoch 0, gen_loss = 0.41674934768978555, disc_loss = 0.18038018433070635
Trained batch 395 in epoch 0, gen_loss = 0.4167682598486091, disc_loss = 0.18037937100355825
Trained batch 396 in epoch 0, gen_loss = 0.4166348277771803, disc_loss = 0.18022580216322048
Trained batch 397 in epoch 0, gen_loss = 0.41647754557168665, disc_loss = 0.18019475357784848
Trained batch 398 in epoch 0, gen_loss = 0.41633061851773945, disc_loss = 0.18007938329009035
Trained batch 399 in epoch 0, gen_loss = 0.41632983677089214, disc_loss = 0.1801723200874403
Trained batch 400 in epoch 0, gen_loss = 0.4161204879272014, disc_loss = 0.1802287682231302
Trained batch 401 in epoch 0, gen_loss = 0.4160340718220715, disc_loss = 0.1805231037470906
Trained batch 402 in epoch 0, gen_loss = 0.41604015556517665, disc_loss = 0.18049727140396613
Trained batch 403 in epoch 0, gen_loss = 0.4161106847447924, disc_loss = 0.18046143190418878
Trained batch 404 in epoch 0, gen_loss = 0.4161350317943243, disc_loss = 0.1804392719756306
Trained batch 405 in epoch 0, gen_loss = 0.4160243607506963, disc_loss = 0.18046933567160603
Trained batch 406 in epoch 0, gen_loss = 0.41608533153369914, disc_loss = 0.18022697585971262
Trained batch 407 in epoch 0, gen_loss = 0.41597627848386765, disc_loss = 0.18044816622711427
Trained batch 408 in epoch 0, gen_loss = 0.4156173118606465, disc_loss = 0.18057714508787198
Trained batch 409 in epoch 0, gen_loss = 0.4155455841523845, disc_loss = 0.18094629020680014
Trained batch 410 in epoch 0, gen_loss = 0.4152685609032058, disc_loss = 0.18150939515490455
Trained batch 411 in epoch 0, gen_loss = 0.415222075523682, disc_loss = 0.18163295155752776
Trained batch 412 in epoch 0, gen_loss = 0.4149333482504468, disc_loss = 0.18179211587381278
Trained batch 413 in epoch 0, gen_loss = 0.4147307872772217, disc_loss = 0.18202008562981362
Trained batch 414 in epoch 0, gen_loss = 0.4145836826548519, disc_loss = 0.18217911185719163
Trained batch 415 in epoch 0, gen_loss = 0.41452161523585135, disc_loss = 0.18230810772645503
Trained batch 416 in epoch 0, gen_loss = 0.41438831621222644, disc_loss = 0.18233431361579924
Trained batch 417 in epoch 0, gen_loss = 0.4140592629544473, disc_loss = 0.1824001861361867
Trained batch 418 in epoch 0, gen_loss = 0.41401408594946304, disc_loss = 0.18245040211462035
Trained batch 419 in epoch 0, gen_loss = 0.41370826966705776, disc_loss = 0.18255634528274337
Trained batch 420 in epoch 0, gen_loss = 0.41360559047259515, disc_loss = 0.1826362426209124
Trained batch 421 in epoch 0, gen_loss = 0.4135482165062032, disc_loss = 0.18280220537124228
Trained batch 422 in epoch 0, gen_loss = 0.41318406844533645, disc_loss = 0.18279711663036058
Trained batch 423 in epoch 0, gen_loss = 0.41300611971121914, disc_loss = 0.18280833826470627
Trained batch 424 in epoch 0, gen_loss = 0.41270930346320656, disc_loss = 0.18279067201211172
Trained batch 425 in epoch 0, gen_loss = 0.41255552521054173, disc_loss = 0.1827078312679146
Trained batch 426 in epoch 0, gen_loss = 0.41255119163761095, disc_loss = 0.18305669645113604
Trained batch 427 in epoch 0, gen_loss = 0.4124870353909296, disc_loss = 0.18299430668371441
Trained batch 428 in epoch 0, gen_loss = 0.412402505641217, disc_loss = 0.18312654959033559
Trained batch 429 in epoch 0, gen_loss = 0.4123450102501137, disc_loss = 0.18307054552661126
Trained batch 430 in epoch 0, gen_loss = 0.41225226261499587, disc_loss = 0.18303582860965048
Trained batch 431 in epoch 0, gen_loss = 0.4122687200843184, disc_loss = 0.18288857866233835
Trained batch 432 in epoch 0, gen_loss = 0.41213310398075376, disc_loss = 0.18277725416039897
Trained batch 433 in epoch 0, gen_loss = 0.4120236506797202, disc_loss = 0.18300737856462393
Trained batch 434 in epoch 0, gen_loss = 0.4118387072935872, disc_loss = 0.18306841449919103
Trained batch 435 in epoch 0, gen_loss = 0.41168043106247526, disc_loss = 0.18297511708726166
Trained batch 436 in epoch 0, gen_loss = 0.41149269163744945, disc_loss = 0.18294645754249608
Trained batch 437 in epoch 0, gen_loss = 0.411242879280761, disc_loss = 0.18287125706553595
Trained batch 438 in epoch 0, gen_loss = 0.41095822608280835, disc_loss = 0.182697075854028
Trained batch 439 in epoch 0, gen_loss = 0.4107091088186611, disc_loss = 0.18268647543154656
Trained batch 440 in epoch 0, gen_loss = 0.4106935758033848, disc_loss = 0.18265017908058065
Trained batch 441 in epoch 0, gen_loss = 0.4106605478406492, disc_loss = 0.1824691061528897
Trained batch 442 in epoch 0, gen_loss = 0.4106465429403981, disc_loss = 0.1822543824911454
Trained batch 443 in epoch 0, gen_loss = 0.4104911219160836, disc_loss = 0.18203773522783104
Trained batch 444 in epoch 0, gen_loss = 0.41053568801183377, disc_loss = 0.18177748656088716
Trained batch 445 in epoch 0, gen_loss = 0.4106722476102846, disc_loss = 0.1816045346423449
Trained batch 446 in epoch 0, gen_loss = 0.4105738245400806, disc_loss = 0.18141516433482868
Trained batch 447 in epoch 0, gen_loss = 0.41047085715191706, disc_loss = 0.1811731684595413
Trained batch 448 in epoch 0, gen_loss = 0.41042407749755877, disc_loss = 0.18112945852423695
Trained batch 449 in epoch 0, gen_loss = 0.41037265135182277, disc_loss = 0.18110322234117324
Trained batch 450 in epoch 0, gen_loss = 0.410331898610502, disc_loss = 0.18108735453676095
Trained batch 451 in epoch 0, gen_loss = 0.4103449999099284, disc_loss = 0.18122886434873253
Trained batch 452 in epoch 0, gen_loss = 0.4102972342610096, disc_loss = 0.18140741745647349
Trained batch 453 in epoch 0, gen_loss = 0.4102261336889561, disc_loss = 0.18189696104795003
Trained batch 454 in epoch 0, gen_loss = 0.4100780466100672, disc_loss = 0.181799966231971
Trained batch 455 in epoch 0, gen_loss = 0.4098704838961886, disc_loss = 0.18178162305909945
Trained batch 456 in epoch 0, gen_loss = 0.40969015830716105, disc_loss = 0.18172234127652592
Trained batch 457 in epoch 0, gen_loss = 0.409599922109379, disc_loss = 0.18176798981626613
Trained batch 458 in epoch 0, gen_loss = 0.40965446816810075, disc_loss = 0.1815742052369193
Trained batch 459 in epoch 0, gen_loss = 0.4095358355537705, disc_loss = 0.18161868868071748
Trained batch 460 in epoch 0, gen_loss = 0.40969479484827037, disc_loss = 0.18144930073680304
Testing Epoch 0
Traceback (most recent call last):
  File "srgan_bones.py", line 330, in <module>
    img_grid = utils.make_thickness_images(imgs_hr[:5], imgs_lr[:5], imgs_sr[:5])
NameError: name 'imgs_sr' is not defined