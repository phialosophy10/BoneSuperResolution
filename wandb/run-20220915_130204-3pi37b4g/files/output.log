/work3/soeba/HALOS/halos/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.
  warnings.warn(
/work3/soeba/HALOS/halos/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Training Epoch 0
Trained batch 915 in epoch 0, gen_loss = 0.9838035856271935, disc_loss = 0.027688606217448643
Testing Epoch 0
Training Epoch 1
Trained batch 915 in epoch 1, gen_loss = 0.9407646069425162, disc_loss = 0.022693807493176465
Testing Epoch 1
Training Epoch 2
Trained batch 915 in epoch 2, gen_loss = 0.875239495041589, disc_loss = 0.0007414039382995736
Testing Epoch 2
Training Epoch 3
Trained batch 915 in epoch 3, gen_loss = 0.8597674773249564, disc_loss = 0.00044127810577654553
Testing Epoch 3
Training Epoch 4
Trained batch 915 in epoch 4, gen_loss = 0.8731021092448172, disc_loss = 0.0067718795270391
Testing Epoch 4
Training Epoch 5
Trained batch 915 in epoch 5, gen_loss = 0.8698240817243876, disc_loss = 0.005449860187780669
Testing Epoch 5
Training Epoch 6
Trained batch 915 in epoch 6, gen_loss = 0.7604896720319856, disc_loss = 0.09161954249643654
Testing Epoch 6
Training Epoch 7
Trained batch 915 in epoch 7, gen_loss = 0.6864563973384653, disc_loss = 0.13237972586114757
Testing Epoch 7
Training Epoch 8
Trained batch 915 in epoch 8, gen_loss = 0.6619862223035904, disc_loss = 0.14412931526002504
Testing Epoch 8
Training Epoch 9
Trained batch 915 in epoch 9, gen_loss = 0.6953941077059013, disc_loss = 0.1227235470869981
Testing Epoch 9
Training Epoch 10
Trained batch 915 in epoch 10, gen_loss = 0.7115374615489135, disc_loss = 0.11542738564353128
Testing Epoch 10
Training Epoch 11
Trained batch 915 in epoch 11, gen_loss = 0.7059436419645251, disc_loss = 0.11499781991603203
Testing Epoch 11
Training Epoch 12
Trained batch 915 in epoch 12, gen_loss = 0.7139698681157228, disc_loss = 0.10873608459417575
Testing Epoch 12
Training Epoch 13
Trained batch 915 in epoch 13, gen_loss = 0.7347199303874803, disc_loss = 0.0971636845502498
Testing Epoch 13
Training Epoch 14
Trained batch 915 in epoch 14, gen_loss = 0.736257358539573, disc_loss = 0.09818228774838247
Testing Epoch 14

Training Epoch 15
Trained batch 915 in epoch 15, gen_loss = 0.7408861759643367, disc_loss = 0.09294132156273163
Testing Epoch 15
Training Epoch 16
Trained batch 915 in epoch 16, gen_loss = 0.7489862166442726, disc_loss = 0.09254761873825766
Testing Epoch 16
Training Epoch 17
Trained batch 915 in epoch 17, gen_loss = 0.7328926777735548, disc_loss = 0.09788688621585416
Testing Epoch 17
Training Epoch 18
Trained batch 915 in epoch 18, gen_loss = 0.7554884505454109, disc_loss = 0.08636460608017822
Testing Epoch 18
Training Epoch 19
Trained batch 915 in epoch 19, gen_loss = 0.77142530139617, disc_loss = 0.07919268858391287
Testing Epoch 19
Training Epoch 20
Trained batch 915 in epoch 20, gen_loss = 0.7632074038209353, disc_loss = 0.08016502319765989
Testing Epoch 20
/work3/soeba/HALOS/utils.py:110: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, axs = plt.subplots(batch_size, 4, figsize=(8,10))
Training Epoch 21
Trained batch 915 in epoch 21, gen_loss = 0.7655638165804497, disc_loss = 0.0800743827110311
Testing Epoch 21
Training Epoch 22
Trained batch 915 in epoch 22, gen_loss = 0.7651108794532488, disc_loss = 0.07624916700691017
Testing Epoch 22
Training Epoch 23
Trained batch 915 in epoch 23, gen_loss = 0.7776428478969236, disc_loss = 0.07047481542884262
Testing Epoch 23
Training Epoch 24
Trained batch 915 in epoch 24, gen_loss = 0.7813942629242048, disc_loss = 0.07158089006384713
Testing Epoch 24