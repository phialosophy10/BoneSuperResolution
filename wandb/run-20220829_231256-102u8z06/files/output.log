wandb: WARNING Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.
/work3/soeba/HALOS/halos/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.
  warnings.warn(
/work3/soeba/HALOS/halos/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Training Epoch 0
Trained batch 0 in epoch 0, gen_loss = 0.5231918096542358, disc_loss = 0.5269511938095093
Trained batch 1 in epoch 0, gen_loss = 0.5101669728755951, disc_loss = 0.5363621711730957
Trained batch 2 in epoch 0, gen_loss = 0.4945052961508433, disc_loss = 0.5838287274042765
Trained batch 3 in epoch 0, gen_loss = 0.46875277906656265, disc_loss = 0.5343799367547035
Trained batch 4 in epoch 0, gen_loss = 0.47035870552062986, disc_loss = 0.48735093474388125
Trained batch 5 in epoch 0, gen_loss = 0.45999206602573395, disc_loss = 0.44227323432763416
Trained batch 6 in epoch 0, gen_loss = 0.45396892939295086, disc_loss = 0.3992647911821093
Trained batch 7 in epoch 0, gen_loss = 0.4570238031446934, disc_loss = 0.3759729899466038
Trained batch 8 in epoch 0, gen_loss = 0.4548349877198537, disc_loss = 0.35205012891027665
Trained batch 9 in epoch 0, gen_loss = 0.45507156252861025, disc_loss = 0.3284956872463226
Trained batch 10 in epoch 0, gen_loss = 0.46059171719984576, disc_loss = 0.30900325016541913
Trained batch 11 in epoch 0, gen_loss = 0.4572920302549998, disc_loss = 0.29035641625523567
Trained batch 12 in epoch 0, gen_loss = 0.4587509586260869, disc_loss = 0.2774949578138498
Trained batch 13 in epoch 0, gen_loss = 0.4581134468317032, disc_loss = 0.26781971965517315
Trained batch 14 in epoch 0, gen_loss = 0.45575400193532306, disc_loss = 0.2570310741662979
Trained batch 15 in epoch 0, gen_loss = 0.4597872644662857, disc_loss = 0.25171900540590286
Trained batch 16 in epoch 0, gen_loss = 0.457327746293124, disc_loss = 0.2446041440262514
Trained batch 17 in epoch 0, gen_loss = 0.46007611519760555, disc_loss = 0.23587473730246225
Trained batch 18 in epoch 0, gen_loss = 0.4560788838486922, disc_loss = 0.22716485826592697
Trained batch 19 in epoch 0, gen_loss = 0.4551855534315109, disc_loss = 0.22118468806147576
Trained batch 20 in epoch 0, gen_loss = 0.4563264733269101, disc_loss = 0.2176893814688637
Trained batch 21 in epoch 0, gen_loss = 0.45550812645391986, disc_loss = 0.21196066994558682
Trained batch 22 in epoch 0, gen_loss = 0.45419280943663226, disc_loss = 0.20969301591748776
Trained batch 23 in epoch 0, gen_loss = 0.45786933104197186, disc_loss = 0.20885217686494192
Trained batch 24 in epoch 0, gen_loss = 0.45757399916648867, disc_loss = 0.20441097825765608
Trained batch 25 in epoch 0, gen_loss = 0.4553932398557663, disc_loss = 0.20077875399818787
Trained batch 26 in epoch 0, gen_loss = 0.4525856132860537, disc_loss = 0.200750303765138
Trained batch 27 in epoch 0, gen_loss = 0.45539761866842, disc_loss = 0.1983877838190113
Trained batch 28 in epoch 0, gen_loss = 0.4529710874475282, disc_loss = 0.19472530098824664
Trained batch 29 in epoch 0, gen_loss = 0.4514178733030955, disc_loss = 0.19211633130908012
Trained batch 30 in epoch 0, gen_loss = 0.4519226724101651, disc_loss = 0.18889819494178217
Trained batch 31 in epoch 0, gen_loss = 0.45148772466927767, disc_loss = 0.1850017209071666
Trained batch 32 in epoch 0, gen_loss = 0.4515504584167943, disc_loss = 0.1810892831647035
Trained batch 33 in epoch 0, gen_loss = 0.4520584169556113, disc_loss = 0.17727318997768796
Trained batch 34 in epoch 0, gen_loss = 0.45481545414243424, disc_loss = 0.17370515359299524
Trained batch 35 in epoch 0, gen_loss = 0.4545009136199951, disc_loss = 0.1701983996770448
Trained batch 36 in epoch 0, gen_loss = 0.45483949136089635, disc_loss = 0.16687390721730283
Trained batch 37 in epoch 0, gen_loss = 0.4557226559049205, disc_loss = 0.16377944489450832
Trained batch 38 in epoch 0, gen_loss = 0.4558531573185554, disc_loss = 0.16098566907338607
Trained batch 39 in epoch 0, gen_loss = 0.4557714261114597, disc_loss = 0.1578482149168849
Trained batch 40 in epoch 0, gen_loss = 0.45511950007299096, disc_loss = 0.15512337135832485
Trained batch 41 in epoch 0, gen_loss = 0.4560539715346836, disc_loss = 0.15236262817467963
Trained batch 42 in epoch 0, gen_loss = 0.45747328428334966, disc_loss = 0.14987850189208984
Trained batch 43 in epoch 0, gen_loss = 0.4566295932639729, disc_loss = 0.14941940998489206
Trained batch 44 in epoch 0, gen_loss = 0.4563378347290887, disc_loss = 0.1494110686911477
Trained batch 45 in epoch 0, gen_loss = 0.4562998271506766, disc_loss = 0.14724102600113206
Trained batch 46 in epoch 0, gen_loss = 0.45672572990681265, disc_loss = 0.14624739406590767
Trained batch 47 in epoch 0, gen_loss = 0.45622946446140605, disc_loss = 0.14575513560945788
Trained batch 48 in epoch 0, gen_loss = 0.4564560007075874, disc_loss = 0.1439935355162134
Trained batch 49 in epoch 0, gen_loss = 0.45677014350891115, disc_loss = 0.14180996015667915
Trained batch 50 in epoch 0, gen_loss = 0.45539439308877083, disc_loss = 0.14020021887970904
Trained batch 51 in epoch 0, gen_loss = 0.457099029651055, disc_loss = 0.13842318737162992
Trained batch 52 in epoch 0, gen_loss = 0.4587043669988524, disc_loss = 0.13650165034352607
Trained batch 53 in epoch 0, gen_loss = 0.45853208005428314, disc_loss = 0.13443843585749468
Trained batch 54 in epoch 0, gen_loss = 0.4576952571218664, disc_loss = 0.13246723968874324
Trained batch 55 in epoch 0, gen_loss = 0.4583630700196539, disc_loss = 0.13061031902075879
Trained batch 56 in epoch 0, gen_loss = 0.4588683383506641, disc_loss = 0.12887867458426117
Trained batch 57 in epoch 0, gen_loss = 0.4590657266049549, disc_loss = 0.12706782665617508
Trained batch 58 in epoch 0, gen_loss = 0.45758174385054634, disc_loss = 0.12573720511617298
Trained batch 59 in epoch 0, gen_loss = 0.4570780649781227, disc_loss = 0.12503091540808478
Trained batch 60 in epoch 0, gen_loss = 0.4571503356831973, disc_loss = 0.1234047377756873
Trained batch 61 in epoch 0, gen_loss = 0.45903791391080423, disc_loss = 0.12201825111743904
Trained batch 62 in epoch 0, gen_loss = 0.4592624666198852, disc_loss = 0.1205544120203408
Trained batch 63 in epoch 0, gen_loss = 0.4582630004733801, disc_loss = 0.11924497011932544
Trained batch 64 in epoch 0, gen_loss = 0.45802800884613626, disc_loss = 0.11782046467638933
Trained batch 65 in epoch 0, gen_loss = 0.45831865704420843, disc_loss = 0.11631491512173053
Trained batch 66 in epoch 0, gen_loss = 0.4584821240225835, disc_loss = 0.11485169141261435
Trained batch 67 in epoch 0, gen_loss = 0.459025823894669, disc_loss = 0.11344726526123636
Trained batch 68 in epoch 0, gen_loss = 0.45965804224428924, disc_loss = 0.11209741394049015
Trained batch 69 in epoch 0, gen_loss = 0.45948851108551025, disc_loss = 0.11078821173203843
Trained batch 70 in epoch 0, gen_loss = 0.4596463811229652, disc_loss = 0.10948373070380217
Trained batch 71 in epoch 0, gen_loss = 0.45967016576064956, disc_loss = 0.10821395776131087
Trained batch 72 in epoch 0, gen_loss = 0.45989120741413064, disc_loss = 0.10699848823045215
Trained batch 73 in epoch 0, gen_loss = 0.45904205860318364, disc_loss = 0.10580981575657387
Trained batch 74 in epoch 0, gen_loss = 0.45874698996543883, disc_loss = 0.10462005473673344
Trained batch 75 in epoch 0, gen_loss = 0.45843999244664846, disc_loss = 0.10344851536578253
Trained batch 76 in epoch 0, gen_loss = 0.4585411664727446, disc_loss = 0.10233089199604152
Trained batch 77 in epoch 0, gen_loss = 0.459545754469358, disc_loss = 0.10121953578140491
Trained batch 78 in epoch 0, gen_loss = 0.46025794744491577, disc_loss = 0.10016690576566925
Trained batch 79 in epoch 0, gen_loss = 0.46079868227243426, disc_loss = 0.09916770998388529
Trained batch 80 in epoch 0, gen_loss = 0.461347551257522, disc_loss = 0.09814478436277972
Trained batch 81 in epoch 0, gen_loss = 0.4607269255126395, disc_loss = 0.09714676273941267
Trained batch 82 in epoch 0, gen_loss = 0.46072375199880944, disc_loss = 0.09617571880569659
Trained batch 83 in epoch 0, gen_loss = 0.46057545358226415, disc_loss = 0.09519099661459525
Trained batch 84 in epoch 0, gen_loss = 0.46086144482388214, disc_loss = 0.09427188122097184
Trained batch 85 in epoch 0, gen_loss = 0.4604263458141061, disc_loss = 0.09338278340738873
Trained batch 86 in epoch 0, gen_loss = 0.4601866959155291, disc_loss = 0.09250883227107169
Trained batch 87 in epoch 0, gen_loss = 0.45947896655310283, disc_loss = 0.09173037638803097
Trained batch 88 in epoch 0, gen_loss = 0.4582559848099612, disc_loss = 0.09094915047204227
Trained batch 89 in epoch 0, gen_loss = 0.4582774586147732, disc_loss = 0.0903025110769603
Trained batch 90 in epoch 0, gen_loss = 0.4585243532290825, disc_loss = 0.08953193585378127
Trained batch 91 in epoch 0, gen_loss = 0.45837166549071023, disc_loss = 0.08873797181750769
Trained batch 92 in epoch 0, gen_loss = 0.4593719324117066, disc_loss = 0.08798553884750412
Trained batch 93 in epoch 0, gen_loss = 0.4599125236272812, disc_loss = 0.08726572671389961
Trained batch 94 in epoch 0, gen_loss = 0.45942660946595043, disc_loss = 0.08648262694478034
Trained batch 95 in epoch 0, gen_loss = 0.45890737355997163, disc_loss = 0.08572690561413765
Trained batch 96 in epoch 0, gen_loss = 0.45837346641058774, disc_loss = 0.08497840766170897
Trained batch 97 in epoch 0, gen_loss = 0.459349695212987, disc_loss = 0.08426750438021761
Trained batch 98 in epoch 0, gen_loss = 0.45906072644272233, disc_loss = 0.08356653784185347
Trained batch 99 in epoch 0, gen_loss = 0.45862445414066316, disc_loss = 0.08285717567428946
Trained batch 100 in epoch 0, gen_loss = 0.458519585946999, disc_loss = 0.08215449154745824
Trained batch 101 in epoch 0, gen_loss = 0.4582909030072829, disc_loss = 0.08149117182063706
Trained batch 102 in epoch 0, gen_loss = 0.4574022235222233, disc_loss = 0.0809201736147832
Trained batch 103 in epoch 0, gen_loss = 0.4566452239568417, disc_loss = 0.0804511250462383
Trained batch 104 in epoch 0, gen_loss = 0.4568674433799017, disc_loss = 0.07996731393394016
Trained batch 105 in epoch 0, gen_loss = 0.4560690904563328, disc_loss = 0.07981704997847665
Trained batch 106 in epoch 0, gen_loss = 0.4542619605487752, disc_loss = 0.0810179680446598
Trained batch 107 in epoch 0, gen_loss = 0.45513480929312883, disc_loss = 0.08145324920338613
Trained batch 108 in epoch 0, gen_loss = 0.45568125647142393, disc_loss = 0.0814333871827213
Trained batch 109 in epoch 0, gen_loss = 0.4556069276549599, disc_loss = 0.08098465465009212
Trained batch 110 in epoch 0, gen_loss = 0.4548370515978014, disc_loss = 0.08042171120912105
Trained batch 111 in epoch 0, gen_loss = 0.4544501051838909, disc_loss = 0.07996002933941782
Trained batch 112 in epoch 0, gen_loss = 0.45387335794161904, disc_loss = 0.07948935035187586
Trained batch 113 in epoch 0, gen_loss = 0.45335676717130763, disc_loss = 0.07908592693376959
Trained batch 114 in epoch 0, gen_loss = 0.4534049391746521, disc_loss = 0.07871911545162616
Trained batch 115 in epoch 0, gen_loss = 0.45317917599760255, disc_loss = 0.07829176070939364
Trained batch 116 in epoch 0, gen_loss = 0.45273132456673515, disc_loss = 0.0780383524699853
Trained batch 117 in epoch 0, gen_loss = 0.4530294865874921, disc_loss = 0.07784401049237635
Trained batch 118 in epoch 0, gen_loss = 0.45287467802272124, disc_loss = 0.07757131572218001
Trained batch 119 in epoch 0, gen_loss = 0.4535191997885704, disc_loss = 0.07722409285294513
Trained batch 120 in epoch 0, gen_loss = 0.4537323540892483, disc_loss = 0.07771589174435652
Trained batch 121 in epoch 0, gen_loss = 0.4536569426294233, disc_loss = 0.08162402598278932
Trained batch 122 in epoch 0, gen_loss = 0.45331525269562634, disc_loss = 0.08190872052275552
Trained batch 123 in epoch 0, gen_loss = 0.4539496047842887, disc_loss = 0.08223167396781425
Trained batch 124 in epoch 0, gen_loss = 0.4545106201171875, disc_loss = 0.08273276053369046
Trained batch 125 in epoch 0, gen_loss = 0.4548604843162355, disc_loss = 0.08359262499485225
Trained batch 126 in epoch 0, gen_loss = 0.4545510289237255, disc_loss = 0.08438277864960704
Trained batch 127 in epoch 0, gen_loss = 0.45468269078992307, disc_loss = 0.08462754018546548
Trained batch 128 in epoch 0, gen_loss = 0.45443149084268614, disc_loss = 0.08446035885013813
Trained batch 129 in epoch 0, gen_loss = 0.45415655099428615, disc_loss = 0.0844527449602118
Trained batch 130 in epoch 0, gen_loss = 0.4539671062513162, disc_loss = 0.08605779296512367
Trained batch 131 in epoch 0, gen_loss = 0.45402217949881696, disc_loss = 0.0892229124085244
Trained batch 132 in epoch 0, gen_loss = 0.45374289984093574, disc_loss = 0.08926273528066345
Trained batch 133 in epoch 0, gen_loss = 0.45347194929621115, disc_loss = 0.08964996499968554
Trained batch 134 in epoch 0, gen_loss = 0.4527994003560808, disc_loss = 0.08952964228336457
Trained batch 135 in epoch 0, gen_loss = 0.45281856244101243, disc_loss = 0.08928665117470219
Trained batch 136 in epoch 0, gen_loss = 0.4529691223245468, disc_loss = 0.08890031972886438
Trained batch 137 in epoch 0, gen_loss = 0.45288357095441956, disc_loss = 0.0885039236045618
Trained batch 138 in epoch 0, gen_loss = 0.4525241513046429, disc_loss = 0.08809638646491569
Trained batch 139 in epoch 0, gen_loss = 0.4525296915854726, disc_loss = 0.0879472782170134
Trained batch 140 in epoch 0, gen_loss = 0.4522173897171697, disc_loss = 0.0891601885701959
Trained batch 141 in epoch 0, gen_loss = 0.4518604981647411, disc_loss = 0.09233951512199472
Trained batch 142 in epoch 0, gen_loss = 0.45191286368803546, disc_loss = 0.09245383730353592
Trained batch 143 in epoch 0, gen_loss = 0.4517596550285816, disc_loss = 0.09364779926060389
Trained batch 144 in epoch 0, gen_loss = 0.4515694032455313, disc_loss = 0.09534739020826488
Trained batch 145 in epoch 0, gen_loss = 0.45175860209824287, disc_loss = 0.09648609989360996
Trained batch 146 in epoch 0, gen_loss = 0.45216044905234354, disc_loss = 0.09717239825954647
Trained batch 147 in epoch 0, gen_loss = 0.4517840857844095, disc_loss = 0.09792139362292113
Trained batch 148 in epoch 0, gen_loss = 0.45157490980705156, disc_loss = 0.09867912002107841
Trained batch 149 in epoch 0, gen_loss = 0.451174880862236, disc_loss = 0.09866577775528033
Trained batch 150 in epoch 0, gen_loss = 0.45132172995845216, disc_loss = 0.09881728967738072
Trained batch 151 in epoch 0, gen_loss = 0.4517455687256236, disc_loss = 0.09895925053493365
Trained batch 152 in epoch 0, gen_loss = 0.45188985365668155, disc_loss = 0.09901969905431364
Trained batch 153 in epoch 0, gen_loss = 0.45154736936092377, disc_loss = 0.09929169057686994
Trained batch 154 in epoch 0, gen_loss = 0.45135569303266465, disc_loss = 0.10160687624687148
Trained batch 155 in epoch 0, gen_loss = 0.4518958803934929, disc_loss = 0.10183008465295036
Trained batch 156 in epoch 0, gen_loss = 0.45236231756817763, disc_loss = 0.10233108558140364
Trained batch 157 in epoch 0, gen_loss = 0.4521239580987375, disc_loss = 0.10319072754369883
Trained batch 158 in epoch 0, gen_loss = 0.4521622751493874, disc_loss = 0.10345705829373321
Trained batch 159 in epoch 0, gen_loss = 0.4515684969723225, disc_loss = 0.10591341416584328
Trained batch 160 in epoch 0, gen_loss = 0.451592202882589, disc_loss = 0.10758246686193884
Trained batch 161 in epoch 0, gen_loss = 0.45161490896601736, disc_loss = 0.10773563358564804
Trained batch 162 in epoch 0, gen_loss = 0.4517667814632135, disc_loss = 0.10816404709101089
Trained batch 163 in epoch 0, gen_loss = 0.45153113400063866, disc_loss = 0.10858306026358794
Trained batch 164 in epoch 0, gen_loss = 0.451121277700771, disc_loss = 0.10879372043365781
Trained batch 165 in epoch 0, gen_loss = 0.45062922084906015, disc_loss = 0.10878499221415763
Trained batch 166 in epoch 0, gen_loss = 0.45076075356877493, disc_loss = 0.1087472707501607
Trained batch 167 in epoch 0, gen_loss = 0.45091000270275844, disc_loss = 0.10852657969198412
Trained batch 168 in epoch 0, gen_loss = 0.4507460945104001, disc_loss = 0.10825507458541873
Trained batch 169 in epoch 0, gen_loss = 0.45075840704581316, disc_loss = 0.10819149053491214
Trained batch 170 in epoch 0, gen_loss = 0.4508126231662014, disc_loss = 0.10832064551960306
Trained batch 171 in epoch 0, gen_loss = 0.4509399195396623, disc_loss = 0.10901114033795027
Trained batch 172 in epoch 0, gen_loss = 0.4509201552826545, disc_loss = 0.1103771413444956
Trained batch 173 in epoch 0, gen_loss = 0.4507350966163065, disc_loss = 0.11018053323415847
Trained batch 174 in epoch 0, gen_loss = 0.45079984171049936, disc_loss = 0.11016369650406496
Trained batch 175 in epoch 0, gen_loss = 0.45073799179358914, disc_loss = 0.10998172051569616
Trained batch 176 in epoch 0, gen_loss = 0.45079668514472615, disc_loss = 0.109796942511605
Trained batch 177 in epoch 0, gen_loss = 0.4509681419710095, disc_loss = 0.11003994126542566
Trained batch 178 in epoch 0, gen_loss = 0.4509295521501722, disc_loss = 0.11116010988099947
Trained batch 179 in epoch 0, gen_loss = 0.45182736416657765, disc_loss = 0.11151205397521456
Trained batch 180 in epoch 0, gen_loss = 0.4514678976812415, disc_loss = 0.11146560291764815
Trained batch 181 in epoch 0, gen_loss = 0.4513187270898085, disc_loss = 0.11124921378239498
Trained batch 182 in epoch 0, gen_loss = 0.45115007321691253, disc_loss = 0.11144880210482032
Trained batch 183 in epoch 0, gen_loss = 0.45112057548502216, disc_loss = 0.11216773603961844
Trained batch 184 in epoch 0, gen_loss = 0.4512910266180296, disc_loss = 0.11219220178554187
Trained batch 185 in epoch 0, gen_loss = 0.4510987735884164, disc_loss = 0.11212353769850025
Trained batch 186 in epoch 0, gen_loss = 0.45121818541842984, disc_loss = 0.11199777263570278
Trained batch 187 in epoch 0, gen_loss = 0.45087017753022784, disc_loss = 0.11207559701808273
Trained batch 188 in epoch 0, gen_loss = 0.4507713932839651, disc_loss = 0.11274748872078601
Trained batch 189 in epoch 0, gen_loss = 0.4509178395334043, disc_loss = 0.11375405134535149
Trained batch 190 in epoch 0, gen_loss = 0.45088367608829316, disc_loss = 0.11435117152231838
Trained batch 191 in epoch 0, gen_loss = 0.45074198658888537, disc_loss = 0.11439397881622426
Trained batch 192 in epoch 0, gen_loss = 0.4508760855605565, disc_loss = 0.11432555589072137
Trained batch 193 in epoch 0, gen_loss = 0.450537605076721, disc_loss = 0.11441727966723061
Trained batch 194 in epoch 0, gen_loss = 0.4514981691653912, disc_loss = 0.11490777212266738
Trained batch 195 in epoch 0, gen_loss = 0.4515032645086853, disc_loss = 0.11599273037887653
Trained batch 196 in epoch 0, gen_loss = 0.4511867223052204, disc_loss = 0.11611998740224366
Trained batch 197 in epoch 0, gen_loss = 0.4512453849869545, disc_loss = 0.11612297655694713
Trained batch 198 in epoch 0, gen_loss = 0.45158019047885684, disc_loss = 0.1159446594497217
Trained batch 199 in epoch 0, gen_loss = 0.45141193628311155, disc_loss = 0.11582208060659469
Trained batch 200 in epoch 0, gen_loss = 0.4516516124727714, disc_loss = 0.11582210427381802
Trained batch 201 in epoch 0, gen_loss = 0.4521404771816612, disc_loss = 0.116019028736887
Trained batch 202 in epoch 0, gen_loss = 0.45173227331908467, disc_loss = 0.11632157022619775
Trained batch 203 in epoch 0, gen_loss = 0.45154089860472024, disc_loss = 0.11623576054276497
Trained batch 204 in epoch 0, gen_loss = 0.45132559843179654, disc_loss = 0.11604611412962762
Trained batch 205 in epoch 0, gen_loss = 0.4511936613656942, disc_loss = 0.11618920528664461
Trained batch 206 in epoch 0, gen_loss = 0.4511537294168979, disc_loss = 0.11747092963308815
Trained batch 207 in epoch 0, gen_loss = 0.4516946299431416, disc_loss = 0.11725306819873647
Trained batch 208 in epoch 0, gen_loss = 0.4515932233710038, disc_loss = 0.11747649605229996
Trained batch 209 in epoch 0, gen_loss = 0.45112101307937075, disc_loss = 0.11793102246842214
Trained batch 210 in epoch 0, gen_loss = 0.45089908050134847, disc_loss = 0.11789699040918271
Trained batch 211 in epoch 0, gen_loss = 0.4509836138700539, disc_loss = 0.11814179075530397
Trained batch 212 in epoch 0, gen_loss = 0.45060441233742404, disc_loss = 0.11837234163543148
Trained batch 213 in epoch 0, gen_loss = 0.45039255917072296, disc_loss = 0.11842706194582665
Trained batch 214 in epoch 0, gen_loss = 0.45054696327032046, disc_loss = 0.11834481179194395
Trained batch 215 in epoch 0, gen_loss = 0.45048459205362534, disc_loss = 0.11847873626242357
Trained batch 216 in epoch 0, gen_loss = 0.4502174548015067, disc_loss = 0.11916141288380744
Trained batch 217 in epoch 0, gen_loss = 0.4502975819034314, disc_loss = 0.11945451865263215
Trained batch 218 in epoch 0, gen_loss = 0.45001891704454816, disc_loss = 0.11981826293448063
Trained batch 219 in epoch 0, gen_loss = 0.44971249347383324, disc_loss = 0.12007297935302962
Trained batch 220 in epoch 0, gen_loss = 0.44921994478993826, disc_loss = 0.12051799415496948
Trained batch 221 in epoch 0, gen_loss = 0.44886875904358187, disc_loss = 0.12076392361206247
Trained batch 222 in epoch 0, gen_loss = 0.44865317088071544, disc_loss = 0.12106583612415556
Trained batch 223 in epoch 0, gen_loss = 0.4486282145870583, disc_loss = 0.12119677011755162
Trained batch 224 in epoch 0, gen_loss = 0.4484430730342865, disc_loss = 0.12127491620679696
Trained batch 225 in epoch 0, gen_loss = 0.44812246023026187, disc_loss = 0.12158575575732816
Trained batch 226 in epoch 0, gen_loss = 0.44766913964884925, disc_loss = 0.12342118167437384
Trained batch 227 in epoch 0, gen_loss = 0.44777592650631015, disc_loss = 0.12408556698478367
Trained batch 228 in epoch 0, gen_loss = 0.44771007601350676, disc_loss = 0.1245666397532131
Trained batch 229 in epoch 0, gen_loss = 0.447391641917436, disc_loss = 0.12498921616731778
Trained batch 230 in epoch 0, gen_loss = 0.44701792590029826, disc_loss = 0.12525817314041898
Trained batch 231 in epoch 0, gen_loss = 0.4468344772427246, disc_loss = 0.12559107415667126
Trained batch 232 in epoch 0, gen_loss = 0.44634081941305825, disc_loss = 0.1260127444459773
Trained batch 233 in epoch 0, gen_loss = 0.44633494113755023, disc_loss = 0.12622075808895195
Trained batch 234 in epoch 0, gen_loss = 0.4458056496812942, disc_loss = 0.1266283622526742
Trained batch 235 in epoch 0, gen_loss = 0.4455460272097992, disc_loss = 0.12706853279654504
Trained batch 236 in epoch 0, gen_loss = 0.44526566156355135, disc_loss = 0.1274570348094913
Trained batch 237 in epoch 0, gen_loss = 0.4451878059561513, disc_loss = 0.1276255387171101
Trained batch 238 in epoch 0, gen_loss = 0.4448490863564623, disc_loss = 0.12802956737266175
Trained batch 239 in epoch 0, gen_loss = 0.4447028727581104, disc_loss = 0.12837298022738347
Trained batch 240 in epoch 0, gen_loss = 0.444388131259388, disc_loss = 0.12842944827842762
Trained batch 241 in epoch 0, gen_loss = 0.44438817111913825, disc_loss = 0.12891368796540933
Trained batch 242 in epoch 0, gen_loss = 0.44439162945551147, disc_loss = 0.12979607039548977
Trained batch 243 in epoch 0, gen_loss = 0.4438739498374892, disc_loss = 0.1301645568236098
Trained batch 244 in epoch 0, gen_loss = 0.44361424969167124, disc_loss = 0.13051542421536785
Trained batch 245 in epoch 0, gen_loss = 0.443592432916649, disc_loss = 0.1309406462435921
Trained batch 246 in epoch 0, gen_loss = 0.4433299769998079, disc_loss = 0.13127145515792524
Trained batch 247 in epoch 0, gen_loss = 0.44297866859743673, disc_loss = 0.1317035790247422
Trained batch 248 in epoch 0, gen_loss = 0.442656056229848, disc_loss = 0.1319681067378765
Trained batch 249 in epoch 0, gen_loss = 0.44250634157657626, disc_loss = 0.13213732708245515
Trained batch 250 in epoch 0, gen_loss = 0.44211536444040883, disc_loss = 0.13247264477153461
Trained batch 251 in epoch 0, gen_loss = 0.44166597357345005, disc_loss = 0.13291371306995786
Trained batch 252 in epoch 0, gen_loss = 0.44137981369090173, disc_loss = 0.133157626685241
Trained batch 253 in epoch 0, gen_loss = 0.44116574338101966, disc_loss = 0.133126715044161
Trained batch 254 in epoch 0, gen_loss = 0.44118907428255266, disc_loss = 0.13334406054049147
Trained batch 255 in epoch 0, gen_loss = 0.44113469461444765, disc_loss = 0.13412503631116124
Trained batch 256 in epoch 0, gen_loss = 0.4410720060539617, disc_loss = 0.13451990346761297
Trained batch 257 in epoch 0, gen_loss = 0.4410748299240142, disc_loss = 0.13471770749333523
Trained batch 258 in epoch 0, gen_loss = 0.44098874305205915, disc_loss = 0.13493045060589737
Trained batch 259 in epoch 0, gen_loss = 0.44071989139685264, disc_loss = 0.13517158198385285
Trained batch 260 in epoch 0, gen_loss = 0.4405954467610838, disc_loss = 0.13553188770467064
Trained batch 261 in epoch 0, gen_loss = 0.4403866567001998, disc_loss = 0.13584148986904904
Trained batch 262 in epoch 0, gen_loss = 0.4400186273534942, disc_loss = 0.13607767508312096
Trained batch 263 in epoch 0, gen_loss = 0.4398243789645759, disc_loss = 0.13633619757566715
Trained batch 264 in epoch 0, gen_loss = 0.43978051756912806, disc_loss = 0.13677076866744825
Trained batch 265 in epoch 0, gen_loss = 0.4393973030093917, disc_loss = 0.1369552314911682
Trained batch 266 in epoch 0, gen_loss = 0.43890004390187926, disc_loss = 0.13738484603216808
Trained batch 267 in epoch 0, gen_loss = 0.43856403576349146, disc_loss = 0.1377267530674476
Trained batch 268 in epoch 0, gen_loss = 0.43854988175253884, disc_loss = 0.13798973541083612
Trained batch 269 in epoch 0, gen_loss = 0.43829500675201416, disc_loss = 0.13812065953043876
Trained batch 270 in epoch 0, gen_loss = 0.43834609837989524, disc_loss = 0.1382193567636818
Trained batch 271 in epoch 0, gen_loss = 0.438035060377682, disc_loss = 0.13856100625878967
Trained batch 272 in epoch 0, gen_loss = 0.4378239071412838, disc_loss = 0.13924889594671272
Trained batch 273 in epoch 0, gen_loss = 0.4376910185291819, disc_loss = 0.1395808353979331
Trained batch 274 in epoch 0, gen_loss = 0.4376329735192386, disc_loss = 0.1397004115649245
Trained batch 275 in epoch 0, gen_loss = 0.4373657124629919, disc_loss = 0.14003217836464013
Trained batch 276 in epoch 0, gen_loss = 0.43706173156573025, disc_loss = 0.14040864531340796
Trained batch 277 in epoch 0, gen_loss = 0.43682176024793723, disc_loss = 0.14066772694237156
Trained batch 278 in epoch 0, gen_loss = 0.4364181728132309, disc_loss = 0.14088355737160824
Trained batch 279 in epoch 0, gen_loss = 0.4362324055816446, disc_loss = 0.14129653594988797
Trained batch 280 in epoch 0, gen_loss = 0.4362742478737203, disc_loss = 0.14152231261637924
Trained batch 281 in epoch 0, gen_loss = 0.436043011486953, disc_loss = 0.1415356606416774
Trained batch 282 in epoch 0, gen_loss = 0.43598592881600345, disc_loss = 0.14146596762638
Trained batch 283 in epoch 0, gen_loss = 0.4361423373642102, disc_loss = 0.14156566839009313
Trained batch 284 in epoch 0, gen_loss = 0.4360495881030434, disc_loss = 0.14198226233976974
Trained batch 285 in epoch 0, gen_loss = 0.4356767898881352, disc_loss = 0.14245841256831285
Trained batch 286 in epoch 0, gen_loss = 0.435403255427756, disc_loss = 0.14278347962227642
Trained batch 287 in epoch 0, gen_loss = 0.4353536932418744, disc_loss = 0.14311560704502174
Trained batch 288 in epoch 0, gen_loss = 0.4352360773045299, disc_loss = 0.1437529538497071
Trained batch 289 in epoch 0, gen_loss = 0.4351187578563033, disc_loss = 0.14384145254333472
Trained batch 290 in epoch 0, gen_loss = 0.43496535129563507, disc_loss = 0.1437906712050393
Trained batch 291 in epoch 0, gen_loss = 0.4348314112588151, disc_loss = 0.14378397488864522
Trained batch 292 in epoch 0, gen_loss = 0.43469459133750343, disc_loss = 0.1439847202189648
Trained batch 293 in epoch 0, gen_loss = 0.43444436646643136, disc_loss = 0.14424510174082453
Trained batch 294 in epoch 0, gen_loss = 0.43418613783383775, disc_loss = 0.14432665948771825
Trained batch 295 in epoch 0, gen_loss = 0.43407522715829516, disc_loss = 0.14436749485321343
Trained batch 296 in epoch 0, gen_loss = 0.4343031449149353, disc_loss = 0.1443231463595372
Trained batch 297 in epoch 0, gen_loss = 0.43436843876870684, disc_loss = 0.1447392610658035
Trained batch 298 in epoch 0, gen_loss = 0.4339057687532942, disc_loss = 0.1452961133039217
Trained batch 299 in epoch 0, gen_loss = 0.4339004495739937, disc_loss = 0.14514957170312603
Trained batch 300 in epoch 0, gen_loss = 0.433766936840013, disc_loss = 0.14537238223830926
Trained batch 301 in epoch 0, gen_loss = 0.4336516447611992, disc_loss = 0.14556819753758363
Trained batch 302 in epoch 0, gen_loss = 0.43358497464224055, disc_loss = 0.14596586869322625
Trained batch 303 in epoch 0, gen_loss = 0.4332680662015551, disc_loss = 0.1463150834039736
Trained batch 304 in epoch 0, gen_loss = 0.43293540507066447, disc_loss = 0.14703097549251845
Trained batch 305 in epoch 0, gen_loss = 0.43274585873472926, disc_loss = 0.14709359524905488
Trained batch 306 in epoch 0, gen_loss = 0.4327524566495069, disc_loss = 0.14712776855872003
Trained batch 307 in epoch 0, gen_loss = 0.4324486997800988, disc_loss = 0.14729549973453213
Trained batch 308 in epoch 0, gen_loss = 0.43247892430299306, disc_loss = 0.1472836149898644
Trained batch 309 in epoch 0, gen_loss = 0.43223924742590997, disc_loss = 0.14749730638558825
Trained batch 310 in epoch 0, gen_loss = 0.43189880414791043, disc_loss = 0.14781674034174808
Trained batch 311 in epoch 0, gen_loss = 0.43201618536542624, disc_loss = 0.1477330839082312
Trained batch 312 in epoch 0, gen_loss = 0.43189003463751213, disc_loss = 0.14800254329920007
Trained batch 313 in epoch 0, gen_loss = 0.4318467357735725, disc_loss = 0.1479322275882409
Trained batch 314 in epoch 0, gen_loss = 0.4317312233031742, disc_loss = 0.14793708935261718
Trained batch 315 in epoch 0, gen_loss = 0.4317951296703725, disc_loss = 0.14817071037036897
Trained batch 316 in epoch 0, gen_loss = 0.43156747637486986, disc_loss = 0.14824917772050347
Trained batch 317 in epoch 0, gen_loss = 0.4315397447947436, disc_loss = 0.1483291130360661
Trained batch 318 in epoch 0, gen_loss = 0.43130059703764123, disc_loss = 0.14883007931882125
Trained batch 319 in epoch 0, gen_loss = 0.4309417019598186, disc_loss = 0.14903905839310028
Trained batch 320 in epoch 0, gen_loss = 0.43070626880892343, disc_loss = 0.14922433470106014
Trained batch 321 in epoch 0, gen_loss = 0.4306824133262871, disc_loss = 0.14938348088043263
Trained batch 322 in epoch 0, gen_loss = 0.4304575887811442, disc_loss = 0.1495902435940578
Trained batch 323 in epoch 0, gen_loss = 0.43045408453470396, disc_loss = 0.14956750744309874
Trained batch 324 in epoch 0, gen_loss = 0.43022583411290094, disc_loss = 0.14952337921238862
Trained batch 325 in epoch 0, gen_loss = 0.4299036478338066, disc_loss = 0.14961902016168174
Trained batch 326 in epoch 0, gen_loss = 0.42969605372221825, disc_loss = 0.14963456970676553
Trained batch 327 in epoch 0, gen_loss = 0.42951859097655226, disc_loss = 0.149855112571769
Trained batch 328 in epoch 0, gen_loss = 0.42926241652219127, disc_loss = 0.14979778362104051
Trained batch 329 in epoch 0, gen_loss = 0.42908377412593723, disc_loss = 0.14984453375253715
Trained batch 330 in epoch 0, gen_loss = 0.42868375877239195, disc_loss = 0.14981003691445124
Trained batch 331 in epoch 0, gen_loss = 0.4287541604724275, disc_loss = 0.14975540250741753
Trained batch 332 in epoch 0, gen_loss = 0.428705427292231, disc_loss = 0.14977492307600854
Trained batch 333 in epoch 0, gen_loss = 0.4283177153971381, disc_loss = 0.14972086077344096
Trained batch 334 in epoch 0, gen_loss = 0.4281160436459442, disc_loss = 0.14982172429228008
Trained batch 335 in epoch 0, gen_loss = 0.4280523828097752, disc_loss = 0.14983976255947104
Trained batch 336 in epoch 0, gen_loss = 0.4276859707223909, disc_loss = 0.14980344320108524
Trained batch 337 in epoch 0, gen_loss = 0.42763819664540376, disc_loss = 0.14985764080096456
Trained batch 338 in epoch 0, gen_loss = 0.42764640417070865, disc_loss = 0.1500188459128493
Trained batch 339 in epoch 0, gen_loss = 0.4275962532443159, disc_loss = 0.1499912614824579
Trained batch 340 in epoch 0, gen_loss = 0.4271754195962833, disc_loss = 0.15015517079607307
Trained batch 341 in epoch 0, gen_loss = 0.4271101683203937, disc_loss = 0.15018341298710708
Trained batch 342 in epoch 0, gen_loss = 0.42703341655759003, disc_loss = 0.15066049011755442
Trained batch 343 in epoch 0, gen_loss = 0.4268009071779806, disc_loss = 0.15098587748984438
Trained batch 344 in epoch 0, gen_loss = 0.42677673524704535, disc_loss = 0.1508526407344186
Trained batch 345 in epoch 0, gen_loss = 0.4266574442214359, disc_loss = 0.15121004149597678
Trained batch 346 in epoch 0, gen_loss = 0.4264598329919216, disc_loss = 0.15128292581898506
Trained batch 347 in epoch 0, gen_loss = 0.4264662186989839, disc_loss = 0.15125874391938934
Trained batch 348 in epoch 0, gen_loss = 0.42640731532799137, disc_loss = 0.15116516400907817
Trained batch 349 in epoch 0, gen_loss = 0.42641529083251956, disc_loss = 0.15109476247536285
Trained batch 350 in epoch 0, gen_loss = 0.4261757681342611, disc_loss = 0.15105556117694433
Trained batch 351 in epoch 0, gen_loss = 0.4261126753620126, disc_loss = 0.1508311181043444
Trained batch 352 in epoch 0, gen_loss = 0.4261485132212004, disc_loss = 0.15078824915245134
Trained batch 353 in epoch 0, gen_loss = 0.42595534573840554, disc_loss = 0.1513059575815743
Trained batch 354 in epoch 0, gen_loss = 0.42591609316812434, disc_loss = 0.15182353078164684
Trained batch 355 in epoch 0, gen_loss = 0.4258678277556816, disc_loss = 0.1518275528110229
Trained batch 356 in epoch 0, gen_loss = 0.4257172879551639, disc_loss = 0.15189785571346262
Trained batch 357 in epoch 0, gen_loss = 0.42544196521103717, disc_loss = 0.1519223774663646
Trained batch 358 in epoch 0, gen_loss = 0.4251453266336393, disc_loss = 0.15195993818135148
Trained batch 359 in epoch 0, gen_loss = 0.42486710656020377, disc_loss = 0.15181722467661732
Trained batch 360 in epoch 0, gen_loss = 0.4248424360626622, disc_loss = 0.1515893885437554
Trained batch 361 in epoch 0, gen_loss = 0.4245483761663595, disc_loss = 0.15153009000424686
Trained batch 362 in epoch 0, gen_loss = 0.42457503195308127, disc_loss = 0.15134304764452225
Trained batch 363 in epoch 0, gen_loss = 0.42454672424675344, disc_loss = 0.1511716574768198
Trained batch 364 in epoch 0, gen_loss = 0.4246020735126652, disc_loss = 0.15089785020339164
Trained batch 365 in epoch 0, gen_loss = 0.4244556168878013, disc_loss = 0.1509724464623736
Trained batch 366 in epoch 0, gen_loss = 0.42451608116035566, disc_loss = 0.1508391961424893
Trained batch 367 in epoch 0, gen_loss = 0.4245162525254747, disc_loss = 0.15150798134930918
Trained batch 368 in epoch 0, gen_loss = 0.42440477708167823, disc_loss = 0.15160174024238335
Trained batch 369 in epoch 0, gen_loss = 0.4243826925754547, disc_loss = 0.15171494071648733
Trained batch 370 in epoch 0, gen_loss = 0.42440123707457694, disc_loss = 0.15212472508638855
Trained batch 371 in epoch 0, gen_loss = 0.424605207017032, disc_loss = 0.15201253758903632
Trained batch 372 in epoch 0, gen_loss = 0.4245216602774152, disc_loss = 0.1519462650257285
Trained batch 373 in epoch 0, gen_loss = 0.42438502785037546, disc_loss = 0.15190002834673394
Trained batch 374 in epoch 0, gen_loss = 0.4243653651078542, disc_loss = 0.15187632455925146
Trained batch 375 in epoch 0, gen_loss = 0.42422852490810636, disc_loss = 0.15219283585951843
Trained batch 376 in epoch 0, gen_loss = 0.42422666046916646, disc_loss = 0.15198971612245557
Trained batch 377 in epoch 0, gen_loss = 0.4241707739691255, disc_loss = 0.1520410834530753
Trained batch 378 in epoch 0, gen_loss = 0.42429673797536965, disc_loss = 0.1520347279941265
Trained batch 379 in epoch 0, gen_loss = 0.42420616981230286, disc_loss = 0.1520033238798772
Trained batch 380 in epoch 0, gen_loss = 0.42414122194129966, disc_loss = 0.15192166925591277
Trained batch 381 in epoch 0, gen_loss = 0.42395971106921193, disc_loss = 0.15206737625680825
Trained batch 382 in epoch 0, gen_loss = 0.4238390890643117, disc_loss = 0.15186600622966615
Trained batch 383 in epoch 0, gen_loss = 0.42362793992894393, disc_loss = 0.15181876973656472
Trained batch 384 in epoch 0, gen_loss = 0.423457871242003, disc_loss = 0.15194828075344685
Trained batch 385 in epoch 0, gen_loss = 0.4232473578786603, disc_loss = 0.1519190005697503
Trained batch 386 in epoch 0, gen_loss = 0.42327778291640666, disc_loss = 0.15185855716486155
Trained batch 387 in epoch 0, gen_loss = 0.4229188109027971, disc_loss = 0.1518836965659614
Trained batch 388 in epoch 0, gen_loss = 0.4227838146196233, disc_loss = 0.1518367822198221
Trained batch 389 in epoch 0, gen_loss = 0.4227315251643841, disc_loss = 0.15183376798358483
Trained batch 390 in epoch 0, gen_loss = 0.4227288058956566, disc_loss = 0.15171057939091148
Trained batch 391 in epoch 0, gen_loss = 0.422724227135887, disc_loss = 0.15144735183661842
Trained batch 392 in epoch 0, gen_loss = 0.4224692356343791, disc_loss = 0.15142966640530198
Trained batch 393 in epoch 0, gen_loss = 0.4225283141819959, disc_loss = 0.15133246537865266
Trained batch 394 in epoch 0, gen_loss = 0.4225864039191717, disc_loss = 0.1511898386563304
Trained batch 395 in epoch 0, gen_loss = 0.4223557390799426, disc_loss = 0.15122224972348172
Trained batch 396 in epoch 0, gen_loss = 0.4222874486776683, disc_loss = 0.15124635168020462
Trained batch 397 in epoch 0, gen_loss = 0.42221667007285746, disc_loss = 0.1513952552832326
Trained batch 398 in epoch 0, gen_loss = 0.4223676586061492, disc_loss = 0.1512816681160141
Trained batch 399 in epoch 0, gen_loss = 0.42233590699732304, disc_loss = 0.1512000054726377
Trained batch 400 in epoch 0, gen_loss = 0.4223833440842474, disc_loss = 0.15103696257414811
Trained batch 401 in epoch 0, gen_loss = 0.422306194679061, disc_loss = 0.1510147902030331
Trained batch 402 in epoch 0, gen_loss = 0.4222614566978095, disc_loss = 0.15086900703978745
Trained batch 403 in epoch 0, gen_loss = 0.42234148785914527, disc_loss = 0.15071883971461714
Trained batch 404 in epoch 0, gen_loss = 0.42242543822453343, disc_loss = 0.150430126014499
Trained batch 405 in epoch 0, gen_loss = 0.422349305942728, disc_loss = 0.15024597686371338
Trained batch 406 in epoch 0, gen_loss = 0.4225223075406264, disc_loss = 0.14997113663108255
Trained batch 407 in epoch 0, gen_loss = 0.4225081765622485, disc_loss = 0.14987532802236578
Trained batch 408 in epoch 0, gen_loss = 0.4225750024132157, disc_loss = 0.14961972777574598
Trained batch 409 in epoch 0, gen_loss = 0.4225570192424262, disc_loss = 0.14974693223016292
Trained batch 410 in epoch 0, gen_loss = 0.42278904730676153, disc_loss = 0.15033758893482152
Trained batch 411 in epoch 0, gen_loss = 0.42274782604094846, disc_loss = 0.15006975311948692
Trained batch 412 in epoch 0, gen_loss = 0.42276550371190824, disc_loss = 0.14996079634649556
Trained batch 413 in epoch 0, gen_loss = 0.4228441031370762, disc_loss = 0.14964922888717358
Trained batch 414 in epoch 0, gen_loss = 0.422988078321319, disc_loss = 0.1493339026055063
Trained batch 415 in epoch 0, gen_loss = 0.4230958757778773, disc_loss = 0.14912055868458432
Trained batch 416 in epoch 0, gen_loss = 0.4232758953988695, disc_loss = 0.1487985345885622
Trained batch 417 in epoch 0, gen_loss = 0.42346903681755066, disc_loss = 0.14847580230523025
Trained batch 418 in epoch 0, gen_loss = 0.4236452608660468, disc_loss = 0.14823150641211533
Trained batch 419 in epoch 0, gen_loss = 0.42376767829770134, disc_loss = 0.1479440241275976
Trained batch 420 in epoch 0, gen_loss = 0.4239474895306268, disc_loss = 0.1476617877276486
Trained batch 421 in epoch 0, gen_loss = 0.42387592072170494, disc_loss = 0.14767288327243586
Trained batch 422 in epoch 0, gen_loss = 0.424169092454527, disc_loss = 0.14764860324147674
Trained batch 423 in epoch 0, gen_loss = 0.4241704114203183, disc_loss = 0.14799236684361086
Trained batch 424 in epoch 0, gen_loss = 0.4241389028465047, disc_loss = 0.14829586809829753
Trained batch 425 in epoch 0, gen_loss = 0.42391974748300276, disc_loss = 0.14916688283170823
Trained batch 426 in epoch 0, gen_loss = 0.4240513126381108, disc_loss = 0.15042082357069858
Trained batch 427 in epoch 0, gen_loss = 0.4240228994009651, disc_loss = 0.1506391657738311
Trained batch 428 in epoch 0, gen_loss = 0.4241082258435674, disc_loss = 0.15133469017539478
Trained batch 429 in epoch 0, gen_loss = 0.424130811178407, disc_loss = 0.15161673779014584
Trained batch 430 in epoch 0, gen_loss = 0.423905305408823, disc_loss = 0.1519106235722482
Trained batch 431 in epoch 0, gen_loss = 0.4237460341580488, disc_loss = 0.152154730622122
Trained batch 432 in epoch 0, gen_loss = 0.4234528297227194, disc_loss = 0.15238676299975526
Trained batch 433 in epoch 0, gen_loss = 0.4232978734964599, disc_loss = 0.1525995111165886
Trained batch 434 in epoch 0, gen_loss = 0.42312627249750595, disc_loss = 0.15279885234803647
Trained batch 435 in epoch 0, gen_loss = 0.42283101482402297, disc_loss = 0.1529679006080938
Trained batch 436 in epoch 0, gen_loss = 0.42263276667016597, disc_loss = 0.15313944833255128
Trained batch 437 in epoch 0, gen_loss = 0.4223885845103765, disc_loss = 0.1532742534764111
Trained batch 438 in epoch 0, gen_loss = 0.4223730514560039, disc_loss = 0.1534672930628774
Trained batch 439 in epoch 0, gen_loss = 0.4222445204176686, disc_loss = 0.1536376871740107
Trained batch 440 in epoch 0, gen_loss = 0.4219855817537459, disc_loss = 0.15376384234237495
Trained batch 441 in epoch 0, gen_loss = 0.42180547563199006, disc_loss = 0.15388461467662481
Trained batch 442 in epoch 0, gen_loss = 0.4217215335799663, disc_loss = 0.15405444648436378
Trained batch 443 in epoch 0, gen_loss = 0.4216087977359961, disc_loss = 0.15420456639037947
Trained batch 444 in epoch 0, gen_loss = 0.4214217805460598, disc_loss = 0.15435353556668827
Trained batch 445 in epoch 0, gen_loss = 0.4212279761452311, disc_loss = 0.15444948402914163
Trained batch 446 in epoch 0, gen_loss = 0.4210303768482251, disc_loss = 0.15454148865144457
Trained batch 447 in epoch 0, gen_loss = 0.4210328574824546, disc_loss = 0.15463254526548553
Trained batch 448 in epoch 0, gen_loss = 0.4208959852135792, disc_loss = 0.15476716718613637
Trained batch 449 in epoch 0, gen_loss = 0.4208517260683907, disc_loss = 0.15498181845371922
Trained batch 450 in epoch 0, gen_loss = 0.4207548162757425, disc_loss = 0.1551451226324669
Trained batch 451 in epoch 0, gen_loss = 0.42047431741931796, disc_loss = 0.1553314345233571
Trained batch 452 in epoch 0, gen_loss = 0.4201842694487793, disc_loss = 0.15552412146788758
Trained batch 453 in epoch 0, gen_loss = 0.42007877438078894, disc_loss = 0.1556895857303514
Trained batch 454 in epoch 0, gen_loss = 0.4199009549486768, disc_loss = 0.15590429950177997
Trained batch 455 in epoch 0, gen_loss = 0.4195634111631335, disc_loss = 0.15604492592862235
Trained batch 456 in epoch 0, gen_loss = 0.41944105209093907, disc_loss = 0.1562979663059898
Trained batch 457 in epoch 0, gen_loss = 0.4192089314804327, disc_loss = 0.15646562994331706
Trained batch 458 in epoch 0, gen_loss = 0.4191560995864453, disc_loss = 0.15658006989772477
Trained batch 459 in epoch 0, gen_loss = 0.41899681227362673, disc_loss = 0.1567671306330063
Trained batch 460 in epoch 0, gen_loss = 0.4188555274118312, disc_loss = 0.15693032123156522
Testing Epoch 0
Traceback (most recent call last):
  File "srgan_bones.py", line 330, in <module>
    img_grid = utils.make_thickness_images_dif(imgs_hr[:5], imgs_lr[:5], gen_hr[:5])
  File "/work3/soeba/HALOS/utils.py", line 56, in make_thickness_images_dif
    dif_im_tmp[:,:,0] = bin_im_hr
  File "/work3/soeba/HALOS/halos/lib/python3.8/site-packages/torch/_tensor.py", line 759, in __array__
    return self.numpy().astype(dtype, copy=False)
TypeError: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.