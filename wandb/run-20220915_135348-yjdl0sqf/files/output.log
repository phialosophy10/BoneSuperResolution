/work3/soeba/HALOS/halos/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.
  warnings.warn(
/work3/soeba/HALOS/halos/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Training Epoch 0
Trained batch 0 in epoch 0, gen_loss = 0.7980793714523315, disc_loss = 0.7908132672309875
Trained batch 1 in epoch 0, gen_loss = 0.7514790892601013, disc_loss = 0.6164034008979797
Trained batch 2 in epoch 0, gen_loss = 0.7265980243682861, disc_loss = 0.6256338953971863
Trained batch 3 in epoch 0, gen_loss = 0.6926123201847076, disc_loss = 0.6415263116359711
Trained batch 4 in epoch 0, gen_loss = 0.6614637851715088, disc_loss = 0.5707572340965271
Trained batch 5 in epoch 0, gen_loss = 0.6416875024636587, disc_loss = 0.513402392466863
Trained batch 6 in epoch 0, gen_loss = 0.637942544051579, disc_loss = 0.4856704388345991
Trained batch 7 in epoch 0, gen_loss = 0.6385790705680847, disc_loss = 0.45176736637949944
Trained batch 8 in epoch 0, gen_loss = 0.6327315171559652, disc_loss = 0.4154624177349938
Trained batch 9 in epoch 0, gen_loss = 0.6289638757705689, disc_loss = 0.38671868443489077
Trained batch 10 in epoch 0, gen_loss = 0.6258014657280662, disc_loss = 0.3688425895842639
Trained batch 11 in epoch 0, gen_loss = 0.6207415163516998, disc_loss = 0.3534419847031434
Trained batch 12 in epoch 0, gen_loss = 0.6183540408427899, disc_loss = 0.3367801514955667
Trained batch 13 in epoch 0, gen_loss = 0.6217731961182186, disc_loss = 0.32134870120457243
Trained batch 14 in epoch 0, gen_loss = 0.6190321445465088, disc_loss = 0.30676756501197816
Trained batch 15 in epoch 0, gen_loss = 0.6186380945146084, disc_loss = 0.2950464282184839
Trained batch 16 in epoch 0, gen_loss = 0.6182071215966168, disc_loss = 0.28464366407955394
Trained batch 17 in epoch 0, gen_loss = 0.6173954374260373, disc_loss = 0.2757960640721851
Trained batch 18 in epoch 0, gen_loss = 0.6227519167096991, disc_loss = 0.26568098915250676
Trained batch 19 in epoch 0, gen_loss = 0.6243885725736618, disc_loss = 0.25634752325713633
Trained batch 20 in epoch 0, gen_loss = 0.6240585190909249, disc_loss = 0.24833216731037414
Trained batch 21 in epoch 0, gen_loss = 0.6265292330221697, disc_loss = 0.24061480604789473
Trained batch 22 in epoch 0, gen_loss = 0.6242299753686656, disc_loss = 0.23456372579802637
Trained batch 23 in epoch 0, gen_loss = 0.627785769601663, disc_loss = 0.22874945060660443
Trained batch 24 in epoch 0, gen_loss = 0.6294102025032043, disc_loss = 0.22419653773307802
Trained batch 25 in epoch 0, gen_loss = 0.6359128287205329, disc_loss = 0.21812928697237602
Trained batch 26 in epoch 0, gen_loss = 0.6414972565792225, disc_loss = 0.21247146002672337
Trained batch 27 in epoch 0, gen_loss = 0.6448324876172202, disc_loss = 0.20876757667532989
Trained batch 28 in epoch 0, gen_loss = 0.64954706923715, disc_loss = 0.20850388525888838
Trained batch 29 in epoch 0, gen_loss = 0.6521532595157623, disc_loss = 0.20741026774048804
Trained batch 30 in epoch 0, gen_loss = 0.6578430302681462, disc_loss = 0.20789322329144325
Trained batch 31 in epoch 0, gen_loss = 0.6589342188090086, disc_loss = 0.20453317766077816
Trained batch 32 in epoch 0, gen_loss = 0.6616435177398451, disc_loss = 0.20244501824631836
Trained batch 33 in epoch 0, gen_loss = 0.6630516367800096, disc_loss = 0.1985854574424379
Trained batch 34 in epoch 0, gen_loss = 0.6637720738138472, disc_loss = 0.19435200478349413
Trained batch 35 in epoch 0, gen_loss = 0.6631525970167584, disc_loss = 0.1904000337753031
Trained batch 36 in epoch 0, gen_loss = 0.6614425085686348, disc_loss = 0.18675817257246455
Trained batch 37 in epoch 0, gen_loss = 0.6609782309908616, disc_loss = 0.18332928948496519
Trained batch 38 in epoch 0, gen_loss = 0.6623077774659182, disc_loss = 0.18011208480367294
Trained batch 39 in epoch 0, gen_loss = 0.6616427883505821, disc_loss = 0.17721653087064623
Trained batch 40 in epoch 0, gen_loss = 0.6642463919593067, disc_loss = 0.17420551844122933
Trained batch 41 in epoch 0, gen_loss = 0.6655691833723159, disc_loss = 0.17116086247066656
Trained batch 42 in epoch 0, gen_loss = 0.6674524698146554, disc_loss = 0.16830414732874827
Trained batch 43 in epoch 0, gen_loss = 0.6686885668472811, disc_loss = 0.16592433938587253
Trained batch 44 in epoch 0, gen_loss = 0.6684575173589918, disc_loss = 0.16438503373000357
Trained batch 45 in epoch 0, gen_loss = 0.6735427444395812, disc_loss = 0.16439083291460638
Trained batch 46 in epoch 0, gen_loss = 0.673024225742259, disc_loss = 0.16491962311432717
Trained batch 47 in epoch 0, gen_loss = 0.6757001876831055, disc_loss = 0.16262322175316513
Trained batch 48 in epoch 0, gen_loss = 0.6780078739536052, disc_loss = 0.16074975389911203
Trained batch 49 in epoch 0, gen_loss = 0.6806958103179932, disc_loss = 0.15908794321119785
Trained batch 50 in epoch 0, gen_loss = 0.6811493436495463, disc_loss = 0.15740453561439233
Trained batch 51 in epoch 0, gen_loss = 0.6824188106335126, disc_loss = 0.15526148247031066
Trained batch 52 in epoch 0, gen_loss = 0.684546300825083, disc_loss = 0.15295735205400665
Trained batch 53 in epoch 0, gen_loss = 0.683955215745502, disc_loss = 0.15085015025127818
Trained batch 54 in epoch 0, gen_loss = 0.6860469568859447, disc_loss = 0.14877392609011042
Trained batch 55 in epoch 0, gen_loss = 0.6865124574729374, disc_loss = 0.14679742085614375
Trained batch 56 in epoch 0, gen_loss = 0.686982230136269, disc_loss = 0.14481338322685475
Trained batch 57 in epoch 0, gen_loss = 0.6873511384273397, disc_loss = 0.14304965098613295
Trained batch 58 in epoch 0, gen_loss = 0.6888209460145336, disc_loss = 0.14140726329158929
Trained batch 59 in epoch 0, gen_loss = 0.690439760684967, disc_loss = 0.13991516685734193
Trained batch 60 in epoch 0, gen_loss = 0.6900954705769898, disc_loss = 0.13855442093288312
Trained batch 61 in epoch 0, gen_loss = 0.6920667963643228, disc_loss = 0.13694820727311796
Trained batch 62 in epoch 0, gen_loss = 0.6929775249390375, disc_loss = 0.13526047316808548
Trained batch 63 in epoch 0, gen_loss = 0.6935367342084646, disc_loss = 0.13372413284378126
Trained batch 64 in epoch 0, gen_loss = 0.6935178509125343, disc_loss = 0.1324301197551764
Trained batch 65 in epoch 0, gen_loss = 0.6937747389981241, disc_loss = 0.13112794235348701
Trained batch 66 in epoch 0, gen_loss = 0.6957547949321234, disc_loss = 0.12980109253036443
Trained batch 67 in epoch 0, gen_loss = 0.6976842669879689, disc_loss = 0.1284677774152335
Trained batch 68 in epoch 0, gen_loss = 0.6989278845165087, disc_loss = 0.1271708066156809
Trained batch 69 in epoch 0, gen_loss = 0.700411913224629, disc_loss = 0.12607789481324808
Trained batch 70 in epoch 0, gen_loss = 0.7009421175634357, disc_loss = 0.1251186263078535
Trained batch 71 in epoch 0, gen_loss = 0.7041457568605741, disc_loss = 0.12570356148191625
Trained batch 72 in epoch 0, gen_loss = 0.703748639315775, disc_loss = 0.130911690578477
Trained batch 73 in epoch 0, gen_loss = 0.7042709919246467, disc_loss = 0.13032740419982253
Trained batch 74 in epoch 0, gen_loss = 0.7061038454373677, disc_loss = 0.13070586626728375
Trained batch 75 in epoch 0, gen_loss = 0.7066968232393265, disc_loss = 0.13018179572138347
Trained batch 76 in epoch 0, gen_loss = 0.705763227753825, disc_loss = 0.130176010311811
Trained batch 77 in epoch 0, gen_loss = 0.7076825744066483, disc_loss = 0.12946705448512846
Trained batch 78 in epoch 0, gen_loss = 0.7071785511849802, disc_loss = 0.12879080113164987
Trained batch 79 in epoch 0, gen_loss = 0.7077456966042519, disc_loss = 0.1278527919203043
Trained batch 80 in epoch 0, gen_loss = 0.7078977830616044, disc_loss = 0.1267654394937886
Trained batch 81 in epoch 0, gen_loss = 0.7077908741264809, disc_loss = 0.12578427182828508
Trained batch 82 in epoch 0, gen_loss = 0.707945675735014, disc_loss = 0.12499612444136517
Trained batch 83 in epoch 0, gen_loss = 0.7077859036979222, disc_loss = 0.12421449091995046
Trained batch 84 in epoch 0, gen_loss = 0.7096373284564299, disc_loss = 0.12412337928133853
Trained batch 85 in epoch 0, gen_loss = 0.7089797459369482, disc_loss = 0.12531119034907154
Trained batch 86 in epoch 0, gen_loss = 0.7099344983868215, disc_loss = 0.12623578828127904
Trained batch 87 in epoch 0, gen_loss = 0.7086475179954008, disc_loss = 0.1263799317265776
Trained batch 88 in epoch 0, gen_loss = 0.708813561482376, disc_loss = 0.12550193895952086
Trained batch 89 in epoch 0, gen_loss = 0.7085077749358283, disc_loss = 0.12506907801661227
Trained batch 90 in epoch 0, gen_loss = 0.7088079000567342, disc_loss = 0.1245858598459553
Trained batch 91 in epoch 0, gen_loss = 0.7073460985784945, disc_loss = 0.1251221060671884
Trained batch 92 in epoch 0, gen_loss = 0.7094220839520936, disc_loss = 0.12577003621125735
Trained batch 93 in epoch 0, gen_loss = 0.7087965873961753, disc_loss = 0.12564835653818668
Trained batch 94 in epoch 0, gen_loss = 0.7074956668050666, disc_loss = 0.12557860326610112
Trained batch 95 in epoch 0, gen_loss = 0.7082064505666494, disc_loss = 0.12525577722893408
Trained batch 96 in epoch 0, gen_loss = 0.7073486742285109, disc_loss = 0.12497177240006703
Trained batch 97 in epoch 0, gen_loss = 0.7078617336798687, disc_loss = 0.12452876001444398
Trained batch 98 in epoch 0, gen_loss = 0.7068529201276375, disc_loss = 0.12445988479738283
Trained batch 99 in epoch 0, gen_loss = 0.7077689528465271, disc_loss = 0.12478320728987455
Trained batch 100 in epoch 0, gen_loss = 0.706934126886991, disc_loss = 0.12444700769121105
Trained batch 101 in epoch 0, gen_loss = 0.7074876795796787, disc_loss = 0.12402479135084386
Trained batch 102 in epoch 0, gen_loss = 0.7067741925276599, disc_loss = 0.12377732677511799
Trained batch 103 in epoch 0, gen_loss = 0.7072892005626972, disc_loss = 0.12316403405454296
Trained batch 104 in epoch 0, gen_loss = 0.7074307140849886, disc_loss = 0.1224063315916629
Trained batch 105 in epoch 0, gen_loss = 0.7068318252293568, disc_loss = 0.12192192297639712
Trained batch 106 in epoch 0, gen_loss = 0.7067985083455237, disc_loss = 0.12115887677836641
Trained batch 107 in epoch 0, gen_loss = 0.7076623047943469, disc_loss = 0.12070461873103071
Trained batch 108 in epoch 0, gen_loss = 0.7081143380305089, disc_loss = 0.12001983226712691
Trained batch 109 in epoch 0, gen_loss = 0.70787144520066, disc_loss = 0.11951598863710057
Trained batch 110 in epoch 0, gen_loss = 0.7068261718964791, disc_loss = 0.11943924930450078
Trained batch 111 in epoch 0, gen_loss = 0.7089102906840188, disc_loss = 0.120680244839085
Trained batch 112 in epoch 0, gen_loss = 0.7077015221646402, disc_loss = 0.12505884448775148
Trained batch 113 in epoch 0, gen_loss = 0.7081785411165472, disc_loss = 0.12457559140104997
Trained batch 114 in epoch 0, gen_loss = 0.7074197826178178, disc_loss = 0.1251329111016315
Trained batch 115 in epoch 0, gen_loss = 0.7078921573943106, disc_loss = 0.12567572359894885
Trained batch 116 in epoch 0, gen_loss = 0.707407884618156, disc_loss = 0.1262941040799149
Trained batch 117 in epoch 0, gen_loss = 0.7063951386233508, disc_loss = 0.12685246525679605
Trained batch 118 in epoch 0, gen_loss = 0.7053780460558018, disc_loss = 0.12722776003745423
Trained batch 119 in epoch 0, gen_loss = 0.704956645766894, disc_loss = 0.12748706117272376
Trained batch 120 in epoch 0, gen_loss = 0.7055429210347578, disc_loss = 0.12709548151936412
Trained batch 121 in epoch 0, gen_loss = 0.7049194305646614, disc_loss = 0.127374957269821
Trained batch 122 in epoch 0, gen_loss = 0.7049644167830305, disc_loss = 0.12720653554046057
Trained batch 123 in epoch 0, gen_loss = 0.705230511003925, disc_loss = 0.1274094733619882
Trained batch 124 in epoch 0, gen_loss = 0.7043551459312439, disc_loss = 0.1280293008685112
Trained batch 125 in epoch 0, gen_loss = 0.7053228408571274, disc_loss = 0.12924621851434784
Trained batch 126 in epoch 0, gen_loss = 0.7042141228210269, disc_loss = 0.129976138883219
Trained batch 127 in epoch 0, gen_loss = 0.7036419440992177, disc_loss = 0.13013567571761087
Trained batch 128 in epoch 0, gen_loss = 0.7039300032364305, disc_loss = 0.13046446055635924
Trained batch 129 in epoch 0, gen_loss = 0.7029651380502261, disc_loss = 0.13072099863336636
Trained batch 130 in epoch 0, gen_loss = 0.7021092972682632, disc_loss = 0.13085077003440784
Trained batch 131 in epoch 0, gen_loss = 0.7017100891380599, disc_loss = 0.13099882068733373
Trained batch 132 in epoch 0, gen_loss = 0.7012496173829961, disc_loss = 0.1310097557821668
Trained batch 133 in epoch 0, gen_loss = 0.7005753036755235, disc_loss = 0.1307414627230879
Trained batch 134 in epoch 0, gen_loss = 0.7006858741795575, disc_loss = 0.1304322753239561
Trained batch 135 in epoch 0, gen_loss = 0.6995385403142256, disc_loss = 0.13244978541179614
Trained batch 136 in epoch 0, gen_loss = 0.7003772054275457, disc_loss = 0.13317203407522535
Trained batch 137 in epoch 0, gen_loss = 0.6999121830947157, disc_loss = 0.13301933384028033
Trained batch 138 in epoch 0, gen_loss = 0.6983539828722426, disc_loss = 0.13366542220544472
Trained batch 139 in epoch 0, gen_loss = 0.6984498481665339, disc_loss = 0.13422431488122258
Trained batch 140 in epoch 0, gen_loss = 0.6975751195816283, disc_loss = 0.1342863770241433
Trained batch 141 in epoch 0, gen_loss = 0.6965457207300294, disc_loss = 0.13453319165068614
Trained batch 142 in epoch 0, gen_loss = 0.6966122757721614, disc_loss = 0.13472741529658125
Trained batch 143 in epoch 0, gen_loss = 0.695695013842649, disc_loss = 0.13462913310569194
Trained batch 144 in epoch 0, gen_loss = 0.6949979368982644, disc_loss = 0.13435562694894856
Trained batch 145 in epoch 0, gen_loss = 0.6948887140375294, disc_loss = 0.13409111847820349
Trained batch 146 in epoch 0, gen_loss = 0.6948050323392259, disc_loss = 0.13349761337447327
Trained batch 147 in epoch 0, gen_loss = 0.6954472079067617, disc_loss = 0.13285526479720264
Trained batch 148 in epoch 0, gen_loss = 0.6950553585618935, disc_loss = 0.1329799816262402
Trained batch 149 in epoch 0, gen_loss = 0.6973399911324183, disc_loss = 0.13534724531074366
Trained batch 150 in epoch 0, gen_loss = 0.6963597107802005, disc_loss = 0.1369679623240272
Trained batch 151 in epoch 0, gen_loss = 0.6960919109222136, disc_loss = 0.13736186879932097
Testing Epoch 0
Training Epoch 1
Traceback (most recent call last):
  File "srgan_bones.py", line 285, in <module>
    gen_hr = generator(imgs_lr)
  File "/work3/soeba/HALOS/halos/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "srgan_bones.py", line 198, in forward
    out = self.res_blocks(out1)
  File "/work3/soeba/HALOS/halos/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/work3/soeba/HALOS/halos/lib/python3.8/site-packages/torch/nn/modules/container.py", line 139, in forward
    input = module(input)
  File "/work3/soeba/HALOS/halos/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "srgan_bones.py", line 175, in forward
    return x + self.conv_block(x)
  File "/work3/soeba/HALOS/halos/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/work3/soeba/HALOS/halos/lib/python3.8/site-packages/torch/nn/modules/container.py", line 139, in forward
    input = module(input)
  File "/work3/soeba/HALOS/halos/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/work3/soeba/HALOS/halos/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py", line 168, in forward
    return F.batch_norm(
  File "/work3/soeba/HALOS/halos/lib/python3.8/site-packages/torch/nn/functional.py", line 2438, in batch_norm
    return torch.batch_norm(
RuntimeError: CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 31.75 GiB total capacity; 29.63 GiB already allocated; 15.69 MiB free; 30.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF