wandb: WARNING Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.
/work3/soeba/HALOS/halos/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.
  warnings.warn(
/work3/soeba/HALOS/halos/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Training Epoch 0
Trained batch 0 in epoch 0, gen_loss = 0.48653724789619446, disc_loss = 0.5984059572219849
Trained batch 1 in epoch 0, gen_loss = 0.45271605253219604, disc_loss = 0.6965020298957825
Trained batch 2 in epoch 0, gen_loss = 0.4688761035601298, disc_loss = 0.7720617055892944
Trained batch 3 in epoch 0, gen_loss = 0.4665049761533737, disc_loss = 0.6595145165920258
Trained batch 4 in epoch 0, gen_loss = 0.44816088676452637, disc_loss = 0.589404284954071
Trained batch 5 in epoch 0, gen_loss = 0.44166435301303864, disc_loss = 0.5305652767419815
Trained batch 6 in epoch 0, gen_loss = 0.44489953773362295, disc_loss = 0.4843304029532841
Trained batch 7 in epoch 0, gen_loss = 0.44537585973739624, disc_loss = 0.4398550596088171
Trained batch 8 in epoch 0, gen_loss = 0.43649650944603813, disc_loss = 0.40518346263302696
Trained batch 9 in epoch 0, gen_loss = 0.4279459983110428, disc_loss = 0.37549212127923964
Trained batch 10 in epoch 0, gen_loss = 0.43393574248660693, disc_loss = 0.3526534160429781
Trained batch 11 in epoch 0, gen_loss = 0.43904336790243786, disc_loss = 0.3336301352828741
Trained batch 12 in epoch 0, gen_loss = 0.43882527259679943, disc_loss = 0.3160629232342427
Trained batch 13 in epoch 0, gen_loss = 0.4369438333170755, disc_loss = 0.3009184082703931
Trained batch 14 in epoch 0, gen_loss = 0.4379836916923523, disc_loss = 0.2874381457765897
Trained batch 15 in epoch 0, gen_loss = 0.43785993196070194, disc_loss = 0.27397731179371476
Trained batch 16 in epoch 0, gen_loss = 0.4382772305432488, disc_loss = 0.26207512704765096
Trained batch 17 in epoch 0, gen_loss = 0.4410170548492008, disc_loss = 0.25163081246945596
Trained batch 18 in epoch 0, gen_loss = 0.43882864399960164, disc_loss = 0.244550174396289
Trained batch 19 in epoch 0, gen_loss = 0.4431565642356873, disc_loss = 0.2394711297005415
Trained batch 20 in epoch 0, gen_loss = 0.4399925399394262, disc_loss = 0.2330549479950042
Trained batch 21 in epoch 0, gen_loss = 0.4411460594697432, disc_loss = 0.2308419021693143
Trained batch 22 in epoch 0, gen_loss = 0.440435695907344, disc_loss = 0.22577248675667722
Trained batch 23 in epoch 0, gen_loss = 0.4423231730858485, disc_loss = 0.22210895114888748
Trained batch 24 in epoch 0, gen_loss = 0.44272890329360964, disc_loss = 0.21827061742544174
Trained batch 25 in epoch 0, gen_loss = 0.44345846199072325, disc_loss = 0.2134814583338224
Trained batch 26 in epoch 0, gen_loss = 0.4464174784995891, disc_loss = 0.20841151586285345
Trained batch 27 in epoch 0, gen_loss = 0.44817241707018446, disc_loss = 0.20377895262624537
Trained batch 28 in epoch 0, gen_loss = 0.4476090607971981, disc_loss = 0.1988928517134025
Trained batch 29 in epoch 0, gen_loss = 0.4481745024522146, disc_loss = 0.1948020735134681
Trained batch 30 in epoch 0, gen_loss = 0.45236109533617574, disc_loss = 0.1908638702044564
Trained batch 31 in epoch 0, gen_loss = 0.4518588623031974, disc_loss = 0.18674709973856807
Trained batch 32 in epoch 0, gen_loss = 0.4523719504024043, disc_loss = 0.1828209746516112
Trained batch 33 in epoch 0, gen_loss = 0.4520111504723044, disc_loss = 0.1789560601772631
Trained batch 34 in epoch 0, gen_loss = 0.45212645956448144, disc_loss = 0.17584891798240798
Trained batch 35 in epoch 0, gen_loss = 0.4543977255622546, disc_loss = 0.17256455682218075
Trained batch 36 in epoch 0, gen_loss = 0.45454410443434845, disc_loss = 0.16901153406581362
Trained batch 37 in epoch 0, gen_loss = 0.4541698953038768, disc_loss = 0.16626992684445882
Trained batch 38 in epoch 0, gen_loss = 0.4543703970236656, disc_loss = 0.16348988429093972
Trained batch 39 in epoch 0, gen_loss = 0.45187519267201426, disc_loss = 0.16347954347729682
Trained batch 40 in epoch 0, gen_loss = 0.4512323940672526, disc_loss = 0.16667197390300473
Trained batch 41 in epoch 0, gen_loss = 0.45375964045524597, disc_loss = 0.164623937081723
Trained batch 42 in epoch 0, gen_loss = 0.45561339550240093, disc_loss = 0.16274272459884023
Trained batch 43 in epoch 0, gen_loss = 0.45660569789734756, disc_loss = 0.16018135854127732
Trained batch 44 in epoch 0, gen_loss = 0.45786609848340354, disc_loss = 0.15743100634879537
Trained batch 45 in epoch 0, gen_loss = 0.4601803583943326, disc_loss = 0.15499351643349812
Trained batch 46 in epoch 0, gen_loss = 0.458695076881571, disc_loss = 0.15258690873359113
Trained batch 47 in epoch 0, gen_loss = 0.45977141956488293, disc_loss = 0.15048608424452445
Trained batch 48 in epoch 0, gen_loss = 0.4617372361981139, disc_loss = 0.1485301802809141
Trained batch 49 in epoch 0, gen_loss = 0.46221854746341706, disc_loss = 0.14676948435604573
Trained batch 50 in epoch 0, gen_loss = 0.46344012898557324, disc_loss = 0.1449943071632993
Trained batch 51 in epoch 0, gen_loss = 0.46618769203241056, disc_loss = 0.14290657789947894
Trained batch 52 in epoch 0, gen_loss = 0.4675627750045848, disc_loss = 0.14079147946300372
Trained batch 53 in epoch 0, gen_loss = 0.4687497566143672, disc_loss = 0.13882693858748232
Trained batch 54 in epoch 0, gen_loss = 0.46860900629650465, disc_loss = 0.13682341538369655
Trained batch 55 in epoch 0, gen_loss = 0.46803296942796024, disc_loss = 0.13493242241176112
Trained batch 56 in epoch 0, gen_loss = 0.4691290588755357, disc_loss = 0.13303682518502077
Trained batch 57 in epoch 0, gen_loss = 0.4697420170594906, disc_loss = 0.1312533876826537
Trained batch 58 in epoch 0, gen_loss = 0.4698157573150376, disc_loss = 0.12971828502239818
Trained batch 59 in epoch 0, gen_loss = 0.4704110711812973, disc_loss = 0.12816587292278805
Trained batch 60 in epoch 0, gen_loss = 0.4705707181672581, disc_loss = 0.12646129235747408
Trained batch 61 in epoch 0, gen_loss = 0.4697538263374759, disc_loss = 0.1250394809570524
Trained batch 62 in epoch 0, gen_loss = 0.4700727415463281, disc_loss = 0.12351603028438395
Trained batch 63 in epoch 0, gen_loss = 0.4700244776904583, disc_loss = 0.1219991130346898
Trained batch 64 in epoch 0, gen_loss = 0.47014127648793735, disc_loss = 0.12057035040970032
Trained batch 65 in epoch 0, gen_loss = 0.47002559268113336, disc_loss = 0.11909693550092704
Trained batch 66 in epoch 0, gen_loss = 0.46920137796829, disc_loss = 0.11784259833173076
Trained batch 67 in epoch 0, gen_loss = 0.4693575036876342, disc_loss = 0.11650469151380308
Trained batch 68 in epoch 0, gen_loss = 0.4688626484594483, disc_loss = 0.11514192390377107
Trained batch 69 in epoch 0, gen_loss = 0.4683741731303079, disc_loss = 0.11381645585809436
Trained batch 70 in epoch 0, gen_loss = 0.46763901769275396, disc_loss = 0.11257757047112558
Trained batch 71 in epoch 0, gen_loss = 0.4680799175467756, disc_loss = 0.11133697352165149
Trained batch 72 in epoch 0, gen_loss = 0.4690206308070927, disc_loss = 0.11014248831325198
Trained batch 73 in epoch 0, gen_loss = 0.4692787519983343, disc_loss = 0.10899082319559278
Trained batch 74 in epoch 0, gen_loss = 0.46884646932284035, disc_loss = 0.10789885722100735
Trained batch 75 in epoch 0, gen_loss = 0.46802556397099243, disc_loss = 0.10682491445913911
Trained batch 76 in epoch 0, gen_loss = 0.46702764521945606, disc_loss = 0.10602906198760906
Trained batch 77 in epoch 0, gen_loss = 0.4667975921661426, disc_loss = 0.10565433524644528
Trained batch 78 in epoch 0, gen_loss = 0.46798759736592255, disc_loss = 0.1055603738214019
Trained batch 79 in epoch 0, gen_loss = 0.4686008390039206, disc_loss = 0.10498781267087906
Trained batch 80 in epoch 0, gen_loss = 0.4688243060200303, disc_loss = 0.10413368071578903
Trained batch 81 in epoch 0, gen_loss = 0.4683692876885577, disc_loss = 0.10338987598604546
Trained batch 82 in epoch 0, gen_loss = 0.4686404376862997, disc_loss = 0.1026112405605704
Trained batch 83 in epoch 0, gen_loss = 0.4696040909205164, disc_loss = 0.10194544908812359
Trained batch 84 in epoch 0, gen_loss = 0.4695411468253416, disc_loss = 0.10211412632728324
Trained batch 85 in epoch 0, gen_loss = 0.4680481303569882, disc_loss = 0.10435802270766607
Trained batch 86 in epoch 0, gen_loss = 0.4681543194014451, disc_loss = 0.1060493989879715
Trained batch 87 in epoch 0, gen_loss = 0.46799184212630446, disc_loss = 0.1062357901299203
Trained batch 88 in epoch 0, gen_loss = 0.46779382396280095, disc_loss = 0.10556003625054708
Trained batch 89 in epoch 0, gen_loss = 0.4685447053776847, disc_loss = 0.10475113046252066
Trained batch 90 in epoch 0, gen_loss = 0.468642430973577, disc_loss = 0.1040088274608274
Trained batch 91 in epoch 0, gen_loss = 0.46812861671914224, disc_loss = 0.10330943546622344
Trained batch 92 in epoch 0, gen_loss = 0.46832617700740853, disc_loss = 0.10284669486986052
Trained batch 93 in epoch 0, gen_loss = 0.468307744949422, disc_loss = 0.10302693291785235
Trained batch 94 in epoch 0, gen_loss = 0.4680030345916748, disc_loss = 0.1058555911050031
Trained batch 95 in epoch 0, gen_loss = 0.4681135819603999, disc_loss = 0.10651619753722723
Trained batch 96 in epoch 0, gen_loss = 0.46754806742225724, disc_loss = 0.10634517130086717
Trained batch 97 in epoch 0, gen_loss = 0.46704504197957564, disc_loss = 0.10633788651273567
Trained batch 98 in epoch 0, gen_loss = 0.4671881439709904, disc_loss = 0.10602266618022413
Trained batch 99 in epoch 0, gen_loss = 0.46722635060548784, disc_loss = 0.10580752683803439
Trained batch 100 in epoch 0, gen_loss = 0.4673286018395188, disc_loss = 0.10747827853232918
Trained batch 101 in epoch 0, gen_loss = 0.4675125596570034, disc_loss = 0.10991377206336633
Trained batch 102 in epoch 0, gen_loss = 0.467150979829066, disc_loss = 0.11183377765842432
Trained batch 103 in epoch 0, gen_loss = 0.4673502534054793, disc_loss = 0.11280381778040184
Trained batch 104 in epoch 0, gen_loss = 0.4674409591016315, disc_loss = 0.11345110846062502
Trained batch 105 in epoch 0, gen_loss = 0.466964767788941, disc_loss = 0.11369468761994592
Trained batch 106 in epoch 0, gen_loss = 0.46633457636164727, disc_loss = 0.11392770413413783
Trained batch 107 in epoch 0, gen_loss = 0.4655370094158031, disc_loss = 0.11592176168536146
Trained batch 108 in epoch 0, gen_loss = 0.4648581497166135, disc_loss = 0.11702182504493709
Trained batch 109 in epoch 0, gen_loss = 0.4637302320111882, disc_loss = 0.11773569473827428
Trained batch 110 in epoch 0, gen_loss = 0.4640575419138144, disc_loss = 0.11770005696998523
Trained batch 111 in epoch 0, gen_loss = 0.4638502470084599, disc_loss = 0.11811564568363662
Trained batch 112 in epoch 0, gen_loss = 0.46380541968134653, disc_loss = 0.11927814661335628
Trained batch 113 in epoch 0, gen_loss = 0.46354426705000695, disc_loss = 0.12029619108101255
Trained batch 114 in epoch 0, gen_loss = 0.46360193050425985, disc_loss = 0.1209688787712999
Trained batch 115 in epoch 0, gen_loss = 0.4634550630018629, disc_loss = 0.12154217422458119
Trained batch 116 in epoch 0, gen_loss = 0.46270827375925505, disc_loss = 0.12213438951497914
Trained batch 117 in epoch 0, gen_loss = 0.4620415873951831, disc_loss = 0.12236708552592386
Trained batch 118 in epoch 0, gen_loss = 0.46240211209329235, disc_loss = 0.12270596571767531
Trained batch 119 in epoch 0, gen_loss = 0.46292462522784866, disc_loss = 0.12313585441249113
Trained batch 120 in epoch 0, gen_loss = 0.46255665943642293, disc_loss = 0.1259525214929965
Trained batch 121 in epoch 0, gen_loss = 0.46268388555675255, disc_loss = 0.12639593929968407
Trained batch 122 in epoch 0, gen_loss = 0.4629099780951089, disc_loss = 0.1266219511597864
Trained batch 123 in epoch 0, gen_loss = 0.4635527090680215, disc_loss = 0.1266632841809863
Trained batch 124 in epoch 0, gen_loss = 0.463280684709549, disc_loss = 0.12656263409554958
Trained batch 125 in epoch 0, gen_loss = 0.4627239190869861, disc_loss = 0.12728008519976386
Trained batch 126 in epoch 0, gen_loss = 0.4618266389125914, disc_loss = 0.13059143040065221
Trained batch 127 in epoch 0, gen_loss = 0.4610838871449232, disc_loss = 0.1309330806689104
Trained batch 128 in epoch 0, gen_loss = 0.4608859301999558, disc_loss = 0.13119779236961243
Trained batch 129 in epoch 0, gen_loss = 0.4609704478428914, disc_loss = 0.13152666950168518
Trained batch 130 in epoch 0, gen_loss = 0.4602599903827405, disc_loss = 0.13205214594099812
Trained batch 131 in epoch 0, gen_loss = 0.46025643023577606, disc_loss = 0.1325078824337459
Trained batch 132 in epoch 0, gen_loss = 0.4603872003411888, disc_loss = 0.13306282414473536
Trained batch 133 in epoch 0, gen_loss = 0.4595269169380416, disc_loss = 0.1333858361745726
Trained batch 134 in epoch 0, gen_loss = 0.4591423816151089, disc_loss = 0.13349227760401036
Trained batch 135 in epoch 0, gen_loss = 0.45970535103012533, disc_loss = 0.13359334417070975
Trained batch 136 in epoch 0, gen_loss = 0.4593016953363906, disc_loss = 0.13330605254936828
Trained batch 137 in epoch 0, gen_loss = 0.4590576971354692, disc_loss = 0.13330002743210914
Trained batch 138 in epoch 0, gen_loss = 0.4593260832827726, disc_loss = 0.13341177301816373
Trained batch 139 in epoch 0, gen_loss = 0.45955897867679596, disc_loss = 0.13623637108664427
Trained batch 140 in epoch 0, gen_loss = 0.45965507140396333, disc_loss = 0.13653318191620897
Trained batch 141 in epoch 0, gen_loss = 0.459502067574313, disc_loss = 0.13658056291423634
Trained batch 142 in epoch 0, gen_loss = 0.459464488746403, disc_loss = 0.1364580011320906
Trained batch 143 in epoch 0, gen_loss = 0.45949381672673756, disc_loss = 0.1362735209857217
Trained batch 144 in epoch 0, gen_loss = 0.4593640056149713, disc_loss = 0.1360203956607087
Trained batch 145 in epoch 0, gen_loss = 0.4589235586662815, disc_loss = 0.1356995120925242
Trained batch 146 in epoch 0, gen_loss = 0.4591824850257562, disc_loss = 0.1356617746380519
Trained batch 147 in epoch 0, gen_loss = 0.4589774401204006, disc_loss = 0.13726739412979097
Trained batch 148 in epoch 0, gen_loss = 0.4588967171691408, disc_loss = 0.1379676014059942
Trained batch 149 in epoch 0, gen_loss = 0.4582020290692647, disc_loss = 0.13815514642745255
Trained batch 150 in epoch 0, gen_loss = 0.45748721290108385, disc_loss = 0.13827707928093458
Trained batch 151 in epoch 0, gen_loss = 0.457318532035539, disc_loss = 0.13820827094298838
Trained batch 152 in epoch 0, gen_loss = 0.45698657963011, disc_loss = 0.13804447912653678
Trained batch 153 in epoch 0, gen_loss = 0.4560931615241162, disc_loss = 0.13797352306980204
Trained batch 154 in epoch 0, gen_loss = 0.45645519764192644, disc_loss = 0.1376485956171828
Trained batch 155 in epoch 0, gen_loss = 0.45622773449390364, disc_loss = 0.1382650683204142
Trained batch 156 in epoch 0, gen_loss = 0.45551147639371786, disc_loss = 0.1389451507051849
Trained batch 157 in epoch 0, gen_loss = 0.45553997328764273, disc_loss = 0.13873435484976332
Trained batch 158 in epoch 0, gen_loss = 0.4560646967692945, disc_loss = 0.13841089844094506
Trained batch 159 in epoch 0, gen_loss = 0.45603413358330724, disc_loss = 0.13870343662565573
Trained batch 160 in epoch 0, gen_loss = 0.45624846664274704, disc_loss = 0.14018828735357117
Trained batch 161 in epoch 0, gen_loss = 0.4559677160448498, disc_loss = 0.141063597501704
Trained batch 162 in epoch 0, gen_loss = 0.4558180563288964, disc_loss = 0.1410098743562135
Trained batch 163 in epoch 0, gen_loss = 0.45565693225802445, disc_loss = 0.14086290320563244
Trained batch 164 in epoch 0, gen_loss = 0.45482668298663514, disc_loss = 0.14109689294163025
Trained batch 165 in epoch 0, gen_loss = 0.4541098326444626, disc_loss = 0.14121160340910577
Trained batch 166 in epoch 0, gen_loss = 0.4538509012339358, disc_loss = 0.14105315041354674
Trained batch 167 in epoch 0, gen_loss = 0.4543875883377734, disc_loss = 0.140735958558729
Trained batch 168 in epoch 0, gen_loss = 0.453924509195181, disc_loss = 0.1408447773917716
Trained batch 169 in epoch 0, gen_loss = 0.4540497569476857, disc_loss = 0.1417450830230818
Trained batch 170 in epoch 0, gen_loss = 0.4537387044109099, disc_loss = 0.14216354305240494
Trained batch 171 in epoch 0, gen_loss = 0.45324742343536645, disc_loss = 0.1424477298657388
Trained batch 172 in epoch 0, gen_loss = 0.4529647208707181, disc_loss = 0.14231956210741073
Trained batch 173 in epoch 0, gen_loss = 0.4527904987335205, disc_loss = 0.14234593184129601
Trained batch 174 in epoch 0, gen_loss = 0.45256859387670245, disc_loss = 0.14230074081037725
Trained batch 175 in epoch 0, gen_loss = 0.4525034950876778, disc_loss = 0.14214676291554829
Trained batch 176 in epoch 0, gen_loss = 0.4522934493708745, disc_loss = 0.14201901338489378
Trained batch 177 in epoch 0, gen_loss = 0.4520176670859369, disc_loss = 0.14196945700710745
Trained batch 178 in epoch 0, gen_loss = 0.4517940334434616, disc_loss = 0.14207110082595376
Trained batch 179 in epoch 0, gen_loss = 0.4514201515250736, disc_loss = 0.14275588085874916
Trained batch 180 in epoch 0, gen_loss = 0.4516058541134576, disc_loss = 0.14266331344637093
Trained batch 181 in epoch 0, gen_loss = 0.4517369294887061, disc_loss = 0.1425330643286253
Trained batch 182 in epoch 0, gen_loss = 0.45129062634348216, disc_loss = 0.14269454210304497
Trained batch 183 in epoch 0, gen_loss = 0.45171872461619583, disc_loss = 0.14271429200332775
Trained batch 184 in epoch 0, gen_loss = 0.4516774230712169, disc_loss = 0.1428229913014818
Trained batch 185 in epoch 0, gen_loss = 0.4514634676517979, disc_loss = 0.14504978392193074
Trained batch 186 in epoch 0, gen_loss = 0.45235367956008504, disc_loss = 0.1448569298886201
Trained batch 187 in epoch 0, gen_loss = 0.45220632787714615, disc_loss = 0.14534787765327603
Trained batch 188 in epoch 0, gen_loss = 0.4519064959394869, disc_loss = 0.1470733426373314
Trained batch 189 in epoch 0, gen_loss = 0.45153676977283075, disc_loss = 0.14783567530348113
Trained batch 190 in epoch 0, gen_loss = 0.4513299073536359, disc_loss = 0.14821926909084407
Trained batch 191 in epoch 0, gen_loss = 0.4509160160087049, disc_loss = 0.14876225647943406
Trained batch 192 in epoch 0, gen_loss = 0.4504742313543132, disc_loss = 0.1492427889682314
Trained batch 193 in epoch 0, gen_loss = 0.44988930455802645, disc_loss = 0.14962427605173945
Trained batch 194 in epoch 0, gen_loss = 0.4496229537022419, disc_loss = 0.14980366828923042
Trained batch 195 in epoch 0, gen_loss = 0.4490429100637533, disc_loss = 0.15007947175288383
Trained batch 196 in epoch 0, gen_loss = 0.4490868114880499, disc_loss = 0.15048576553876933
Trained batch 197 in epoch 0, gen_loss = 0.4486890031833841, disc_loss = 0.15071243389199176
Trained batch 198 in epoch 0, gen_loss = 0.44849105081965573, disc_loss = 0.15104273585930242
Trained batch 199 in epoch 0, gen_loss = 0.44793172538280485, disc_loss = 0.15145357335917653
Trained batch 200 in epoch 0, gen_loss = 0.4477590433697202, disc_loss = 0.15175129097900283
Trained batch 201 in epoch 0, gen_loss = 0.44744643023108494, disc_loss = 0.151969363885277
Trained batch 202 in epoch 0, gen_loss = 0.44753530768338096, disc_loss = 0.1521272206669811
Trained batch 203 in epoch 0, gen_loss = 0.44734515877915365, disc_loss = 0.15251832640748106
Trained batch 204 in epoch 0, gen_loss = 0.4471911764726406, disc_loss = 0.1528980027884245
Trained batch 205 in epoch 0, gen_loss = 0.446761688416444, disc_loss = 0.15330382410292198
Trained batch 206 in epoch 0, gen_loss = 0.4462665385670132, disc_loss = 0.15361281101938318
Trained batch 207 in epoch 0, gen_loss = 0.4458209009697804, disc_loss = 0.1540483605528537
Trained batch 208 in epoch 0, gen_loss = 0.4454540018544813, disc_loss = 0.154462129189566
Trained batch 209 in epoch 0, gen_loss = 0.44523354044982366, disc_loss = 0.1553932166081809
Trained batch 210 in epoch 0, gen_loss = 0.4450134859548361, disc_loss = 0.15589909152220494
Trained batch 211 in epoch 0, gen_loss = 0.4449262794739795, disc_loss = 0.15606724684355114
Trained batch 212 in epoch 0, gen_loss = 0.44478062410869507, disc_loss = 0.15628893165346322
Trained batch 213 in epoch 0, gen_loss = 0.4443619962886115, disc_loss = 0.15663008255205144
Trained batch 214 in epoch 0, gen_loss = 0.44420936149220136, disc_loss = 0.15688343937833643
Trained batch 215 in epoch 0, gen_loss = 0.4439152396387524, disc_loss = 0.15708845663229348
Trained batch 216 in epoch 0, gen_loss = 0.4437068766712593, disc_loss = 0.15730912725050603
Trained batch 217 in epoch 0, gen_loss = 0.44340566607243426, disc_loss = 0.15771900181479137
Trained batch 218 in epoch 0, gen_loss = 0.4431570022617845, disc_loss = 0.15824494836639325
Trained batch 219 in epoch 0, gen_loss = 0.44301428496837614, disc_loss = 0.15845285270532425
Trained batch 220 in epoch 0, gen_loss = 0.4428081636515138, disc_loss = 0.1584883542200424
Trained batch 221 in epoch 0, gen_loss = 0.44288544504492133, disc_loss = 0.15826517631308185
Trained batch 222 in epoch 0, gen_loss = 0.4430713141712907, disc_loss = 0.15845369757257502
Trained batch 223 in epoch 0, gen_loss = 0.4432080937549472, disc_loss = 0.15878712797504185
Trained batch 224 in epoch 0, gen_loss = 0.4433305647638109, disc_loss = 0.1591678644468387
Trained batch 225 in epoch 0, gen_loss = 0.44303478893980514, disc_loss = 0.15920987770587733
Trained batch 226 in epoch 0, gen_loss = 0.44304265337893617, disc_loss = 0.15922248979726814
Trained batch 227 in epoch 0, gen_loss = 0.4430539866811351, disc_loss = 0.15935493468944179
Trained batch 228 in epoch 0, gen_loss = 0.4428114149247715, disc_loss = 0.15976666973895642
Trained batch 229 in epoch 0, gen_loss = 0.44257217062556226, disc_loss = 0.1598517496867672
Trained batch 230 in epoch 0, gen_loss = 0.4421976919556077, disc_loss = 0.16020704837298239
Trained batch 231 in epoch 0, gen_loss = 0.4420496680099389, disc_loss = 0.16111988393621968
Trained batch 232 in epoch 0, gen_loss = 0.44197606490405333, disc_loss = 0.1610916368743253
Trained batch 233 in epoch 0, gen_loss = 0.4415367929599224, disc_loss = 0.16187285299962148
Trained batch 234 in epoch 0, gen_loss = 0.4413951096382547, disc_loss = 0.1624020953682509
Trained batch 235 in epoch 0, gen_loss = 0.4408988450038231, disc_loss = 0.1627927497581784
Trained batch 236 in epoch 0, gen_loss = 0.4406491720475225, disc_loss = 0.16278540433427705
Trained batch 237 in epoch 0, gen_loss = 0.4403818441288812, disc_loss = 0.16307158554520676
Trained batch 238 in epoch 0, gen_loss = 0.4401399127858453, disc_loss = 0.1632074123817383
Trained batch 239 in epoch 0, gen_loss = 0.44051429939766723, disc_loss = 0.16328907533703993
Trained batch 240 in epoch 0, gen_loss = 0.440296253722733, disc_loss = 0.1633869091674871
Trained batch 241 in epoch 0, gen_loss = 0.44000422203343764, disc_loss = 0.16347923079767257
Trained batch 242 in epoch 0, gen_loss = 0.43936657157454473, disc_loss = 0.1636446683050551
Trained batch 243 in epoch 0, gen_loss = 0.4390251871015205, disc_loss = 0.16362746559907912
Trained batch 244 in epoch 0, gen_loss = 0.4389591590482361, disc_loss = 0.1636255329284741
Trained batch 245 in epoch 0, gen_loss = 0.43882255425782707, disc_loss = 0.1638308220081092
Trained batch 246 in epoch 0, gen_loss = 0.4386670447071554, disc_loss = 0.16389062088963233
Trained batch 247 in epoch 0, gen_loss = 0.4383821713347589, disc_loss = 0.1642609141812089
Trained batch 248 in epoch 0, gen_loss = 0.4382246064852519, disc_loss = 0.16477718691240592
Trained batch 249 in epoch 0, gen_loss = 0.4379351238012314, disc_loss = 0.16487905173748732
Trained batch 250 in epoch 0, gen_loss = 0.4377992479687193, disc_loss = 0.1648173246057741
Trained batch 251 in epoch 0, gen_loss = 0.43779134939587305, disc_loss = 0.16478813616263252
Trained batch 252 in epoch 0, gen_loss = 0.4374158872446053, disc_loss = 0.16528707989325872
Trained batch 253 in epoch 0, gen_loss = 0.4374061441562307, disc_loss = 0.16577229524896606
Trained batch 254 in epoch 0, gen_loss = 0.4371401014281254, disc_loss = 0.16572286497582409
Trained batch 255 in epoch 0, gen_loss = 0.4370764169143513, disc_loss = 0.1657750623926404
Trained batch 256 in epoch 0, gen_loss = 0.43672583086945205, disc_loss = 0.16600656236163142
Trained batch 257 in epoch 0, gen_loss = 0.4364138247892838, disc_loss = 0.1661813666813018
Trained batch 258 in epoch 0, gen_loss = 0.43631978442309904, disc_loss = 0.1663339012536188
Trained batch 259 in epoch 0, gen_loss = 0.4362589621773133, disc_loss = 0.16619291018264798
Trained batch 260 in epoch 0, gen_loss = 0.4360173150497378, disc_loss = 0.16622875498828987
Trained batch 261 in epoch 0, gen_loss = 0.43604487510582873, disc_loss = 0.1662358163237458
Trained batch 262 in epoch 0, gen_loss = 0.43615265762851263, disc_loss = 0.1662125467843316
Trained batch 263 in epoch 0, gen_loss = 0.4358364436211008, disc_loss = 0.16619801898769132
Trained batch 264 in epoch 0, gen_loss = 0.4356075763702393, disc_loss = 0.16652648476356605
Trained batch 265 in epoch 0, gen_loss = 0.43506245774434027, disc_loss = 0.1667176165681024
Trained batch 266 in epoch 0, gen_loss = 0.43521627471241614, disc_loss = 0.16655983392860782
Trained batch 267 in epoch 0, gen_loss = 0.4353425964268286, disc_loss = 0.16662448960870727
Trained batch 268 in epoch 0, gen_loss = 0.43514464501997796, disc_loss = 0.1666928191208817
Trained batch 269 in epoch 0, gen_loss = 0.4348863612722467, disc_loss = 0.16728137890911765
Trained batch 270 in epoch 0, gen_loss = 0.4350414584043721, disc_loss = 0.16711031952769773
Trained batch 271 in epoch 0, gen_loss = 0.435118212200263, disc_loss = 0.16694703281117493
Trained batch 272 in epoch 0, gen_loss = 0.4350206031030788, disc_loss = 0.16694576388068033
Trained batch 273 in epoch 0, gen_loss = 0.4350122447213987, disc_loss = 0.1669447983200424
Trained batch 274 in epoch 0, gen_loss = 0.4348862708698619, disc_loss = 0.1667636413127184
Trained batch 275 in epoch 0, gen_loss = 0.4346153158424557, disc_loss = 0.16657156728720968
Trained batch 276 in epoch 0, gen_loss = 0.43456981349938184, disc_loss = 0.16722631243995595
Trained batch 277 in epoch 0, gen_loss = 0.4340913207839719, disc_loss = 0.1678082869461865
Trained batch 278 in epoch 0, gen_loss = 0.43388429999778777, disc_loss = 0.16797010290126005
Trained batch 279 in epoch 0, gen_loss = 0.4338485992380551, disc_loss = 0.16864229871093162
Trained batch 280 in epoch 0, gen_loss = 0.4334667327140998, disc_loss = 0.16893895298534228
Trained batch 281 in epoch 0, gen_loss = 0.4333129569571069, disc_loss = 0.16907389693834046
Trained batch 282 in epoch 0, gen_loss = 0.43320676536947594, disc_loss = 0.1691020128939156
Trained batch 283 in epoch 0, gen_loss = 0.433045454218354, disc_loss = 0.16912311708397218
Trained batch 284 in epoch 0, gen_loss = 0.4330927336425112, disc_loss = 0.1690418000741486
Trained batch 285 in epoch 0, gen_loss = 0.43308203297478337, disc_loss = 0.16912248677906247
Trained batch 286 in epoch 0, gen_loss = 0.4328257999146026, disc_loss = 0.16908796572285453
Trained batch 287 in epoch 0, gen_loss = 0.43277715353502166, disc_loss = 0.16898407398386756
Trained batch 288 in epoch 0, gen_loss = 0.4325487348653866, disc_loss = 0.1690358204451296
Trained batch 289 in epoch 0, gen_loss = 0.43245768834804665, disc_loss = 0.16909653507815353
Trained batch 290 in epoch 0, gen_loss = 0.4322324549824102, disc_loss = 0.16910923694186808
Trained batch 291 in epoch 0, gen_loss = 0.4319729894807894, disc_loss = 0.16923775737594865
Trained batch 292 in epoch 0, gen_loss = 0.43194833865751586, disc_loss = 0.1691577600270713
Trained batch 293 in epoch 0, gen_loss = 0.43220141824005415, disc_loss = 0.1688219324755324
Trained batch 294 in epoch 0, gen_loss = 0.43219638386015163, disc_loss = 0.16895433646011151
Trained batch 295 in epoch 0, gen_loss = 0.43200292740319224, disc_loss = 0.1690962016267853
Trained batch 296 in epoch 0, gen_loss = 0.4320803717129961, disc_loss = 0.1689175645246951
Trained batch 297 in epoch 0, gen_loss = 0.43191995706734243, disc_loss = 0.16900469702137197
Trained batch 298 in epoch 0, gen_loss = 0.43140439395123104, disc_loss = 0.16917892432706214
Trained batch 299 in epoch 0, gen_loss = 0.431328927675883, disc_loss = 0.16901279642557104
Trained batch 300 in epoch 0, gen_loss = 0.43132487018639065, disc_loss = 0.16921895573478798
Trained batch 301 in epoch 0, gen_loss = 0.4314984681195771, disc_loss = 0.16908511198094942
Trained batch 302 in epoch 0, gen_loss = 0.43108649232206564, disc_loss = 0.1692123324994502
Trained batch 303 in epoch 0, gen_loss = 0.43081387063782467, disc_loss = 0.16967820409856932
Trained batch 304 in epoch 0, gen_loss = 0.43080082525972463, disc_loss = 0.16955768538669486
Trained batch 305 in epoch 0, gen_loss = 0.43085189270817376, disc_loss = 0.16974454577038295
Trained batch 306 in epoch 0, gen_loss = 0.4307551928568352, disc_loss = 0.16965770987787737
Trained batch 307 in epoch 0, gen_loss = 0.43037729033014993, disc_loss = 0.16969618757025567
Trained batch 308 in epoch 0, gen_loss = 0.43054592377931167, disc_loss = 0.1697738862093116
Trained batch 309 in epoch 0, gen_loss = 0.43047207430485757, disc_loss = 0.16962948762961932
Trained batch 310 in epoch 0, gen_loss = 0.4302605358925663, disc_loss = 0.16987141774566036
Trained batch 311 in epoch 0, gen_loss = 0.4300207272171974, disc_loss = 0.1697540796087243
Trained batch 312 in epoch 0, gen_loss = 0.4298967362973637, disc_loss = 0.1699876554798299
Trained batch 313 in epoch 0, gen_loss = 0.42981470978943404, disc_loss = 0.16978707311046162
Trained batch 314 in epoch 0, gen_loss = 0.4296762394526648, disc_loss = 0.16968904960131834
Trained batch 315 in epoch 0, gen_loss = 0.42948715445361557, disc_loss = 0.16975961634417688
Trained batch 316 in epoch 0, gen_loss = 0.4294141166217696, disc_loss = 0.16976769761496727
Trained batch 317 in epoch 0, gen_loss = 0.42898701225061836, disc_loss = 0.16992342827619059
Trained batch 318 in epoch 0, gen_loss = 0.42910622681569904, disc_loss = 0.16969423551334295
Trained batch 319 in epoch 0, gen_loss = 0.42904889676719904, disc_loss = 0.1695631406677421
Trained batch 320 in epoch 0, gen_loss = 0.4290092939342665, disc_loss = 0.16956674044725495
Trained batch 321 in epoch 0, gen_loss = 0.42886030090891797, disc_loss = 0.16990585814328482
Trained batch 322 in epoch 0, gen_loss = 0.428850103181213, disc_loss = 0.17015067775713585
Trained batch 323 in epoch 0, gen_loss = 0.42869842300812405, disc_loss = 0.17002065497577007
Trained batch 324 in epoch 0, gen_loss = 0.4287791067820329, disc_loss = 0.1702636967656704
Trained batch 325 in epoch 0, gen_loss = 0.42849898996528674, disc_loss = 0.17034411836598365
Trained batch 326 in epoch 0, gen_loss = 0.4285573452619967, disc_loss = 0.17047574017367778
Trained batch 327 in epoch 0, gen_loss = 0.4284655884271715, disc_loss = 0.17045706841617642
Trained batch 328 in epoch 0, gen_loss = 0.42839989220117725, disc_loss = 0.17025379197371948
Trained batch 329 in epoch 0, gen_loss = 0.4283928768201308, disc_loss = 0.17006058450788258
Trained batch 330 in epoch 0, gen_loss = 0.42819160421089103, disc_loss = 0.16984696287290388
Trained batch 331 in epoch 0, gen_loss = 0.42798034182514055, disc_loss = 0.169738703255867
Trained batch 332 in epoch 0, gen_loss = 0.42806070855071954, disc_loss = 0.16957351079283356
Trained batch 333 in epoch 0, gen_loss = 0.4279879314635328, disc_loss = 0.1694068044680589
Trained batch 334 in epoch 0, gen_loss = 0.4278643241569177, disc_loss = 0.16918784568447676
Trained batch 335 in epoch 0, gen_loss = 0.42772669966022175, disc_loss = 0.1689653237339198
Trained batch 336 in epoch 0, gen_loss = 0.4276756348878764, disc_loss = 0.16883568628760226
Trained batch 337 in epoch 0, gen_loss = 0.4276814423721923, disc_loss = 0.16871418653485867
Trained batch 338 in epoch 0, gen_loss = 0.42762536021460473, disc_loss = 0.16865858369147005
Trained batch 339 in epoch 0, gen_loss = 0.4276279817609226, disc_loss = 0.16831914017196087
Trained batch 340 in epoch 0, gen_loss = 0.427499039344424, disc_loss = 0.16811225785603445
Trained batch 341 in epoch 0, gen_loss = 0.4275690825187672, disc_loss = 0.16785505918883964
Trained batch 342 in epoch 0, gen_loss = 0.42744621286934387, disc_loss = 0.16790142042274553
Trained batch 343 in epoch 0, gen_loss = 0.42729968863517737, disc_loss = 0.16840692258136736
Trained batch 344 in epoch 0, gen_loss = 0.42721668023994, disc_loss = 0.16862059658312278
Trained batch 345 in epoch 0, gen_loss = 0.42701151579446184, disc_loss = 0.16865816505119807
Trained batch 346 in epoch 0, gen_loss = 0.42684891338994246, disc_loss = 0.168462012440868
Trained batch 347 in epoch 0, gen_loss = 0.42682442403045195, disc_loss = 0.1683849479906775
Trained batch 348 in epoch 0, gen_loss = 0.4267305660896793, disc_loss = 0.16844483876446087
Trained batch 349 in epoch 0, gen_loss = 0.4265092670917511, disc_loss = 0.1686437813671572
Trained batch 350 in epoch 0, gen_loss = 0.42649972905800215, disc_loss = 0.1686597397641154
Trained batch 351 in epoch 0, gen_loss = 0.4262940092012286, disc_loss = 0.16869207183216614
Trained batch 352 in epoch 0, gen_loss = 0.42626018184778036, disc_loss = 0.16855941876959024
Trained batch 353 in epoch 0, gen_loss = 0.4262733866939437, disc_loss = 0.1685332309844046
Trained batch 354 in epoch 0, gen_loss = 0.4261488577849428, disc_loss = 0.16831481716477534
Trained batch 355 in epoch 0, gen_loss = 0.42613070366087924, disc_loss = 0.1679537923577545
Trained batch 356 in epoch 0, gen_loss = 0.42615188169880075, disc_loss = 0.16804591633629898
Trained batch 357 in epoch 0, gen_loss = 0.42617495224795526, disc_loss = 0.16796827585275112
Trained batch 358 in epoch 0, gen_loss = 0.42603634119365874, disc_loss = 0.16761794736645183
Trained batch 359 in epoch 0, gen_loss = 0.4257877480652597, disc_loss = 0.16748564512882796
Trained batch 360 in epoch 0, gen_loss = 0.42568646945121213, disc_loss = 0.16814237944988167
Trained batch 361 in epoch 0, gen_loss = 0.42538796579310906, disc_loss = 0.16865957948734253
Trained batch 362 in epoch 0, gen_loss = 0.4255351859347551, disc_loss = 0.1693512557264284
Trained batch 363 in epoch 0, gen_loss = 0.4254948081714766, disc_loss = 0.16986033496968858
Trained batch 364 in epoch 0, gen_loss = 0.42535687301256886, disc_loss = 0.1702433528608247
Trained batch 365 in epoch 0, gen_loss = 0.42506834319054754, disc_loss = 0.17092349877469037
Trained batch 366 in epoch 0, gen_loss = 0.42488965164738063, disc_loss = 0.17113138098340755
Trained batch 367 in epoch 0, gen_loss = 0.4246879305852496, disc_loss = 0.17122544393287567
Trained batch 368 in epoch 0, gen_loss = 0.4244667878803522, disc_loss = 0.1711104085825404
Trained batch 369 in epoch 0, gen_loss = 0.4242166269469905, disc_loss = 0.1710114393735657
Trained batch 370 in epoch 0, gen_loss = 0.4243228413024039, disc_loss = 0.17082443462328287
Trained batch 371 in epoch 0, gen_loss = 0.4241910740252464, disc_loss = 0.17060347592898756
Trained batch 372 in epoch 0, gen_loss = 0.42401145945924856, disc_loss = 0.17070131521701973
Trained batch 373 in epoch 0, gen_loss = 0.42379751227756235, disc_loss = 0.1707401596617332
Trained batch 374 in epoch 0, gen_loss = 0.42372584708531696, disc_loss = 0.17054454478124778
Trained batch 375 in epoch 0, gen_loss = 0.4236464951425157, disc_loss = 0.1709215361278187
Trained batch 376 in epoch 0, gen_loss = 0.42347027357440414, disc_loss = 0.17201224191297113
Trained batch 377 in epoch 0, gen_loss = 0.423302607325019, disc_loss = 0.1725602357367438
Trained batch 378 in epoch 0, gen_loss = 0.4232008649837373, disc_loss = 0.172825871723505
Trained batch 379 in epoch 0, gen_loss = 0.42289602999624454, disc_loss = 0.1730658695150755
Trained batch 380 in epoch 0, gen_loss = 0.42270202172084115, disc_loss = 0.17331353205884378
Trained batch 381 in epoch 0, gen_loss = 0.4224039139079798, disc_loss = 0.17355016116247907
Trained batch 382 in epoch 0, gen_loss = 0.4225726736121016, disc_loss = 0.17377097628932864
Trained batch 383 in epoch 0, gen_loss = 0.4225197018434604, disc_loss = 0.17396171015085807
Trained batch 384 in epoch 0, gen_loss = 0.42225653815579106, disc_loss = 0.1741357772042612
Trained batch 385 in epoch 0, gen_loss = 0.4221174115498449, disc_loss = 0.17429494815806187
Trained batch 386 in epoch 0, gen_loss = 0.42189082503318787, disc_loss = 0.1744626752445017
Trained batch 387 in epoch 0, gen_loss = 0.42162139921151487, disc_loss = 0.17461436707046382
Trained batch 388 in epoch 0, gen_loss = 0.42149186862158594, disc_loss = 0.17475555182595026
Trained batch 389 in epoch 0, gen_loss = 0.42138426892268355, disc_loss = 0.17486070425560077
Trained batch 390 in epoch 0, gen_loss = 0.421035239763577, disc_loss = 0.174948404821784
Trained batch 391 in epoch 0, gen_loss = 0.4207121281295407, disc_loss = 0.17503352938885136
Trained batch 392 in epoch 0, gen_loss = 0.42046808498809657, disc_loss = 0.17516862933284605
Trained batch 393 in epoch 0, gen_loss = 0.4204095049860514, disc_loss = 0.17530512564394832
Trained batch 394 in epoch 0, gen_loss = 0.4202936372424983, disc_loss = 0.1754715798446272
Trained batch 395 in epoch 0, gen_loss = 0.4203859274887075, disc_loss = 0.1755925533643952
Trained batch 396 in epoch 0, gen_loss = 0.4203053022211685, disc_loss = 0.1756717973348851
Trained batch 397 in epoch 0, gen_loss = 0.42016068819779245, disc_loss = 0.17572054334789813
Trained batch 398 in epoch 0, gen_loss = 0.4198725842742394, disc_loss = 0.1757656899806028
Trained batch 399 in epoch 0, gen_loss = 0.4197089787572622, disc_loss = 0.17581334986258298
Trained batch 400 in epoch 0, gen_loss = 0.4196653076240844, disc_loss = 0.17571611803238676
Trained batch 401 in epoch 0, gen_loss = 0.41958880795175163, disc_loss = 0.17574354024390823
Trained batch 402 in epoch 0, gen_loss = 0.4194293337927267, disc_loss = 0.17575206853930028
Trained batch 403 in epoch 0, gen_loss = 0.4193626507969186, disc_loss = 0.17575662749552048
Trained batch 404 in epoch 0, gen_loss = 0.4193478925728504, disc_loss = 0.17582655071200412
Trained batch 405 in epoch 0, gen_loss = 0.4190565686801384, disc_loss = 0.17628798593900033
Trained batch 406 in epoch 0, gen_loss = 0.41910069785481296, disc_loss = 0.17646797653516474
Trained batch 407 in epoch 0, gen_loss = 0.41924754866198, disc_loss = 0.1766736347127852
Trained batch 408 in epoch 0, gen_loss = 0.41925977443716056, disc_loss = 0.1765641360281805
Trained batch 409 in epoch 0, gen_loss = 0.4190100818872452, disc_loss = 0.1766183970950362
Trained batch 410 in epoch 0, gen_loss = 0.418799811108559, disc_loss = 0.17688105844976404
Trained batch 411 in epoch 0, gen_loss = 0.4185664142364437, disc_loss = 0.17679996604497716
Trained batch 412 in epoch 0, gen_loss = 0.41850981080214567, disc_loss = 0.17692678341006107
Trained batch 413 in epoch 0, gen_loss = 0.4182113277307455, disc_loss = 0.17712575726768964
Trained batch 414 in epoch 0, gen_loss = 0.41800436047186335, disc_loss = 0.17733366529356284
Trained batch 415 in epoch 0, gen_loss = 0.4179399044324572, disc_loss = 0.17738370723180616
Trained batch 416 in epoch 0, gen_loss = 0.41765124713488333, disc_loss = 0.17742698334550544
Trained batch 417 in epoch 0, gen_loss = 0.4176127524181987, disc_loss = 0.1775331267659245
Trained batch 418 in epoch 0, gen_loss = 0.4176762853004483, disc_loss = 0.1775919094477249
Trained batch 419 in epoch 0, gen_loss = 0.41764012099731534, disc_loss = 0.17743134921239245
Trained batch 420 in epoch 0, gen_loss = 0.417643252999086, disc_loss = 0.17743849701683906
Trained batch 421 in epoch 0, gen_loss = 0.4176648658881255, disc_loss = 0.1773451319564646
Trained batch 422 in epoch 0, gen_loss = 0.4174930527277872, disc_loss = 0.17736546618172605
Trained batch 423 in epoch 0, gen_loss = 0.417449738127443, disc_loss = 0.17734949962446853
Trained batch 424 in epoch 0, gen_loss = 0.41734666592934555, disc_loss = 0.17750344389063472
Trained batch 425 in epoch 0, gen_loss = 0.41714688303045266, disc_loss = 0.17749820132368188
Trained batch 426 in epoch 0, gen_loss = 0.4169810449211603, disc_loss = 0.1775314447794991
Trained batch 427 in epoch 0, gen_loss = 0.41682776453617576, disc_loss = 0.1775353472967084
Trained batch 428 in epoch 0, gen_loss = 0.4165718067776073, disc_loss = 0.17749138288026228
Trained batch 429 in epoch 0, gen_loss = 0.41647398624309273, disc_loss = 0.17744574720516454
Trained batch 430 in epoch 0, gen_loss = 0.4164382962643962, disc_loss = 0.17743039674400038
Trained batch 431 in epoch 0, gen_loss = 0.41632858891454005, disc_loss = 0.1773266042108406
Trained batch 432 in epoch 0, gen_loss = 0.4161747793815428, disc_loss = 0.17722745883702268
Trained batch 433 in epoch 0, gen_loss = 0.41618443922513093, disc_loss = 0.17727825458028487
Trained batch 434 in epoch 0, gen_loss = 0.415960214329862, disc_loss = 0.17736363336015706
Trained batch 435 in epoch 0, gen_loss = 0.4159230758017356, disc_loss = 0.17716687600290693
Trained batch 436 in epoch 0, gen_loss = 0.415926135172833, disc_loss = 0.177092570329865
Trained batch 437 in epoch 0, gen_loss = 0.4159833743964156, disc_loss = 0.17692253426074572
Trained batch 438 in epoch 0, gen_loss = 0.41577715801758214, disc_loss = 0.17678144039980778
Trained batch 439 in epoch 0, gen_loss = 0.4158760775896636, disc_loss = 0.17674843823452566
Trained batch 440 in epoch 0, gen_loss = 0.41582080471813004, disc_loss = 0.1765814767073409
Trained batch 441 in epoch 0, gen_loss = 0.4158068166464163, disc_loss = 0.17642001722142842
Trained batch 442 in epoch 0, gen_loss = 0.41573718856204445, disc_loss = 0.17663311946960525
Trained batch 443 in epoch 0, gen_loss = 0.41561671333001543, disc_loss = 0.17664347299131322
Trained batch 444 in epoch 0, gen_loss = 0.415571119879069, disc_loss = 0.17635190557646618
Trained batch 445 in epoch 0, gen_loss = 0.41571508608591395, disc_loss = 0.17618024060627938
Trained batch 446 in epoch 0, gen_loss = 0.4155868059836778, disc_loss = 0.17601887163783214
Trained batch 447 in epoch 0, gen_loss = 0.4156246260473771, disc_loss = 0.17597255914422152
Trained batch 448 in epoch 0, gen_loss = 0.41563447154975414, disc_loss = 0.17595694997744066
Trained batch 449 in epoch 0, gen_loss = 0.4155134944783317, disc_loss = 0.17585723316089974
Trained batch 450 in epoch 0, gen_loss = 0.4153191070598932, disc_loss = 0.17605566610468598
Trained batch 451 in epoch 0, gen_loss = 0.4150626977196837, disc_loss = 0.17650428231789078
Trained batch 452 in epoch 0, gen_loss = 0.4150027633371206, disc_loss = 0.17646780895032235
Trained batch 453 in epoch 0, gen_loss = 0.41508465066617806, disc_loss = 0.1763269808594679
Trained batch 454 in epoch 0, gen_loss = 0.4150899348023174, disc_loss = 0.17628177298376194
Trained batch 455 in epoch 0, gen_loss = 0.41507286336599736, disc_loss = 0.17600968914318896
Trained batch 456 in epoch 0, gen_loss = 0.41496532477226505, disc_loss = 0.17574190566442596
Trained batch 457 in epoch 0, gen_loss = 0.41506458363418497, disc_loss = 0.17547006062008277
Trained batch 458 in epoch 0, gen_loss = 0.41517104912946945, disc_loss = 0.175332361020651
Trained batch 459 in epoch 0, gen_loss = 0.4152111989648446, disc_loss = 0.17524352325936374
Trained batch 460 in epoch 0, gen_loss = 0.41537625968068387, disc_loss = 0.1752945055372997
Testing Epoch 0
Traceback (most recent call last):
  File "srgan_bones.py", line 330, in <module>
    img_grid = utils.make_thickness_images(imgs_hr[:5], imgs_lr[:5], gen_hr[:5])
  File "/work3/soeba/HALOS/utils.py", line 18, in make_thickness_images
    thickness_hr.append(ps.filters.local_thickness(bin_im_hr.detach().numpy(), mode='dt'))
TypeError: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.