wandb: WARNING Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.
/work3/soeba/HALOS/halos/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.
  warnings.warn(
/work3/soeba/HALOS/halos/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Training Epoch 0
Trained batch 0 in epoch 0, gen_loss = 1.1977653503417969, disc_loss = 0.4634488821029663
Trained batch 1 in epoch 0, gen_loss = 1.2024332284927368, disc_loss = 0.5108547210693359
Trained batch 2 in epoch 0, gen_loss = 1.2140376170476277, disc_loss = 0.5354916652043661
Trained batch 3 in epoch 0, gen_loss = 1.1635588705539703, disc_loss = 0.4660033583641052
Trained batch 4 in epoch 0, gen_loss = 1.132981300354004, disc_loss = 0.41927596032619474
Trained batch 5 in epoch 0, gen_loss = 1.1260237296422322, disc_loss = 0.38079334050416946
Trained batch 6 in epoch 0, gen_loss = 1.1216883318764823, disc_loss = 0.3496160422052656
Trained batch 7 in epoch 0, gen_loss = 1.1402961909770966, disc_loss = 0.32376142777502537
Trained batch 8 in epoch 0, gen_loss = 1.151450475056966, disc_loss = 0.30415325032340157
Trained batch 9 in epoch 0, gen_loss = 1.1603168845176697, disc_loss = 0.28411240726709364
Trained batch 10 in epoch 0, gen_loss = 1.1768196821212769, disc_loss = 0.26784470270980487
Trained batch 11 in epoch 0, gen_loss = 1.2030457158883412, disc_loss = 0.2524910407761733
Trained batch 12 in epoch 0, gen_loss = 1.2291489839553833, disc_loss = 0.23871229474361128
Trained batch 13 in epoch 0, gen_loss = 1.2638754418918066, disc_loss = 0.22628450127584593
Trained batch 14 in epoch 0, gen_loss = 1.2881809314092, disc_loss = 0.2153609464565913
Trained batch 15 in epoch 0, gen_loss = 1.3096081838011742, disc_loss = 0.20538425957784057
Trained batch 16 in epoch 0, gen_loss = 1.329708176500657, disc_loss = 0.19687658197739544
Trained batch 17 in epoch 0, gen_loss = 1.3506106270684137, disc_loss = 0.18948939194281897
Trained batch 18 in epoch 0, gen_loss = 1.369005931051154, disc_loss = 0.18300458200668035
Trained batch 19 in epoch 0, gen_loss = 1.3885627865791321, disc_loss = 0.17842878662049771
Trained batch 20 in epoch 0, gen_loss = 1.4067535513923282, disc_loss = 0.17471480263131006
Trained batch 21 in epoch 0, gen_loss = 1.4196205030788074, disc_loss = 0.17026232555508614
Trained batch 22 in epoch 0, gen_loss = 1.4296879198240198, disc_loss = 0.16570723963820416
Trained batch 23 in epoch 0, gen_loss = 1.4403421133756638, disc_loss = 0.16069338951880732
Trained batch 24 in epoch 0, gen_loss = 1.4496834325790404, disc_loss = 0.15664196342229844
Trained batch 25 in epoch 0, gen_loss = 1.4611033613865192, disc_loss = 0.1521400296344207
Trained batch 26 in epoch 0, gen_loss = 1.4714164557280365, disc_loss = 0.14812895931579448
Trained batch 27 in epoch 0, gen_loss = 1.4774648674896784, disc_loss = 0.1442577415811164
Trained batch 28 in epoch 0, gen_loss = 1.4853673680075283, disc_loss = 0.1407195437571098
Trained batch 29 in epoch 0, gen_loss = 1.4898534814516704, disc_loss = 0.1374329141030709
Trained batch 30 in epoch 0, gen_loss = 1.4922030279713292, disc_loss = 0.13481962909140893
Trained batch 31 in epoch 0, gen_loss = 1.4875887371599674, disc_loss = 0.132578830816783
Trained batch 32 in epoch 0, gen_loss = 1.492960810661316, disc_loss = 0.13058493049307304
Trained batch 33 in epoch 0, gen_loss = 1.4972632387105156, disc_loss = 0.12885416967465596
Trained batch 34 in epoch 0, gen_loss = 1.4986934695925032, disc_loss = 0.1273382960685662
Trained batch 35 in epoch 0, gen_loss = 1.4938322471247778, disc_loss = 0.12701583601948288
Trained batch 36 in epoch 0, gen_loss = 1.5041956482706844, disc_loss = 0.1292511984907292
Trained batch 37 in epoch 0, gen_loss = 1.497101234762292, disc_loss = 0.13226286861065187
Trained batch 38 in epoch 0, gen_loss = 1.4886258168098254, disc_loss = 0.1351138707727958
Trained batch 39 in epoch 0, gen_loss = 1.4945151329040527, disc_loss = 0.13897629557177424
Trained batch 40 in epoch 0, gen_loss = 1.473972250775593, disc_loss = 0.14815779021236955
Trained batch 41 in epoch 0, gen_loss = 1.462245789312181, disc_loss = 0.15057637072390034
Trained batch 42 in epoch 0, gen_loss = 1.4609991908073425, disc_loss = 0.1542071657645148
Trained batch 43 in epoch 0, gen_loss = 1.4535527459599755, disc_loss = 0.1596406331624497
Trained batch 44 in epoch 0, gen_loss = 1.439651800526513, disc_loss = 0.16638695125778516
Trained batch 45 in epoch 0, gen_loss = 1.427420119876447, disc_loss = 0.16754974351952906
Trained batch 46 in epoch 0, gen_loss = 1.418921743301635, disc_loss = 0.16875700866605373
Trained batch 47 in epoch 0, gen_loss = 1.4089961213370163, disc_loss = 0.1693999859659622
Trained batch 48 in epoch 0, gen_loss = 1.400810919245895, disc_loss = 0.16970199460581858
Trained batch 49 in epoch 0, gen_loss = 1.3929952991008758, disc_loss = 0.17034740202128887
Trained batch 50 in epoch 0, gen_loss = 1.387792824530134, disc_loss = 0.17079346342121854
Trained batch 51 in epoch 0, gen_loss = 1.3762905448675156, disc_loss = 0.1719127929984377
Trained batch 52 in epoch 0, gen_loss = 1.3709929607949167, disc_loss = 0.17140345212142422
Trained batch 53 in epoch 0, gen_loss = 1.3673744941199268, disc_loss = 0.17100769853978245
Trained batch 54 in epoch 0, gen_loss = 1.3564781145616012, disc_loss = 0.1712270489470525
Trained batch 55 in epoch 0, gen_loss = 1.353106541293008, disc_loss = 0.1707226263492235
Trained batch 56 in epoch 0, gen_loss = 1.3432786569260715, disc_loss = 0.17080316071708998
Trained batch 57 in epoch 0, gen_loss = 1.3517581388868134, disc_loss = 0.17172686877692567
Trained batch 58 in epoch 0, gen_loss = 1.3375742364737948, disc_loss = 0.17770707979798317
Trained batch 59 in epoch 0, gen_loss = 1.3297416975100835, disc_loss = 0.17792009953409432
Trained batch 60 in epoch 0, gen_loss = 1.3329000873643844, disc_loss = 0.1785365107973091
Trained batch 61 in epoch 0, gen_loss = 1.3292704291882054, disc_loss = 0.17816719406795117
Trained batch 62 in epoch 0, gen_loss = 1.3235546889759244, disc_loss = 0.1780446246857681
Trained batch 63 in epoch 0, gen_loss = 1.3204919984564185, disc_loss = 0.1771886104834266
Trained batch 64 in epoch 0, gen_loss = 1.3137878858126126, disc_loss = 0.17638110478336994
Trained batch 65 in epoch 0, gen_loss = 1.3152236288244075, disc_loss = 0.17741531997241758
Trained batch 66 in epoch 0, gen_loss = 1.3057828216410394, disc_loss = 0.17846351267019314
Trained batch 67 in epoch 0, gen_loss = 1.3033041410586412, disc_loss = 0.17811025443541653
Trained batch 68 in epoch 0, gen_loss = 1.2976636575615925, disc_loss = 0.17768312866489092
Trained batch 69 in epoch 0, gen_loss = 1.2971631356648037, disc_loss = 0.17770250317241465
Trained batch 70 in epoch 0, gen_loss = 1.2906744564083261, disc_loss = 0.17734361371733773
Trained batch 71 in epoch 0, gen_loss = 1.2886781328254275, disc_loss = 0.17669021736623514
Trained batch 72 in epoch 0, gen_loss = 1.2859268874338228, disc_loss = 0.17600415100994177
Trained batch 73 in epoch 0, gen_loss = 1.2840715711181228, disc_loss = 0.1750090817442617
Trained batch 74 in epoch 0, gen_loss = 1.2820046250025432, disc_loss = 0.17399284010132154
Trained batch 75 in epoch 0, gen_loss = 1.2789495257954848, disc_loss = 0.17316062145523334
Trained batch 76 in epoch 0, gen_loss = 1.2811305135875553, disc_loss = 0.17354520405461263
Trained batch 77 in epoch 0, gen_loss = 1.2723759962962224, disc_loss = 0.17730136927312765
Trained batch 78 in epoch 0, gen_loss = 1.2688387149496922, disc_loss = 0.17746641840549965
Trained batch 79 in epoch 0, gen_loss = 1.2666972011327744, disc_loss = 0.17720286776311694
Trained batch 80 in epoch 0, gen_loss = 1.2618061421829978, disc_loss = 0.1769278287243696
Trained batch 81 in epoch 0, gen_loss = 1.259916035140433, disc_loss = 0.17638818215487934
Trained batch 82 in epoch 0, gen_loss = 1.2566906213760376, disc_loss = 0.17617884731615882
Trained batch 83 in epoch 0, gen_loss = 1.2520104824077516, disc_loss = 0.17586653808220512
Trained batch 84 in epoch 0, gen_loss = 1.2562313647831187, disc_loss = 0.17772240660646382
Trained batch 85 in epoch 0, gen_loss = 1.2499002689539, disc_loss = 0.17859560688740986
Trained batch 86 in epoch 0, gen_loss = 1.2453166116243122, disc_loss = 0.1783328041348649
Trained batch 87 in epoch 0, gen_loss = 1.2429273799061775, disc_loss = 0.17866199772635644
Trained batch 88 in epoch 0, gen_loss = 1.2380548983477475, disc_loss = 0.17886922286635035
Trained batch 89 in epoch 0, gen_loss = 1.237105127175649, disc_loss = 0.17931681262950103
Trained batch 90 in epoch 0, gen_loss = 1.2302886771631765, disc_loss = 0.17998523473903372
Trained batch 91 in epoch 0, gen_loss = 1.2270715631868527, disc_loss = 0.17978475797597482
Trained batch 92 in epoch 0, gen_loss = 1.2275317119013878, disc_loss = 0.18014907568532934
Trained batch 93 in epoch 0, gen_loss = 1.2202454399555287, disc_loss = 0.18140875361859798
Trained batch 94 in epoch 0, gen_loss = 1.2201078941947536, disc_loss = 0.1808760045782516
Trained batch 95 in epoch 0, gen_loss = 1.2192082616190116, disc_loss = 0.1802335964748636
Trained batch 96 in epoch 0, gen_loss = 1.2141951746547346, disc_loss = 0.1800081758692707
Trained batch 97 in epoch 0, gen_loss = 1.2108858051348705, disc_loss = 0.17990346568427523
Trained batch 98 in epoch 0, gen_loss = 1.2074957345471238, disc_loss = 0.17975310781838919
Trained batch 99 in epoch 0, gen_loss = 1.207968916296959, disc_loss = 0.18069838609546424
Trained batch 100 in epoch 0, gen_loss = 1.2029449756782833, disc_loss = 0.18295475600822136
Trained batch 101 in epoch 0, gen_loss = 1.1992609331420823, disc_loss = 0.18321330052818738
Trained batch 102 in epoch 0, gen_loss = 1.1993363870000375, disc_loss = 0.18353489124514524
Trained batch 103 in epoch 0, gen_loss = 1.1958435309621005, disc_loss = 0.18412633465889555
Trained batch 104 in epoch 0, gen_loss = 1.1913598815600077, disc_loss = 0.1844389214047364
Trained batch 105 in epoch 0, gen_loss = 1.1883567419816863, disc_loss = 0.18447368433874733
Trained batch 106 in epoch 0, gen_loss = 1.1858862950423052, disc_loss = 0.18464671726399493
Trained batch 107 in epoch 0, gen_loss = 1.1826121917477361, disc_loss = 0.18479921196207957
Trained batch 108 in epoch 0, gen_loss = 1.1804540791642775, disc_loss = 0.1848833899637428
Trained batch 109 in epoch 0, gen_loss = 1.1776415938680822, disc_loss = 0.1847025586461479
Trained batch 110 in epoch 0, gen_loss = 1.173976101853826, disc_loss = 0.18464257155318517
Trained batch 111 in epoch 0, gen_loss = 1.1716090137405055, disc_loss = 0.1844958359309073
Trained batch 112 in epoch 0, gen_loss = 1.1704613914531945, disc_loss = 0.18440122674919862
Trained batch 113 in epoch 0, gen_loss = 1.1655497509136534, disc_loss = 0.18486768709855123
Trained batch 114 in epoch 0, gen_loss = 1.1630685598953912, disc_loss = 0.18528800396167713
Trained batch 115 in epoch 0, gen_loss = 1.1616918280206878, disc_loss = 0.18566024749828824
Trained batch 116 in epoch 0, gen_loss = 1.157218405833611, disc_loss = 0.18617214348453742
Trained batch 117 in epoch 0, gen_loss = 1.1567170827065485, disc_loss = 0.18615787637309503
Trained batch 118 in epoch 0, gen_loss = 1.1556482480353667, disc_loss = 0.18608507448259523
Trained batch 119 in epoch 0, gen_loss = 1.1525812606016794, disc_loss = 0.18617737277721366
Trained batch 120 in epoch 0, gen_loss = 1.1532499869007709, disc_loss = 0.1859626931038277
Trained batch 121 in epoch 0, gen_loss = 1.1518422414044864, disc_loss = 0.1854810466470777
Trained batch 122 in epoch 0, gen_loss = 1.1519811919065026, disc_loss = 0.1851752956163108
Trained batch 123 in epoch 0, gen_loss = 1.1517249393847682, disc_loss = 0.1850085569425456
Trained batch 124 in epoch 0, gen_loss = 1.1479374499320985, disc_loss = 0.18515239533782005
Trained batch 125 in epoch 0, gen_loss = 1.1489157142147186, disc_loss = 0.18526662126301774
Trained batch 126 in epoch 0, gen_loss = 1.145951982088915, disc_loss = 0.18533197934115966
Trained batch 127 in epoch 0, gen_loss = 1.1453380198217928, disc_loss = 0.18497282124008052
Trained batch 128 in epoch 0, gen_loss = 1.1438181210857954, disc_loss = 0.1846735992346161
Trained batch 129 in epoch 0, gen_loss = 1.1423725774654976, disc_loss = 0.18432134811121684
Trained batch 130 in epoch 0, gen_loss = 1.142658727314636, disc_loss = 0.18409578431084866
Trained batch 131 in epoch 0, gen_loss = 1.138763813809915, disc_loss = 0.1847861303919644
Trained batch 132 in epoch 0, gen_loss = 1.1410634853786095, disc_loss = 0.18489538948226691
Trained batch 133 in epoch 0, gen_loss = 1.1374728274879171, disc_loss = 0.18630952907920773
Trained batch 134 in epoch 0, gen_loss = 1.1362876843523095, disc_loss = 0.18632616408997113
Trained batch 135 in epoch 0, gen_loss = 1.136111224837163, disc_loss = 0.1865135504425887
Trained batch 136 in epoch 0, gen_loss = 1.133933602458369, disc_loss = 0.1868349265606299
Trained batch 137 in epoch 0, gen_loss = 1.1321305010629736, disc_loss = 0.18710480454931225
Trained batch 138 in epoch 0, gen_loss = 1.131272983207977, disc_loss = 0.18708919800764365
Trained batch 139 in epoch 0, gen_loss = 1.1297774263790676, disc_loss = 0.18711734794612442
Trained batch 140 in epoch 0, gen_loss = 1.1269907710400033, disc_loss = 0.1871990123697629
Trained batch 141 in epoch 0, gen_loss = 1.12524858704755, disc_loss = 0.18692760219351506
Trained batch 142 in epoch 0, gen_loss = 1.1240904502101712, disc_loss = 0.18665850628818664
Trained batch 143 in epoch 0, gen_loss = 1.1230935665468376, disc_loss = 0.18634804201105404
Trained batch 144 in epoch 0, gen_loss = 1.122053007832889, disc_loss = 0.18623567149043083
Trained batch 145 in epoch 0, gen_loss = 1.122357675065733, disc_loss = 0.18546365224437356
Trained batch 146 in epoch 0, gen_loss = 1.1209859819639296, disc_loss = 0.18488604040677045
Trained batch 147 in epoch 0, gen_loss = 1.121783514683311, disc_loss = 0.18445471802575364
Trained batch 148 in epoch 0, gen_loss = 1.1188833229493775, disc_loss = 0.1845129080016741
Trained batch 149 in epoch 0, gen_loss = 1.1241060380140941, disc_loss = 0.18582519290347893
Trained batch 150 in epoch 0, gen_loss = 1.1206043666561707, disc_loss = 0.18620874794409764
Trained batch 151 in epoch 0, gen_loss = 1.1180673390626907, disc_loss = 0.18638492539819135
Trained batch 152 in epoch 0, gen_loss = 1.1176952443091699, disc_loss = 0.18650086903396776
Trained batch 153 in epoch 0, gen_loss = 1.1196853897788308, disc_loss = 0.1871060969719252
Trained batch 154 in epoch 0, gen_loss = 1.1173726474085162, disc_loss = 0.18691278751338683
Trained batch 155 in epoch 0, gen_loss = 1.1142340493507874, disc_loss = 0.18783312135686478
Trained batch 156 in epoch 0, gen_loss = 1.112191161532311, disc_loss = 0.18777353176550501
Trained batch 157 in epoch 0, gen_loss = 1.1110573267634911, disc_loss = 0.1877159670325397
Trained batch 158 in epoch 0, gen_loss = 1.1092807905479047, disc_loss = 0.18851749193649622
Trained batch 159 in epoch 0, gen_loss = 1.1064743328839541, disc_loss = 0.18898872898425906
Trained batch 160 in epoch 0, gen_loss = 1.1047010018218377, disc_loss = 0.18916368722823096
Trained batch 161 in epoch 0, gen_loss = 1.1030840752301392, disc_loss = 0.18969321414184423
Trained batch 162 in epoch 0, gen_loss = 1.1008622101479513, disc_loss = 0.19015512526309564
Trained batch 163 in epoch 0, gen_loss = 1.0983465186706403, disc_loss = 0.19055744040212252
Trained batch 164 in epoch 0, gen_loss = 1.0966268983754244, disc_loss = 0.19098453862649023
Trained batch 165 in epoch 0, gen_loss = 1.0946381206972053, disc_loss = 0.1914492121796651
Trained batch 166 in epoch 0, gen_loss = 1.092178393266872, disc_loss = 0.19162214674546332
Trained batch 167 in epoch 0, gen_loss = 1.0901542451410067, disc_loss = 0.19193573116457888
Trained batch 168 in epoch 0, gen_loss = 1.0893249656321735, disc_loss = 0.19228018597650104
Trained batch 169 in epoch 0, gen_loss = 1.0868403571493486, disc_loss = 0.19257880105253528
Trained batch 170 in epoch 0, gen_loss = 1.08578271022317, disc_loss = 0.19268115368067174
Trained batch 171 in epoch 0, gen_loss = 1.0840101255926975, disc_loss = 0.1928412387320815
Trained batch 172 in epoch 0, gen_loss = 1.0831301460376364, disc_loss = 0.1931321631061893
Trained batch 173 in epoch 0, gen_loss = 1.0809839138354378, disc_loss = 0.1934131996855996
Trained batch 174 in epoch 0, gen_loss = 1.0797808756147111, disc_loss = 0.19340720923883575
Trained batch 175 in epoch 0, gen_loss = 1.0789358934218234, disc_loss = 0.1934956068130718
Trained batch 176 in epoch 0, gen_loss = 1.077544880452129, disc_loss = 0.1935367816375137
Trained batch 177 in epoch 0, gen_loss = 1.0754071387012354, disc_loss = 0.19360574148595333
Trained batch 178 in epoch 0, gen_loss = 1.0760252562315105, disc_loss = 0.19398339523533203
Trained batch 179 in epoch 0, gen_loss = 1.0728491690423754, disc_loss = 0.19430211126390431
Trained batch 180 in epoch 0, gen_loss = 1.071735002717919, disc_loss = 0.194213655998529
Trained batch 181 in epoch 0, gen_loss = 1.0715464405961088, disc_loss = 0.19410103573822057
Trained batch 182 in epoch 0, gen_loss = 1.0703432498082437, disc_loss = 0.1939585102834011
Trained batch 183 in epoch 0, gen_loss = 1.068569485584031, disc_loss = 0.1937338827866251
Trained batch 184 in epoch 0, gen_loss = 1.070347351319081, disc_loss = 0.19375858667331772
Trained batch 185 in epoch 0, gen_loss = 1.0676680857776313, disc_loss = 0.19467925791057847
Trained batch 186 in epoch 0, gen_loss = 1.0663481303714812, disc_loss = 0.19441476852737646
Trained batch 187 in epoch 0, gen_loss = 1.0659422167438142, disc_loss = 0.1946032353022948
Trained batch 188 in epoch 0, gen_loss = 1.0648300710809293, disc_loss = 0.19446037439678712
Trained batch 189 in epoch 0, gen_loss = 1.0630859635378185, disc_loss = 0.1946172544634656
Trained batch 190 in epoch 0, gen_loss = 1.06254945406739, disc_loss = 0.19435876434308072
Trained batch 191 in epoch 0, gen_loss = 1.0624614721164107, disc_loss = 0.19420526640412086
Trained batch 192 in epoch 0, gen_loss = 1.0615252889499762, disc_loss = 0.1938485043615566
Trained batch 193 in epoch 0, gen_loss = 1.060962728925587, disc_loss = 0.19349243344028585
Trained batch 194 in epoch 0, gen_loss = 1.0622029906664139, disc_loss = 0.19300996055587744
Trained batch 195 in epoch 0, gen_loss = 1.060576608594583, disc_loss = 0.19282265986334912
Trained batch 196 in epoch 0, gen_loss = 1.0610730750911732, disc_loss = 0.19278609364997917
Trained batch 197 in epoch 0, gen_loss = 1.0594392617543538, disc_loss = 0.1924390389273564
Trained batch 198 in epoch 0, gen_loss = 1.0618827061437481, disc_loss = 0.19214805614334254
Trained batch 199 in epoch 0, gen_loss = 1.0610666924715042, disc_loss = 0.19157783029600978
Trained batch 200 in epoch 0, gen_loss = 1.0591719933410189, disc_loss = 0.19164248261211522
Trained batch 201 in epoch 0, gen_loss = 1.062589170909164, disc_loss = 0.19181969863279622
Trained batch 202 in epoch 0, gen_loss = 1.0606064353083156, disc_loss = 0.19174194062504862
Trained batch 203 in epoch 0, gen_loss = 1.0597582491589528, disc_loss = 0.1914385516817371
Trained batch 204 in epoch 0, gen_loss = 1.0598895093289817, disc_loss = 0.1909670833952543
Trained batch 205 in epoch 0, gen_loss = 1.0587003413334632, disc_loss = 0.1905943109174666
Trained batch 206 in epoch 0, gen_loss = 1.0621205824584776, disc_loss = 0.19057276450853416
Trained batch 207 in epoch 0, gen_loss = 1.0606428565314183, disc_loss = 0.19062722755524403
Trained batch 208 in epoch 0, gen_loss = 1.059985846708836, disc_loss = 0.19046751030110287
Trained batch 209 in epoch 0, gen_loss = 1.0616162291594915, disc_loss = 0.1906471741518804
Trained batch 210 in epoch 0, gen_loss = 1.0600273863399199, disc_loss = 0.19060499409999326
Trained batch 211 in epoch 0, gen_loss = 1.0597531351278413, disc_loss = 0.19050709525439538
Trained batch 212 in epoch 0, gen_loss = 1.058941210379623, disc_loss = 0.1901414987102081
Trained batch 213 in epoch 0, gen_loss = 1.06080680974176, disc_loss = 0.18965497409280774
Trained batch 214 in epoch 0, gen_loss = 1.0608354629472245, disc_loss = 0.18906766860637553
Trained batch 215 in epoch 0, gen_loss = 1.060098062786791, disc_loss = 0.18864017737063546
Trained batch 216 in epoch 0, gen_loss = 1.061764331731928, disc_loss = 0.1880956663479728
Trained batch 217 in epoch 0, gen_loss = 1.0627152135065936, disc_loss = 0.18759451985974376
Trained batch 218 in epoch 0, gen_loss = 1.0650559606617445, disc_loss = 0.18685002451481886
Trained batch 219 in epoch 0, gen_loss = 1.0651351660490036, disc_loss = 0.18636019612577828
Trained batch 220 in epoch 0, gen_loss = 1.0680787204617288, disc_loss = 0.18566635050452673
Trained batch 221 in epoch 0, gen_loss = 1.07056850651363, disc_loss = 0.18506108214323586
Trained batch 222 in epoch 0, gen_loss = 1.068928158603976, disc_loss = 0.18512125229755325
Trained batch 223 in epoch 0, gen_loss = 1.071457903566105, disc_loss = 0.18568553632524396
Trained batch 224 in epoch 0, gen_loss = 1.0699223091867236, disc_loss = 0.18541816078954274
Trained batch 225 in epoch 0, gen_loss = 1.0687748839897393, disc_loss = 0.18511191079706218
Trained batch 226 in epoch 0, gen_loss = 1.0680273469324153, disc_loss = 0.18563806843258737
Trained batch 227 in epoch 0, gen_loss = 1.0676827307855874, disc_loss = 0.18556225721381212
Trained batch 228 in epoch 0, gen_loss = 1.0676093187394622, disc_loss = 0.1850767016215616
Trained batch 229 in epoch 0, gen_loss = 1.0680897036324377, disc_loss = 0.18445298182575598
Trained batch 230 in epoch 0, gen_loss = 1.067471652319937, disc_loss = 0.1838718740777536
Trained batch 231 in epoch 0, gen_loss = 1.0683647985088414, disc_loss = 0.183415785046487
Trained batch 232 in epoch 0, gen_loss = 1.0686079059035993, disc_loss = 0.18276536824429496
Trained batch 233 in epoch 0, gen_loss = 1.068272210848637, disc_loss = 0.18226177501691204
Trained batch 234 in epoch 0, gen_loss = 1.0699921711962275, disc_loss = 0.1822628860302428
Trained batch 235 in epoch 0, gen_loss = 1.0685419771125761, disc_loss = 0.18227217253297567
Trained batch 236 in epoch 0, gen_loss = 1.0711977967230077, disc_loss = 0.1818191656363413
Trained batch 237 in epoch 0, gen_loss = 1.0702315746736126, disc_loss = 0.18151959415306063
Trained batch 238 in epoch 0, gen_loss = 1.0721432217494213, disc_loss = 0.18161916484954965
Trained batch 239 in epoch 0, gen_loss = 1.0707603288193543, disc_loss = 0.18131710515978436
Trained batch 240 in epoch 0, gen_loss = 1.0692511240476394, disc_loss = 0.18158850699848653
Trained batch 241 in epoch 0, gen_loss = 1.0720911984108696, disc_loss = 0.18252074553762093
Trained batch 242 in epoch 0, gen_loss = 1.0710750271263436, disc_loss = 0.182463709908871
Trained batch 243 in epoch 0, gen_loss = 1.069334266127133, disc_loss = 0.18258721498986247
Trained batch 244 in epoch 0, gen_loss = 1.0687114416336527, disc_loss = 0.18230053656259362
Trained batch 245 in epoch 0, gen_loss = 1.0693003852677538, disc_loss = 0.18207288541020902
Trained batch 246 in epoch 0, gen_loss = 1.0697485770773791, disc_loss = 0.18155265302370918
Trained batch 247 in epoch 0, gen_loss = 1.0687650105645579, disc_loss = 0.18118667264559096
Trained batch 248 in epoch 0, gen_loss = 1.0680798727824505, disc_loss = 0.18078431653150592
Trained batch 249 in epoch 0, gen_loss = 1.0697038021087646, disc_loss = 0.1809234456270933
Trained batch 250 in epoch 0, gen_loss = 1.0677496135472302, disc_loss = 0.18095747501667278
Trained batch 251 in epoch 0, gen_loss = 1.0675469047966457, disc_loss = 0.18068219459659995
Trained batch 252 in epoch 0, gen_loss = 1.0698285939193997, disc_loss = 0.1805167084361018
Trained batch 253 in epoch 0, gen_loss = 1.0688071961947314, disc_loss = 0.1803416775055523
Trained batch 254 in epoch 0, gen_loss = 1.0684638287506851, disc_loss = 0.17985488747557005
Trained batch 255 in epoch 0, gen_loss = 1.0702824017498642, disc_loss = 0.17934518885158468
Trained batch 256 in epoch 0, gen_loss = 1.0714002404695355, disc_loss = 0.1787248813920448
Trained batch 257 in epoch 0, gen_loss = 1.0711633981198303, disc_loss = 0.17830384031746738
Trained batch 258 in epoch 0, gen_loss = 1.0707237280934014, disc_loss = 0.17798117108096487
Trained batch 259 in epoch 0, gen_loss = 1.0724039336809745, disc_loss = 0.17820077567146375
Trained batch 260 in epoch 0, gen_loss = 1.0702711092557944, disc_loss = 0.17871723852166727
Trained batch 261 in epoch 0, gen_loss = 1.0699033348159936, disc_loss = 0.17882139628170102
Trained batch 262 in epoch 0, gen_loss = 1.070067071416079, disc_loss = 0.17863305145343447
Trained batch 263 in epoch 0, gen_loss = 1.0715083206693332, disc_loss = 0.1781526462203174
Trained batch 264 in epoch 0, gen_loss = 1.0714507892446699, disc_loss = 0.17766711836997068
Trained batch 265 in epoch 0, gen_loss = 1.0711139648928678, disc_loss = 0.17725150728304134
Trained batch 266 in epoch 0, gen_loss = 1.0740370399943004, disc_loss = 0.17688151606627173
Trained batch 267 in epoch 0, gen_loss = 1.074501807787525, disc_loss = 0.17651670216235207
Trained batch 268 in epoch 0, gen_loss = 1.0721334645517697, disc_loss = 0.17713307604526055
Trained batch 269 in epoch 0, gen_loss = 1.0736562937498093, disc_loss = 0.17701760421472568
Trained batch 270 in epoch 0, gen_loss = 1.0721047941829005, disc_loss = 0.17696718636529032
Trained batch 271 in epoch 0, gen_loss = 1.0717411995591486, disc_loss = 0.176706583088483
Trained batch 272 in epoch 0, gen_loss = 1.0714265753279675, disc_loss = 0.17641817023738837
Trained batch 273 in epoch 0, gen_loss = 1.0708780516020573, disc_loss = 0.1760726897793747
Trained batch 274 in epoch 0, gen_loss = 1.0728471627018668, disc_loss = 0.17560375494035807
Trained batch 275 in epoch 0, gen_loss = 1.0737774278158727, disc_loss = 0.17505686535783435
Trained batch 276 in epoch 0, gen_loss = 1.0736283822395312, disc_loss = 0.17456939898511994
Trained batch 277 in epoch 0, gen_loss = 1.0747708108999747, disc_loss = 0.1740339322136246
Trained batch 278 in epoch 0, gen_loss = 1.0750622152213982, disc_loss = 0.17355684241727262
Trained batch 279 in epoch 0, gen_loss = 1.0770637317427567, disc_loss = 0.17344277117933546
Trained batch 280 in epoch 0, gen_loss = 1.0767324048217082, disc_loss = 0.1731716991266322
Trained batch 281 in epoch 0, gen_loss = 1.0751463416831712, disc_loss = 0.173277043149615
Trained batch 282 in epoch 0, gen_loss = 1.0775237615243285, disc_loss = 0.17387001566562552
Trained batch 283 in epoch 0, gen_loss = 1.0786897029045601, disc_loss = 0.17344307732886413
Trained batch 284 in epoch 0, gen_loss = 1.0775923667246836, disc_loss = 0.1731986260858544
Trained batch 285 in epoch 0, gen_loss = 1.0773491597884184, disc_loss = 0.1727587386210273
Trained batch 286 in epoch 0, gen_loss = 1.0784605393634026, disc_loss = 0.1723147779054135
Trained batch 287 in epoch 0, gen_loss = 1.0795956602733996, disc_loss = 0.171870889271506
Trained batch 288 in epoch 0, gen_loss = 1.080172476162135, disc_loss = 0.17140742312717192
Trained batch 289 in epoch 0, gen_loss = 1.0787052603631184, disc_loss = 0.17128477961081884
Trained batch 290 in epoch 0, gen_loss = 1.0803795972026091, disc_loss = 0.17151880173390263
Trained batch 291 in epoch 0, gen_loss = 1.0812751504860512, disc_loss = 0.1710372814232458
Trained batch 292 in epoch 0, gen_loss = 1.079776411272153, disc_loss = 0.1713551350077773
Trained batch 293 in epoch 0, gen_loss = 1.0807609425312807, disc_loss = 0.1715841316206216
Trained batch 294 in epoch 0, gen_loss = 1.0811677497322276, disc_loss = 0.1715030305024426
Trained batch 295 in epoch 0, gen_loss = 1.0794891522744217, disc_loss = 0.17182775884489151
Trained batch 296 in epoch 0, gen_loss = 1.0785242765640168, disc_loss = 0.17167767667233544
Trained batch 297 in epoch 0, gen_loss = 1.0793562616477876, disc_loss = 0.17222043557480077
Trained batch 298 in epoch 0, gen_loss = 1.0783336191472401, disc_loss = 0.17231037505718577
Trained batch 299 in epoch 0, gen_loss = 1.0771809169650077, disc_loss = 0.17238654132311543
Trained batch 300 in epoch 0, gen_loss = 1.0783008417219815, disc_loss = 0.17222842277705272
Trained batch 301 in epoch 0, gen_loss = 1.0774340934508684, disc_loss = 0.17212912766571273
Trained batch 302 in epoch 0, gen_loss = 1.076658688067603, disc_loss = 0.17197633149231425
Trained batch 303 in epoch 0, gen_loss = 1.0765910243713541, disc_loss = 0.17171275675125225
Trained batch 304 in epoch 0, gen_loss = 1.0754797045324669, disc_loss = 0.17170277815006796
Trained batch 305 in epoch 0, gen_loss = 1.075402838829296, disc_loss = 0.17149944438483394
Trained batch 306 in epoch 0, gen_loss = 1.07508395054053, disc_loss = 0.17118021297945069
Trained batch 307 in epoch 0, gen_loss = 1.0735284870901665, disc_loss = 0.1711855798342882
Trained batch 308 in epoch 0, gen_loss = 1.0742853338857299, disc_loss = 0.17145742399431163
Trained batch 309 in epoch 0, gen_loss = 1.0737128652872578, disc_loss = 0.17118302702062552
Trained batch 310 in epoch 0, gen_loss = 1.0742380280778339, disc_loss = 0.17078535812772164
Trained batch 311 in epoch 0, gen_loss = 1.0734902846698577, disc_loss = 0.17060183168341142
Trained batch 312 in epoch 0, gen_loss = 1.073941625821324, disc_loss = 0.1701967291022166
Trained batch 313 in epoch 0, gen_loss = 1.0730580069646714, disc_loss = 0.1700059610602867
Trained batch 314 in epoch 0, gen_loss = 1.073988943345963, disc_loss = 0.17021834949061038
Trained batch 315 in epoch 0, gen_loss = 1.0724657310903827, disc_loss = 0.17043911085075028
Trained batch 316 in epoch 0, gen_loss = 1.0730462101543738, disc_loss = 0.16999366440537036
Trained batch 317 in epoch 0, gen_loss = 1.0745116911781658, disc_loss = 0.16964121985852532
Trained batch 318 in epoch 0, gen_loss = 1.0746438104344012, disc_loss = 0.16954763549552254
Trained batch 319 in epoch 0, gen_loss = 1.0736399107612669, disc_loss = 0.1694136596575845
Trained batch 320 in epoch 0, gen_loss = 1.0744390993660484, disc_loss = 0.16894409158542706
Trained batch 321 in epoch 0, gen_loss = 1.0737483742999734, disc_loss = 0.16868811443748866
Trained batch 322 in epoch 0, gen_loss = 1.0751316190319533, disc_loss = 0.16851065409926252
Trained batch 323 in epoch 0, gen_loss = 1.0742302681376905, disc_loss = 0.1682756697517578
Trained batch 324 in epoch 0, gen_loss = 1.0740615731936234, disc_loss = 0.16796402702537866
Trained batch 325 in epoch 0, gen_loss = 1.0739909064001831, disc_loss = 0.16787192934518394
Trained batch 326 in epoch 0, gen_loss = 1.0726243528204227, disc_loss = 0.1680290949237547
Trained batch 327 in epoch 0, gen_loss = 1.0728345041776575, disc_loss = 0.1679783701135709
Trained batch 328 in epoch 0, gen_loss = 1.075115587301892, disc_loss = 0.1678464657324154
Trained batch 329 in epoch 0, gen_loss = 1.0759898630958615, disc_loss = 0.16742717861903436
Trained batch 330 in epoch 0, gen_loss = 1.0743537344421146, disc_loss = 0.1678167695342774
Trained batch 331 in epoch 0, gen_loss = 1.0760667323168502, disc_loss = 0.1675149708963841
Trained batch 332 in epoch 0, gen_loss = 1.0771176441832706, disc_loss = 0.16711476504489467
Trained batch 333 in epoch 0, gen_loss = 1.0780574228770718, disc_loss = 0.16669437264700135
Trained batch 334 in epoch 0, gen_loss = 1.078906713581797, disc_loss = 0.16628275176148807
Trained batch 335 in epoch 0, gen_loss = 1.07859444893187, disc_loss = 0.16591635072005115
Trained batch 336 in epoch 0, gen_loss = 1.0799287921066454, disc_loss = 0.16553241383140624
Trained batch 337 in epoch 0, gen_loss = 1.0796855362738378, disc_loss = 0.1652139951612734
Trained batch 338 in epoch 0, gen_loss = 1.081023403940651, disc_loss = 0.1647777025777418
Trained batch 339 in epoch 0, gen_loss = 1.080516918880098, disc_loss = 0.16444205142667187
Trained batch 340 in epoch 0, gen_loss = 1.0806124233954812, disc_loss = 0.16420219673111466
Trained batch 341 in epoch 0, gen_loss = 1.0810849927147927, disc_loss = 0.16399744556595883
Trained batch 342 in epoch 0, gen_loss = 1.0818513999353692, disc_loss = 0.16394080402487046
Trained batch 343 in epoch 0, gen_loss = 1.0801384272270425, disc_loss = 0.16448143395855155
Trained batch 344 in epoch 0, gen_loss = 1.0815750982450403, disc_loss = 0.16417328765534836
Trained batch 345 in epoch 0, gen_loss = 1.0825138770775988, disc_loss = 0.1640966067607434
Trained batch 346 in epoch 0, gen_loss = 1.0835734385921907, disc_loss = 0.16379750098614423
Trained batch 347 in epoch 0, gen_loss = 1.08304811859953, disc_loss = 0.16359710086929216
Trained batch 348 in epoch 0, gen_loss = 1.0841999583394617, disc_loss = 0.16323669721775377
Trained batch 349 in epoch 0, gen_loss = 1.0870236611366273, disc_loss = 0.16310502083706005
Trained batch 350 in epoch 0, gen_loss = 1.0874338995697153, disc_loss = 0.1627228686379062
Trained batch 351 in epoch 0, gen_loss = 1.0871770883148366, disc_loss = 0.16241661538581617
Trained batch 352 in epoch 0, gen_loss = 1.0878974536998414, disc_loss = 0.1620191102135114
Trained batch 353 in epoch 0, gen_loss = 1.0897358699706987, disc_loss = 0.16166822823806334
Trained batch 354 in epoch 0, gen_loss = 1.0909391195001736, disc_loss = 0.16125102158058696
Trained batch 355 in epoch 0, gen_loss = 1.091561146331637, disc_loss = 0.1608416393446328
Trained batch 356 in epoch 0, gen_loss = 1.092164262670095, disc_loss = 0.16043751595766187
Trained batch 357 in epoch 0, gen_loss = 1.093021750783121, disc_loss = 0.1600217968745771
Trained batch 358 in epoch 0, gen_loss = 1.0936777671399556, disc_loss = 0.15960679515615148
Trained batch 359 in epoch 0, gen_loss = 1.09410633345445, disc_loss = 0.159204834797937
Trained batch 360 in epoch 0, gen_loss = 1.095304038716155, disc_loss = 0.15883035801474904
Trained batch 361 in epoch 0, gen_loss = 1.0950104095329896, disc_loss = 0.15860048116014971
Trained batch 362 in epoch 0, gen_loss = 1.095487468006197, disc_loss = 0.15826032023887168
Trained batch 363 in epoch 0, gen_loss = 1.094986842720063, disc_loss = 0.158027534941109
Trained batch 364 in epoch 0, gen_loss = 1.0948268490294888, disc_loss = 0.15786227600725547
Trained batch 365 in epoch 0, gen_loss = 1.0944807993583991, disc_loss = 0.1576866996550902
Trained batch 366 in epoch 0, gen_loss = 1.0955744954155966, disc_loss = 0.15729956564364056
Trained batch 367 in epoch 0, gen_loss = 1.0989353924017886, disc_loss = 0.15713161683600882
Trained batch 368 in epoch 0, gen_loss = 1.1015875140825908, disc_loss = 0.1568671466535346
Trained batch 369 in epoch 0, gen_loss = 1.1030414434703621, disc_loss = 0.15649011948120756
Trained batch 370 in epoch 0, gen_loss = 1.1038136638078406, disc_loss = 0.15611921392903336
Trained batch 371 in epoch 0, gen_loss = 1.1043239852754019, disc_loss = 0.15575932346845187
Trained batch 372 in epoch 0, gen_loss = 1.1059161841709557, disc_loss = 0.15538252125326893
Trained batch 373 in epoch 0, gen_loss = 1.1072559539965767, disc_loss = 0.155017986746734
Trained batch 374 in epoch 0, gen_loss = 1.108485732237498, disc_loss = 0.15465647362917662
Trained batch 375 in epoch 0, gen_loss = 1.1094828855167045, disc_loss = 0.15431181971786267
Trained batch 376 in epoch 0, gen_loss = 1.1111230684212094, disc_loss = 0.15393835877806977
Trained batch 377 in epoch 0, gen_loss = 1.1121729919834742, disc_loss = 0.15356842755640626
Trained batch 378 in epoch 0, gen_loss = 1.113407597220982, disc_loss = 0.15322115544062648
Trained batch 379 in epoch 0, gen_loss = 1.1141413373382467, disc_loss = 0.15286189691702787
Trained batch 380 in epoch 0, gen_loss = 1.114546907386129, disc_loss = 0.15251713105308728
Trained batch 381 in epoch 0, gen_loss = 1.1156995656914737, disc_loss = 0.152144361794463
Trained batch 382 in epoch 0, gen_loss = 1.1162031924755704, disc_loss = 0.15180492798941253
Trained batch 383 in epoch 0, gen_loss = 1.117391692629705, disc_loss = 0.15144041232027425
Trained batch 384 in epoch 0, gen_loss = 1.1184873282135308, disc_loss = 0.1510860602915674
Trained batch 385 in epoch 0, gen_loss = 1.1176090396438856, disc_loss = 0.15098167988206285
Trained batch 386 in epoch 0, gen_loss = 1.1186813672075593, disc_loss = 0.1508528367791912
Trained batch 387 in epoch 0, gen_loss = 1.1173943169030947, disc_loss = 0.1508774002485899
Trained batch 388 in epoch 0, gen_loss = 1.118035635329151, disc_loss = 0.15080356725747757
Trained batch 389 in epoch 0, gen_loss = 1.1162185245599503, disc_loss = 0.15114751073698968
Trained batch 390 in epoch 0, gen_loss = 1.1167595089244111, disc_loss = 0.15084386245845377
Trained batch 391 in epoch 0, gen_loss = 1.1183195857673276, disc_loss = 0.1507312630597806
Trained batch 392 in epoch 0, gen_loss = 1.1178836843742972, disc_loss = 0.15049493878264616
Trained batch 393 in epoch 0, gen_loss = 1.1172650908758193, disc_loss = 0.15031925948610916
Trained batch 394 in epoch 0, gen_loss = 1.1177505965474286, disc_loss = 0.1508458896575472
Trained batch 395 in epoch 0, gen_loss = 1.116119101944596, disc_loss = 0.1511521459876964
Trained batch 396 in epoch 0, gen_loss = 1.1159768892775854, disc_loss = 0.15118093710270727
Trained batch 397 in epoch 0, gen_loss = 1.1154103678974074, disc_loss = 0.1510426872012079
Trained batch 398 in epoch 0, gen_loss = 1.1141677395741743, disc_loss = 0.15117002762667367
Trained batch 399 in epoch 0, gen_loss = 1.114539910852909, disc_loss = 0.15112783033866437
Trained batch 400 in epoch 0, gen_loss = 1.1133600651771944, disc_loss = 0.15125209736426423
Trained batch 401 in epoch 0, gen_loss = 1.1127655750779963, disc_loss = 0.15109093439538235
Trained batch 402 in epoch 0, gen_loss = 1.1131204495359, disc_loss = 0.15114048523158205
Trained batch 403 in epoch 0, gen_loss = 1.1126157014381768, disc_loss = 0.15113931411582202
Trained batch 404 in epoch 0, gen_loss = 1.112078238563773, disc_loss = 0.1509028388624206
Trained batch 405 in epoch 0, gen_loss = 1.1112527945652384, disc_loss = 0.15081914248669823
Trained batch 406 in epoch 0, gen_loss = 1.1109404607838436, disc_loss = 0.1512297536372333
Trained batch 407 in epoch 0, gen_loss = 1.1101419315034269, disc_loss = 0.15113158203095345
Trained batch 408 in epoch 0, gen_loss = 1.110080570055395, disc_loss = 0.1511714575825883
Trained batch 409 in epoch 0, gen_loss = 1.10934322941594, disc_loss = 0.15110242930581658
Trained batch 410 in epoch 0, gen_loss = 1.1093027636662598, disc_loss = 0.15099619610411844
Trained batch 411 in epoch 0, gen_loss = 1.110540470885999, disc_loss = 0.1510310423180678
Trained batch 412 in epoch 0, gen_loss = 1.108932387453592, disc_loss = 0.1516880349596606
Trained batch 413 in epoch 0, gen_loss = 1.1078625027684197, disc_loss = 0.15174769684402406
Trained batch 414 in epoch 0, gen_loss = 1.108410857958966, disc_loss = 0.15180238677884442
Trained batch 415 in epoch 0, gen_loss = 1.1091695341926355, disc_loss = 0.15163214857994506
Trained batch 416 in epoch 0, gen_loss = 1.107913012413098, disc_loss = 0.1517704833763466
Trained batch 417 in epoch 0, gen_loss = 1.1075702070049123, disc_loss = 0.15158746500708983
Trained batch 418 in epoch 0, gen_loss = 1.1082093004395115, disc_loss = 0.15164808504425684
Trained batch 419 in epoch 0, gen_loss = 1.1074156647636777, disc_loss = 0.15152774850970932
Trained batch 420 in epoch 0, gen_loss = 1.1071056958615355, disc_loss = 0.15148512375344148
Trained batch 421 in epoch 0, gen_loss = 1.1061344946165221, disc_loss = 0.15150442592782007
Trained batch 422 in epoch 0, gen_loss = 1.1057406155211988, disc_loss = 0.1513452194763202
Trained batch 423 in epoch 0, gen_loss = 1.10603815781058, disc_loss = 0.15131483211638933
Trained batch 424 in epoch 0, gen_loss = 1.1051000902232002, disc_loss = 0.15129948558614534
Trained batch 425 in epoch 0, gen_loss = 1.1049383096571819, disc_loss = 0.15118900155255072
Trained batch 426 in epoch 0, gen_loss = 1.1050411321798588, disc_loss = 0.15121524664518846
Trained batch 427 in epoch 0, gen_loss = 1.1050947410202472, disc_loss = 0.15097321411942907
Trained batch 428 in epoch 0, gen_loss = 1.1040772281604492, disc_loss = 0.15106009904269155
Trained batch 429 in epoch 0, gen_loss = 1.1046943263952123, disc_loss = 0.15101930716096662
Trained batch 430 in epoch 0, gen_loss = 1.1036006526161513, disc_loss = 0.15138478084897358
Trained batch 431 in epoch 0, gen_loss = 1.1030593576530616, disc_loss = 0.1513342060778221
Trained batch 432 in epoch 0, gen_loss = 1.1034374373339064, disc_loss = 0.1516236892963736
Trained batch 433 in epoch 0, gen_loss = 1.1023586256438136, disc_loss = 0.15169328659309358
Trained batch 434 in epoch 0, gen_loss = 1.1015608750540635, disc_loss = 0.15175793101133286
Trained batch 435 in epoch 0, gen_loss = 1.1012485806548267, disc_loss = 0.1517351865640269
Trained batch 436 in epoch 0, gen_loss = 1.1010526767857287, disc_loss = 0.15174110290472922
Trained batch 437 in epoch 0, gen_loss = 1.1007159314199126, disc_loss = 0.15168294455583917
Trained batch 438 in epoch 0, gen_loss = 1.1001014852035018, disc_loss = 0.15157527850351057
Trained batch 439 in epoch 0, gen_loss = 1.1001530887051063, disc_loss = 0.1515021906530654
Trained batch 440 in epoch 0, gen_loss = 1.0992736379846153, disc_loss = 0.15142016189253113
Trained batch 441 in epoch 0, gen_loss = 1.0992830356591428, disc_loss = 0.1515104042332434
Trained batch 442 in epoch 0, gen_loss = 1.0984106102861615, disc_loss = 0.15144155097549306
Trained batch 443 in epoch 0, gen_loss = 1.098332153381528, disc_loss = 0.15123946649685233
Trained batch 444 in epoch 0, gen_loss = 1.0986329814021507, disc_loss = 0.1518335454961222
Trained batch 445 in epoch 0, gen_loss = 1.0971774758913058, disc_loss = 0.15247183584194682
Trained batch 446 in epoch 0, gen_loss = 1.0961272606513643, disc_loss = 0.15254572047633214
Trained batch 447 in epoch 0, gen_loss = 1.0965979351396007, disc_loss = 0.1527058091721431
Trained batch 448 in epoch 0, gen_loss = 1.0962057456805605, disc_loss = 0.1526421934308148
Trained batch 449 in epoch 0, gen_loss = 1.0956702062156465, disc_loss = 0.15255445806930462
Trained batch 450 in epoch 0, gen_loss = 1.0948950170545515, disc_loss = 0.1524767366066542
Trained batch 451 in epoch 0, gen_loss = 1.0943065176891014, disc_loss = 0.15246105070876997
Trained batch 452 in epoch 0, gen_loss = 1.0941406414077246, disc_loss = 0.15254454050320004
Trained batch 453 in epoch 0, gen_loss = 1.0939573327218908, disc_loss = 0.15248891831445405
Trained batch 454 in epoch 0, gen_loss = 1.0933266631194523, disc_loss = 0.15239933035285264
Trained batch 455 in epoch 0, gen_loss = 1.093722151232916, disc_loss = 0.15247183107248133
Trained batch 456 in epoch 0, gen_loss = 1.0932588131865997, disc_loss = 0.1523063676370638
Trained batch 457 in epoch 0, gen_loss = 1.0928223464296374, disc_loss = 0.1521925821973572
Trained batch 458 in epoch 0, gen_loss = 1.0927695672886044, disc_loss = 0.15207670095390874
Trained batch 459 in epoch 0, gen_loss = 1.0924706937826199, disc_loss = 0.151913840862234
Trained batch 460 in epoch 0, gen_loss = 1.092865049903667, disc_loss = 0.15188596837642782
Trained batch 461 in epoch 0, gen_loss = 1.092131857941677, disc_loss = 0.15201799373557687
Trained batch 462 in epoch 0, gen_loss = 1.091286159939179, disc_loss = 0.1520333696784327
Trained batch 463 in epoch 0, gen_loss = 1.0921733589393312, disc_loss = 0.15213408262131672
Trained batch 464 in epoch 0, gen_loss = 1.0921434022406096, disc_loss = 0.15193600652599207
Trained batch 465 in epoch 0, gen_loss = 1.0912298165549537, disc_loss = 0.15197519354857983
Trained batch 466 in epoch 0, gen_loss = 1.0913868769205612, disc_loss = 0.1518168468985555
Trained batch 467 in epoch 0, gen_loss = 1.0915526800558097, disc_loss = 0.15174252867428029
Trained batch 468 in epoch 0, gen_loss = 1.0912428155763825, disc_loss = 0.15160357560922724
Trained batch 469 in epoch 0, gen_loss = 1.0909609816809918, disc_loss = 0.15139121164746108
Trained batch 470 in epoch 0, gen_loss = 1.0913682486980585, disc_loss = 0.15122322961608947
Trained batch 471 in epoch 0, gen_loss = 1.0913037346455001, disc_loss = 0.1510139106717592
Trained batch 472 in epoch 0, gen_loss = 1.0910764514895877, disc_loss = 0.1508457722382553
Trained batch 473 in epoch 0, gen_loss = 1.0911525760144625, disc_loss = 0.15063239541690687
Trained batch 474 in epoch 0, gen_loss = 1.0917148299593675, disc_loss = 0.15043248797325712
Trained batch 475 in epoch 0, gen_loss = 1.0916845268186401, disc_loss = 0.15020105443472245
Trained batch 476 in epoch 0, gen_loss = 1.092430636130539, disc_loss = 0.1499374417521731
Trained batch 477 in epoch 0, gen_loss = 1.0925441085668788, disc_loss = 0.14969659202720456
Trained batch 478 in epoch 0, gen_loss = 1.092543990042115, disc_loss = 0.14949347391055745
Trained batch 479 in epoch 0, gen_loss = 1.09406283951054, disc_loss = 0.14946617988171057
Trained batch 480 in epoch 0, gen_loss = 1.0931790757947553, disc_loss = 0.14947857580335497
Trained batch 481 in epoch 0, gen_loss = 1.0928269499194079, disc_loss = 0.1493345384149816
Trained batch 482 in epoch 0, gen_loss = 1.0942147716602184, disc_loss = 0.14964492536896135
Trained batch 483 in epoch 0, gen_loss = 1.0934474258260294, disc_loss = 0.14962006447652032
Trained batch 484 in epoch 0, gen_loss = 1.0930582200743488, disc_loss = 0.14952191837592838
Trained batch 485 in epoch 0, gen_loss = 1.0929659261011784, disc_loss = 0.1493834579946021
Trained batch 486 in epoch 0, gen_loss = 1.0926143700214872, disc_loss = 0.14922211165836338
Trained batch 487 in epoch 0, gen_loss = 1.0921186021239053, disc_loss = 0.14913832731773985
Trained batch 488 in epoch 0, gen_loss = 1.0922114008652897, disc_loss = 0.1491639817015464
Trained batch 489 in epoch 0, gen_loss = 1.0921882892749748, disc_loss = 0.148970503371437
Trained batch 490 in epoch 0, gen_loss = 1.0919429894741586, disc_loss = 0.14880469935716656
Trained batch 491 in epoch 0, gen_loss = 1.0928447879547996, disc_loss = 0.14862298147328865
Trained batch 492 in epoch 0, gen_loss = 1.0924310522800043, disc_loss = 0.14844829635600168
Trained batch 493 in epoch 0, gen_loss = 1.0928075483091446, disc_loss = 0.14824768948757094
Trained batch 494 in epoch 0, gen_loss = 1.0922527986945527, disc_loss = 0.1481013913460151
Trained batch 495 in epoch 0, gen_loss = 1.0931769863612228, disc_loss = 0.14803028086786188
Trained batch 496 in epoch 0, gen_loss = 1.0926442572648376, disc_loss = 0.14798777160315327
Trained batch 497 in epoch 0, gen_loss = 1.0933748444759224, disc_loss = 0.14799276357479244
Trained batch 498 in epoch 0, gen_loss = 1.0934705590317866, disc_loss = 0.14779461166617985
Trained batch 499 in epoch 0, gen_loss = 1.0930622424483298, disc_loss = 0.14769989525154234
Trained batch 500 in epoch 0, gen_loss = 1.094167856339685, disc_loss = 0.14780203068996975
Trained batch 501 in epoch 0, gen_loss = 1.0935204710024762, disc_loss = 0.14789746667952294
Trained batch 502 in epoch 0, gen_loss = 1.0933985370646415, disc_loss = 0.14773409286695374
Trained batch 503 in epoch 0, gen_loss = 1.0928465658355326, disc_loss = 0.1476870845035014
Trained batch 504 in epoch 0, gen_loss = 1.0931255596108955, disc_loss = 0.14755615943507983
Trained batch 505 in epoch 0, gen_loss = 1.0934778697523675, disc_loss = 0.1474623280529507
Trained batch 506 in epoch 0, gen_loss = 1.092281966696124, disc_loss = 0.147605594185124
Trained batch 507 in epoch 0, gen_loss = 1.09259561073827, disc_loss = 0.1476620280579847
Trained batch 508 in epoch 0, gen_loss = 1.0922545827559262, disc_loss = 0.14749978647031226
Trained batch 509 in epoch 0, gen_loss = 1.0919177913782643, disc_loss = 0.14737669986617916
Trained batch 510 in epoch 0, gen_loss = 1.0921291771350308, disc_loss = 0.1473283219805494
Trained batch 511 in epoch 0, gen_loss = 1.0920459470362402, disc_loss = 0.14714253913189168
Trained batch 512 in epoch 0, gen_loss = 1.0918668462635248, disc_loss = 0.14700944339366337
Trained batch 513 in epoch 0, gen_loss = 1.0927694477575762, disc_loss = 0.1468072052907329
Trained batch 514 in epoch 0, gen_loss = 1.092748049682784, disc_loss = 0.146610442400702
Trained batch 515 in epoch 0, gen_loss = 1.0930463924079903, disc_loss = 0.1464063343447597
Trained batch 516 in epoch 0, gen_loss = 1.0936707203462, disc_loss = 0.14619692563073894
Trained batch 517 in epoch 0, gen_loss = 1.0927488703180004, disc_loss = 0.14629756051931947
Trained batch 518 in epoch 0, gen_loss = 1.0939896392110227, disc_loss = 0.14626382506292795
Trained batch 519 in epoch 0, gen_loss = 1.0941312446617164, disc_loss = 0.14610526621556627
Trained batch 520 in epoch 0, gen_loss = 1.0939321729020286, disc_loss = 0.14605089120795653
Trained batch 521 in epoch 0, gen_loss = 1.0933350309215744, disc_loss = 0.14603756436314505
Trained batch 522 in epoch 0, gen_loss = 1.0934555533397265, disc_loss = 0.14587411032651282
Trained batch 523 in epoch 0, gen_loss = 1.0942884538578623, disc_loss = 0.1457320688370587
Trained batch 524 in epoch 0, gen_loss = 1.0940733602501098, disc_loss = 0.14553953607522305
Trained batch 525 in epoch 0, gen_loss = 1.0941587744896857, disc_loss = 0.14532039575845343
Trained batch 526 in epoch 0, gen_loss = 1.0941214334693308, disc_loss = 0.14512081420229328
Trained batch 527 in epoch 0, gen_loss = 1.0946262857566278, disc_loss = 0.14494502205740323
Trained batch 528 in epoch 0, gen_loss = 1.0953100939441494, disc_loss = 0.1447248883151039
Trained batch 529 in epoch 0, gen_loss = 1.0948649322649218, disc_loss = 0.144640151080939
Trained batch 530 in epoch 0, gen_loss = 1.095257111132257, disc_loss = 0.14446213356659462
Trained batch 531 in epoch 0, gen_loss = 1.0948998004310113, disc_loss = 0.14443557735364465
Trained batch 532 in epoch 0, gen_loss = 1.0944232114819603, disc_loss = 0.1443487678149516
Trained batch 533 in epoch 0, gen_loss = 1.0952454675337795, disc_loss = 0.14426986571080452
Trained batch 534 in epoch 0, gen_loss = 1.0955564052702111, disc_loss = 0.1440426745077717
Trained batch 535 in epoch 0, gen_loss = 1.0954020483169093, disc_loss = 0.14385384620070013
Trained batch 536 in epoch 0, gen_loss = 1.0957149817179923, disc_loss = 0.1436358193660526
Trained batch 537 in epoch 0, gen_loss = 1.0962711950992563, disc_loss = 0.14344855262569003
Trained batch 538 in epoch 0, gen_loss = 1.0966824148824794, disc_loss = 0.14329880354701366
Trained batch 539 in epoch 0, gen_loss = 1.0978886484547898, disc_loss = 0.14324622910676732
Trained batch 540 in epoch 0, gen_loss = 1.097058966012186, disc_loss = 0.14365247636000572
Trained batch 541 in epoch 0, gen_loss = 1.0980730614868917, disc_loss = 0.14364893501108972
Trained batch 542 in epoch 0, gen_loss = 1.0982663549661198, disc_loss = 0.14373658932608246
Trained batch 543 in epoch 0, gen_loss = 1.0973660678000134, disc_loss = 0.14378180656486245
Trained batch 544 in epoch 0, gen_loss = 1.09691310153095, disc_loss = 0.1437949684269111
Trained batch 545 in epoch 0, gen_loss = 1.0979164590438206, disc_loss = 0.14378154922222153
Trained batch 546 in epoch 0, gen_loss = 1.0971476997383334, disc_loss = 0.14379772522454398
Trained batch 547 in epoch 0, gen_loss = 1.0981382514982327, disc_loss = 0.14363692975161177
Trained batch 548 in epoch 0, gen_loss = 1.098376454388509, disc_loss = 0.14346074083911592
Trained batch 549 in epoch 0, gen_loss = 1.098858102072369, disc_loss = 0.14327852099795232
Trained batch 550 in epoch 0, gen_loss = 1.0992806401854203, disc_loss = 0.1430867200585491
Trained batch 551 in epoch 0, gen_loss = 1.0994018286574578, disc_loss = 0.1428694260387641
Trained batch 552 in epoch 0, gen_loss = 1.0991183071520185, disc_loss = 0.14270051977652753
Trained batch 553 in epoch 0, gen_loss = 1.0997338775370527, disc_loss = 0.14255013868266495
Trained batch 554 in epoch 0, gen_loss = 1.0992862974738216, disc_loss = 0.14251558896120603
Trained batch 555 in epoch 0, gen_loss = 1.0991054925022365, disc_loss = 0.1424074778400308
Trained batch 556 in epoch 0, gen_loss = 1.0991470556391847, disc_loss = 0.1422931518358859
Trained batch 557 in epoch 0, gen_loss = 1.0987531230334313, disc_loss = 0.14217470048202407
Trained batch 558 in epoch 0, gen_loss = 1.0981744103973368, disc_loss = 0.1421681089536662
Trained batch 559 in epoch 0, gen_loss = 1.098508381471038, disc_loss = 0.14205632610246538
Trained batch 560 in epoch 0, gen_loss = 1.0990323595928424, disc_loss = 0.14191402796383634
Trained batch 561 in epoch 0, gen_loss = 1.098761878987224, disc_loss = 0.14178252934720167
Trained batch 562 in epoch 0, gen_loss = 1.0984531329220184, disc_loss = 0.14165625076734364
Trained batch 563 in epoch 0, gen_loss = 1.09976473466513, disc_loss = 0.1416636807645889
Trained batch 564 in epoch 0, gen_loss = 1.1002097907847008, disc_loss = 0.14150737640077035
Trained batch 565 in epoch 0, gen_loss = 1.100445825040551, disc_loss = 0.1413550659513937
Trained batch 566 in epoch 0, gen_loss = 1.0993640720844269, disc_loss = 0.1418161741088307
Trained batch 567 in epoch 0, gen_loss = 1.100370215435683, disc_loss = 0.141841722166979
Trained batch 568 in epoch 0, gen_loss = 1.1007873236295, disc_loss = 0.14181661955449198
Trained batch 569 in epoch 0, gen_loss = 1.100654481116094, disc_loss = 0.14175060784869026
Trained batch 570 in epoch 0, gen_loss = 1.1001543984626094, disc_loss = 0.1416456843816044
Trained batch 571 in epoch 0, gen_loss = 1.1006095274449228, disc_loss = 0.1415066674052106
Trained batch 572 in epoch 0, gen_loss = 1.1011039944947494, disc_loss = 0.14134972801929369
Trained batch 573 in epoch 0, gen_loss = 1.1004044019490584, disc_loss = 0.14136793296329858
Trained batch 574 in epoch 0, gen_loss = 1.1009185246281001, disc_loss = 0.14130960073807966
Trained batch 575 in epoch 0, gen_loss = 1.1004760664266844, disc_loss = 0.1411943175739402
Trained batch 576 in epoch 0, gen_loss = 1.1003738364375857, disc_loss = 0.14103618854304947
Trained batch 577 in epoch 0, gen_loss = 1.1014154656020003, disc_loss = 0.14089666769088965
Trained batch 578 in epoch 0, gen_loss = 1.1020239409902142, disc_loss = 0.1407173425152203
Trained batch 579 in epoch 0, gen_loss = 1.1020133500982976, disc_loss = 0.14053605732604346
Trained batch 580 in epoch 0, gen_loss = 1.1016645273017391, disc_loss = 0.14042279365987048
Trained batch 581 in epoch 0, gen_loss = 1.1025459870337622, disc_loss = 0.14022273619585635
Trained batch 582 in epoch 0, gen_loss = 1.10282523733265, disc_loss = 0.1400151126259238
Trained batch 583 in epoch 0, gen_loss = 1.1026138549491966, disc_loss = 0.13985942509854596
Trained batch 584 in epoch 0, gen_loss = 1.1026174187150777, disc_loss = 0.13971814742773517
Trained batch 585 in epoch 0, gen_loss = 1.1037813521184205, disc_loss = 0.13955470808041076
Trained batch 586 in epoch 0, gen_loss = 1.1037617800487571, disc_loss = 0.13937684659746172
Trained batch 587 in epoch 0, gen_loss = 1.1040100589597306, disc_loss = 0.1393885741954302
Trained batch 588 in epoch 0, gen_loss = 1.1046491912447536, disc_loss = 0.1392003310645072
Trained batch 589 in epoch 0, gen_loss = 1.1050098920777693, disc_loss = 0.13901962195286308
Trained batch 590 in epoch 0, gen_loss = 1.1050618038262208, disc_loss = 0.13884805353411966
Trained batch 591 in epoch 0, gen_loss = 1.1057392395529393, disc_loss = 0.13864283748852038
Trained batch 592 in epoch 0, gen_loss = 1.1064843557033732, disc_loss = 0.13847579433257623
Trained batch 593 in epoch 0, gen_loss = 1.1058540148064746, disc_loss = 0.13847746247870954
Trained batch 594 in epoch 0, gen_loss = 1.106222631200021, disc_loss = 0.1382860596878689
Trained batch 595 in epoch 0, gen_loss = 1.1066399581180324, disc_loss = 0.13811337524687484
Trained batch 596 in epoch 0, gen_loss = 1.108190964664047, disc_loss = 0.13798714416036054
Trained batch 597 in epoch 0, gen_loss = 1.1079512328706855, disc_loss = 0.13787813511455937
Trained batch 598 in epoch 0, gen_loss = 1.107828464900114, disc_loss = 0.13791606605724618
Testing Epoch 0
Traceback (most recent call last):
  File "srgan_bones.py", line 314, in <module>
    loss_G = loss_content + oss_GAN #loss_content + 1e-3 * loss_GAN
NameError: name 'oss_GAN' is not defined