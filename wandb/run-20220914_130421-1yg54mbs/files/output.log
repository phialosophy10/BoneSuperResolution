/work3/soeba/HALOS/halos/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.
  warnings.warn(
/work3/soeba/HALOS/halos/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Training Epoch 0
Trained batch 0 in epoch 0, gen_loss = 0.6126667857170105, disc_loss = 0.8946704268455505
Trained batch 1 in epoch 0, gen_loss = 0.6165765821933746, disc_loss = 1.0796160995960236
Trained batch 2 in epoch 0, gen_loss = 0.57352281610171, disc_loss = 0.8769269982973734
Trained batch 3 in epoch 0, gen_loss = 0.5432915166020393, disc_loss = 0.7661690264940262
Trained batch 4 in epoch 0, gen_loss = 0.5239859461784363, disc_loss = 0.6726466655731201
Trained batch 5 in epoch 0, gen_loss = 0.5159577429294586, disc_loss = 0.6107898950576782
Trained batch 6 in epoch 0, gen_loss = 0.5134948832648141, disc_loss = 0.5499946922063828
Trained batch 7 in epoch 0, gen_loss = 0.5063107758760452, disc_loss = 0.5015402007848024
Trained batch 8 in epoch 0, gen_loss = 0.49899179736773175, disc_loss = 0.46697817577256096
Trained batch 9 in epoch 0, gen_loss = 0.5004283100366592, disc_loss = 0.43918065577745435
Trained batch 10 in epoch 0, gen_loss = 0.49920306693423877, disc_loss = 0.4117831520058892
Trained batch 11 in epoch 0, gen_loss = 0.49550750603278476, disc_loss = 0.38711736847956973
Trained batch 12 in epoch 0, gen_loss = 0.4918303696008829, disc_loss = 0.36743377722226656
Trained batch 13 in epoch 0, gen_loss = 0.49323250353336334, disc_loss = 0.3499822994428022
Trained batch 14 in epoch 0, gen_loss = 0.4950087606906891, disc_loss = 0.3327244227131208
Trained batch 15 in epoch 0, gen_loss = 0.49441169761121273, disc_loss = 0.31599901616573334
Trained batch 16 in epoch 0, gen_loss = 0.49406106156461377, disc_loss = 0.30178273600690503
Trained batch 17 in epoch 0, gen_loss = 0.4953101161453459, disc_loss = 0.2885575054420365
Trained batch 18 in epoch 0, gen_loss = 0.4955833444469853, disc_loss = 0.27846892805475937
Trained batch 19 in epoch 0, gen_loss = 0.4954175755381584, disc_loss = 0.2701472230255604
Trained batch 20 in epoch 0, gen_loss = 0.49620787160737173, disc_loss = 0.260455601981708
Trained batch 21 in epoch 0, gen_loss = 0.49595434828238055, disc_loss = 0.2525833012028174
Trained batch 22 in epoch 0, gen_loss = 0.4977913783944171, disc_loss = 0.2459250403487164
Trained batch 23 in epoch 0, gen_loss = 0.4995393306016922, disc_loss = 0.23967345555623373
Trained batch 24 in epoch 0, gen_loss = 0.49937870502471926, disc_loss = 0.23453253090381623
Trained batch 25 in epoch 0, gen_loss = 0.49981271762114304, disc_loss = 0.2282708932000857
Trained batch 26 in epoch 0, gen_loss = 0.5004501122015493, disc_loss = 0.22270094824058037
Trained batch 27 in epoch 0, gen_loss = 0.4996112031596048, disc_loss = 0.21738614780562265
Trained batch 28 in epoch 0, gen_loss = 0.49808724378717356, disc_loss = 0.21187638208783907
Trained batch 29 in epoch 0, gen_loss = 0.4982346216837565, disc_loss = 0.20636059641838073
Trained batch 30 in epoch 0, gen_loss = 0.498721016991523, disc_loss = 0.20126478602328607
Trained batch 31 in epoch 0, gen_loss = 0.4991561472415924, disc_loss = 0.1972417434444651
Trained batch 32 in epoch 0, gen_loss = 0.501085821426276, disc_loss = 0.19393610017317714
Trained batch 33 in epoch 0, gen_loss = 0.5023407024495742, disc_loss = 0.19034728024374037
Trained batch 34 in epoch 0, gen_loss = 0.502069239956992, disc_loss = 0.18670973171080862
Trained batch 35 in epoch 0, gen_loss = 0.5028127117289437, disc_loss = 0.18269681475228733
Trained batch 36 in epoch 0, gen_loss = 0.5046666216205906, disc_loss = 0.17910897590824076
Trained batch 37 in epoch 0, gen_loss = 0.5058196155648482, disc_loss = 0.17627476371432604
Trained batch 38 in epoch 0, gen_loss = 0.5065429531610929, disc_loss = 0.17293433214609438
Trained batch 39 in epoch 0, gen_loss = 0.5078158006072044, disc_loss = 0.1695215728133917
Trained batch 40 in epoch 0, gen_loss = 0.510666169771334, disc_loss = 0.16634269585696662
Trained batch 41 in epoch 0, gen_loss = 0.512338997352691, disc_loss = 0.16310671845539695
Trained batch 42 in epoch 0, gen_loss = 0.5132205472436062, disc_loss = 0.1600741523556238
Trained batch 43 in epoch 0, gen_loss = 0.5134606402028691, disc_loss = 0.15718225834213875
Trained batch 44 in epoch 0, gen_loss = 0.5147304362720914, disc_loss = 0.15435607040094004
Trained batch 45 in epoch 0, gen_loss = 0.5151675200980642, disc_loss = 0.15155987891004138
Trained batch 46 in epoch 0, gen_loss = 0.5158880403701295, disc_loss = 0.1488937590509019
Trained batch 47 in epoch 0, gen_loss = 0.5165314773718516, disc_loss = 0.14629397067862251
Trained batch 48 in epoch 0, gen_loss = 0.5171445158063149, disc_loss = 0.1438070410216341
Trained batch 49 in epoch 0, gen_loss = 0.5178943252563477, disc_loss = 0.14142710261046887
Trained batch 50 in epoch 0, gen_loss = 0.518543608048383, disc_loss = 0.13911924912941223
Trained batch 51 in epoch 0, gen_loss = 0.5181563542439387, disc_loss = 0.13695591774124366
Trained batch 52 in epoch 0, gen_loss = 0.5183493337541256, disc_loss = 0.13478248425812092
Trained batch 53 in epoch 0, gen_loss = 0.5191194790380972, disc_loss = 0.13273174677871996
Trained batch 54 in epoch 0, gen_loss = 0.5185626252131028, disc_loss = 0.13075287873772057
Trained batch 55 in epoch 0, gen_loss = 0.5188228493290288, disc_loss = 0.12886753558580363
Trained batch 56 in epoch 0, gen_loss = 0.5190234450917495, disc_loss = 0.12713434615809666
Trained batch 57 in epoch 0, gen_loss = 0.518907403637623, disc_loss = 0.125520221664217
Trained batch 58 in epoch 0, gen_loss = 0.5198804660368774, disc_loss = 0.1239171669329122
Trained batch 59 in epoch 0, gen_loss = 0.519287056227525, disc_loss = 0.1223421600026389
Trained batch 60 in epoch 0, gen_loss = 0.5200982811998148, disc_loss = 0.12072642657478325
Trained batch 61 in epoch 0, gen_loss = 0.5202567524486973, disc_loss = 0.11907827304375748
Trained batch 62 in epoch 0, gen_loss = 0.5206092250725579, disc_loss = 0.11751304392422003
Trained batch 63 in epoch 0, gen_loss = 0.5205557248555124, disc_loss = 0.11598242507898249
Trained batch 64 in epoch 0, gen_loss = 0.5208937227725983, disc_loss = 0.1145896225881118
Trained batch 65 in epoch 0, gen_loss = 0.5213944600387053, disc_loss = 0.11327492406196667
Trained batch 66 in epoch 0, gen_loss = 0.5216732865838862, disc_loss = 0.11191820347709443
Trained batch 67 in epoch 0, gen_loss = 0.5220517266322585, disc_loss = 0.11062378885553163
Trained batch 68 in epoch 0, gen_loss = 0.5218150032603223, disc_loss = 0.10941239910713141
Trained batch 69 in epoch 0, gen_loss = 0.5220854746443885, disc_loss = 0.1082568070186036
Trained batch 70 in epoch 0, gen_loss = 0.5226294922996575, disc_loss = 0.10705811540845414
Trained batch 71 in epoch 0, gen_loss = 0.5226117533942064, disc_loss = 0.10600218021621306
Trained batch 72 in epoch 0, gen_loss = 0.5228459643174524, disc_loss = 0.10502770828874144
Trained batch 73 in epoch 0, gen_loss = 0.5225851547879141, disc_loss = 0.10390643455792924
Trained batch 74 in epoch 0, gen_loss = 0.5228550700346629, disc_loss = 0.10281826401750246
Trained batch 75 in epoch 0, gen_loss = 0.5227911766422423, disc_loss = 0.10174025791256051
Trained batch 76 in epoch 0, gen_loss = 0.5233014911026149, disc_loss = 0.10086854024753943
Trained batch 77 in epoch 0, gen_loss = 0.523824848807775, disc_loss = 0.10019455888332465
Trained batch 78 in epoch 0, gen_loss = 0.5232242477845542, disc_loss = 0.09961341561961777
Trained batch 79 in epoch 0, gen_loss = 0.523856058344245, disc_loss = 0.09896299592219293
Trained batch 80 in epoch 0, gen_loss = 0.5237884127799376, disc_loss = 0.09828342216802233
Trained batch 81 in epoch 0, gen_loss = 0.5241494807528286, disc_loss = 0.09746902504163545
Trained batch 82 in epoch 0, gen_loss = 0.5239808017230896, disc_loss = 0.09655166094202593
Trained batch 83 in epoch 0, gen_loss = 0.5238069129132089, disc_loss = 0.09559315558345545
Trained batch 84 in epoch 0, gen_loss = 0.5234849736971013, disc_loss = 0.09468550221884951
Trained batch 85 in epoch 0, gen_loss = 0.5242164540429448, disc_loss = 0.09383871022958395
Trained batch 86 in epoch 0, gen_loss = 0.5246937483891674, disc_loss = 0.09296981504336856
Trained batch 87 in epoch 0, gen_loss = 0.5248097564009103, disc_loss = 0.09213108084672554
Trained batch 88 in epoch 0, gen_loss = 0.524796342916703, disc_loss = 0.09138467788612575
Trained batch 89 in epoch 0, gen_loss = 0.5249696450101005, disc_loss = 0.09071424417197704
Trained batch 90 in epoch 0, gen_loss = 0.524810288306121, disc_loss = 0.09009076323319268
Trained batch 91 in epoch 0, gen_loss = 0.525083845400292, disc_loss = 0.08945217056442863
Trained batch 92 in epoch 0, gen_loss = 0.5251669765159648, disc_loss = 0.08871397101670823
Trained batch 93 in epoch 0, gen_loss = 0.5257963078453186, disc_loss = 0.08802958591742084
Trained batch 94 in epoch 0, gen_loss = 0.5258450417142165, disc_loss = 0.08851761659117122
Trained batch 95 in epoch 0, gen_loss = 0.5253377566114068, disc_loss = 0.09450006638265525
Trained batch 96 in epoch 0, gen_loss = 0.5249985542493997, disc_loss = 0.09498779151166223
Trained batch 97 in epoch 0, gen_loss = 0.5249579281223063, disc_loss = 0.09707191462951656
Trained batch 98 in epoch 0, gen_loss = 0.5254945514178035, disc_loss = 0.10110035596998652
Trained batch 99 in epoch 0, gen_loss = 0.5253713953495026, disc_loss = 0.10244047610089183
Trained batch 100 in epoch 0, gen_loss = 0.5247316071302583, disc_loss = 0.10470052666519539
Trained batch 101 in epoch 0, gen_loss = 0.5244629932384864, disc_loss = 0.1051899774511363
Trained batch 102 in epoch 0, gen_loss = 0.5242326224891884, disc_loss = 0.10641676142305426
Trained batch 103 in epoch 0, gen_loss = 0.5237548308303723, disc_loss = 0.10617150149594706
Trained batch 104 in epoch 0, gen_loss = 0.5234389012768155, disc_loss = 0.10558332302385852
Trained batch 105 in epoch 0, gen_loss = 0.5235724839961754, disc_loss = 0.10480025690048933
Trained batch 106 in epoch 0, gen_loss = 0.5236396714348659, disc_loss = 0.10399715482283418
Trained batch 107 in epoch 0, gen_loss = 0.5235998429082058, disc_loss = 0.10320258388916652
Trained batch 108 in epoch 0, gen_loss = 0.5233357756509693, disc_loss = 0.10240346627361184
Trained batch 109 in epoch 0, gen_loss = 0.5230658867142417, disc_loss = 0.10158433533188972
Trained batch 110 in epoch 0, gen_loss = 0.5228817210541116, disc_loss = 0.10078045669607483
Trained batch 111 in epoch 0, gen_loss = 0.5221283760453973, disc_loss = 0.10000396245491824
Trained batch 112 in epoch 0, gen_loss = 0.5217646861498335, disc_loss = 0.09925584228033513
Trained batch 113 in epoch 0, gen_loss = 0.5217224495452747, disc_loss = 0.0984951744934446
Testing Epoch 0
Traceback (most recent call last):
  File "srgan_bones.py", line 332, in <module>
    gen_hr = generator(imgs_lr)
  File "/work3/soeba/HALOS/halos/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "srgan_bones.py", line 198, in forward
    out = self.res_blocks(out1)
  File "/work3/soeba/HALOS/halos/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/work3/soeba/HALOS/halos/lib/python3.8/site-packages/torch/nn/modules/container.py", line 139, in forward
    input = module(input)
  File "/work3/soeba/HALOS/halos/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "srgan_bones.py", line 175, in forward
    return x + self.conv_block(x)
  File "/work3/soeba/HALOS/halos/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/work3/soeba/HALOS/halos/lib/python3.8/site-packages/torch/nn/modules/container.py", line 139, in forward
    input = module(input)
  File "/work3/soeba/HALOS/halos/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/work3/soeba/HALOS/halos/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 457, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/work3/soeba/HALOS/halos/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 453, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 31.75 GiB total capacity; 25.52 GiB already allocated; 103.69 MiB free; 30.49 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF