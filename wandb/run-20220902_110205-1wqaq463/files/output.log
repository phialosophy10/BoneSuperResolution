
wandb: WARNING Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.
/work3/soeba/HALOS/halos/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.
  warnings.warn(
/work3/soeba/HALOS/halos/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Training Epoch 0
Trained batch 0 in epoch 0, gen_loss = 1.4830409288406372, disc_loss = 0.5545046329498291
Trained batch 1 in epoch 0, gen_loss = 1.5166630744934082, disc_loss = 0.7380844950675964
Trained batch 2 in epoch 0, gen_loss = 1.396828333536784, disc_loss = 0.6603763898213705
Trained batch 3 in epoch 0, gen_loss = 1.322593778371811, disc_loss = 0.5795007348060608
Trained batch 4 in epoch 0, gen_loss = 1.2588672876358031, disc_loss = 0.5129628568887711
Trained batch 5 in epoch 0, gen_loss = 1.2216158111890156, disc_loss = 0.4602923318743706
Trained batch 6 in epoch 0, gen_loss = 1.2073795795440674, disc_loss = 0.41835046240261625
Trained batch 7 in epoch 0, gen_loss = 1.1932375878095627, disc_loss = 0.3841362800449133
Trained batch 8 in epoch 0, gen_loss = 1.1795041958491008, disc_loss = 0.3554171472787857
Trained batch 9 in epoch 0, gen_loss = 1.1658009171485901, disc_loss = 0.333105306327343
Trained batch 10 in epoch 0, gen_loss = 1.1612356901168823, disc_loss = 0.31744684278964996
Trained batch 11 in epoch 0, gen_loss = 1.1507389148076375, disc_loss = 0.3066069868703683
Trained batch 12 in epoch 0, gen_loss = 1.1675989444439228, disc_loss = 0.2968832873381101
Trained batch 13 in epoch 0, gen_loss = 1.134196651833398, disc_loss = 0.29985515560422626
Trained batch 14 in epoch 0, gen_loss = 1.134564713637034, disc_loss = 0.299299947420756
Trained batch 15 in epoch 0, gen_loss = 1.1518774144351482, disc_loss = 0.3027298152446747
Trained batch 16 in epoch 0, gen_loss = 1.1411865213338066, disc_loss = 0.3036046256037319
Trained batch 17 in epoch 0, gen_loss = 1.128692302438948, disc_loss = 0.30467309057712555
Trained batch 18 in epoch 0, gen_loss = 1.1355511075571965, disc_loss = 0.3034853794072804
Trained batch 19 in epoch 0, gen_loss = 1.1314103186130524, disc_loss = 0.2988112919032574
Trained batch 20 in epoch 0, gen_loss = 1.1218087843486242, disc_loss = 0.2965063871372314
Trained batch 21 in epoch 0, gen_loss = 1.116195635362105, disc_loss = 0.2955940602855249
Trained batch 22 in epoch 0, gen_loss = 1.1147788141084753, disc_loss = 0.29401553648969403
Trained batch 23 in epoch 0, gen_loss = 1.1047712713479996, disc_loss = 0.29217675266166526
Trained batch 24 in epoch 0, gen_loss = 1.092163474559784, disc_loss = 0.2896072471141815
Trained batch 25 in epoch 0, gen_loss = 1.0867941952668703, disc_loss = 0.2858175371701901
Trained batch 26 in epoch 0, gen_loss = 1.0867063491432756, disc_loss = 0.28408011462953353
Trained batch 27 in epoch 0, gen_loss = 1.0806882168565477, disc_loss = 0.28127848197306904
Trained batch 28 in epoch 0, gen_loss = 1.0732717123524895, disc_loss = 0.2772200616269276
Trained batch 29 in epoch 0, gen_loss = 1.0815684417883555, disc_loss = 0.27352665017048516
Trained batch 30 in epoch 0, gen_loss = 1.0751892558989986, disc_loss = 0.2705430523041756
Trained batch 31 in epoch 0, gen_loss = 1.0753874629735947, disc_loss = 0.26668968331068754
Trained batch 32 in epoch 0, gen_loss = 1.0765195832108005, disc_loss = 0.26307287134907464
Trained batch 33 in epoch 0, gen_loss = 1.0730985946515028, disc_loss = 0.25935953943168416
Trained batch 34 in epoch 0, gen_loss = 1.0866472840309143, disc_loss = 0.2578267723321915
Trained batch 35 in epoch 0, gen_loss = 1.078548476099968, disc_loss = 0.2601628365616004
Trained batch 36 in epoch 0, gen_loss = 1.0867697854299803, disc_loss = 0.2585060024583662
Trained batch 37 in epoch 0, gen_loss = 1.0856879180983494, disc_loss = 0.25666119589617375
Trained batch 38 in epoch 0, gen_loss = 1.0840662396871126, disc_loss = 0.25465207795302075
Trained batch 39 in epoch 0, gen_loss = 1.0876045003533363, disc_loss = 0.25301850624382494
Trained batch 40 in epoch 0, gen_loss = 1.0878604228903608, disc_loss = 0.2507690343914962
Trained batch 41 in epoch 0, gen_loss = 1.0917559081599826, disc_loss = 0.24742028879977407
Trained batch 42 in epoch 0, gen_loss = 1.0914477595063143, disc_loss = 0.24463602328716322
Trained batch 43 in epoch 0, gen_loss = 1.0981796072287993, disc_loss = 0.24189997955479406
Trained batch 44 in epoch 0, gen_loss = 1.0961397502157424, disc_loss = 0.24034687727689744
Trained batch 45 in epoch 0, gen_loss = 1.108329204113587, disc_loss = 0.23817289895985438
Trained batch 46 in epoch 0, gen_loss = 1.1005462179792689, disc_loss = 0.23751939627084326
Trained batch 47 in epoch 0, gen_loss = 1.1102871199448903, disc_loss = 0.23756595064575473
Trained batch 48 in epoch 0, gen_loss = 1.1023953216416496, disc_loss = 0.23713999028716767
Trained batch 49 in epoch 0, gen_loss = 1.101912089586258, disc_loss = 0.23483006566762923
Trained batch 50 in epoch 0, gen_loss = 1.1054659859806883, disc_loss = 0.23348658867910796
Trained batch 51 in epoch 0, gen_loss = 1.0974795921490743, disc_loss = 0.23337279403438935
Trained batch 52 in epoch 0, gen_loss = 1.100948636261922, disc_loss = 0.23132510581668816
Trained batch 53 in epoch 0, gen_loss = 1.1016127941785034, disc_loss = 0.22963950382890524
Trained batch 54 in epoch 0, gen_loss = 1.0970631848682058, disc_loss = 0.2282838822765784
Trained batch 55 in epoch 0, gen_loss = 1.1011155173182487, disc_loss = 0.22815240108008897
Trained batch 56 in epoch 0, gen_loss = 1.0921349054888676, disc_loss = 0.23038625128959356
Trained batch 57 in epoch 0, gen_loss = 1.0979920450983376, disc_loss = 0.23246454871420202
Trained batch 58 in epoch 0, gen_loss = 1.0929412609439786, disc_loss = 0.23227749550241536
Trained batch 59 in epoch 0, gen_loss = 1.088366018732389, disc_loss = 0.23206338348488012
Trained batch 60 in epoch 0, gen_loss = 1.0862802814264767, disc_loss = 0.23159083345385847
Trained batch 61 in epoch 0, gen_loss = 1.0821282469457196, disc_loss = 0.23104866533990828
Trained batch 62 in epoch 0, gen_loss = 1.0796839594841003, disc_loss = 0.23037616731155486
Trained batch 63 in epoch 0, gen_loss = 1.079492568038404, disc_loss = 0.22988729958888143
Trained batch 64 in epoch 0, gen_loss = 1.076577600149008, disc_loss = 0.22966232563440617
Trained batch 65 in epoch 0, gen_loss = 1.0725731389089064, disc_loss = 0.22963844070380385
Trained batch 66 in epoch 0, gen_loss = 1.0707759777111794, disc_loss = 0.22938498198541243
Trained batch 67 in epoch 0, gen_loss = 1.0664239411844927, disc_loss = 0.22985903622911258
Trained batch 68 in epoch 0, gen_loss = 1.0671559287154155, disc_loss = 0.22943175368118976
Trained batch 69 in epoch 0, gen_loss = 1.0640860625675745, disc_loss = 0.22907575741410255
Trained batch 70 in epoch 0, gen_loss = 1.0628649919805393, disc_loss = 0.22831374102495086
Trained batch 71 in epoch 0, gen_loss = 1.0614584187666576, disc_loss = 0.22808235231786966
Trained batch 72 in epoch 0, gen_loss = 1.0562353705706662, disc_loss = 0.22844104662741702
Trained batch 73 in epoch 0, gen_loss = 1.0649780647174731, disc_loss = 0.2308938979498438
Trained batch 74 in epoch 0, gen_loss = 1.0604396438598633, disc_loss = 0.2312388711174329
Trained batch 75 in epoch 0, gen_loss = 1.0582878134752576, disc_loss = 0.2308061259161485
Trained batch 76 in epoch 0, gen_loss = 1.0550499782933818, disc_loss = 0.2310103885345645
Trained batch 77 in epoch 0, gen_loss = 1.053043904976967, disc_loss = 0.2308588712834395
Trained batch 78 in epoch 0, gen_loss = 1.0514239902737774, disc_loss = 0.2315532939532135
Trained batch 79 in epoch 0, gen_loss = 1.0502492308616638, disc_loss = 0.23224394889548422
Trained batch 80 in epoch 0, gen_loss = 1.0485687829830028, disc_loss = 0.2320241367927304
Trained batch 81 in epoch 0, gen_loss = 1.0468736705256672, disc_loss = 0.2317212194022609
Trained batch 82 in epoch 0, gen_loss = 1.0446589632206653, disc_loss = 0.23153476102883558
Trained batch 83 in epoch 0, gen_loss = 1.04343005943866, disc_loss = 0.23105470649898052
Trained batch 84 in epoch 0, gen_loss = 1.0413458999465495, disc_loss = 0.2305358813966022
Trained batch 85 in epoch 0, gen_loss = 1.0390665704427764, disc_loss = 0.22974220238799273
Trained batch 86 in epoch 0, gen_loss = 1.0392290429137219, disc_loss = 0.22964744740861587
Trained batch 87 in epoch 0, gen_loss = 1.0351675809784369, disc_loss = 0.23283164753493937
Trained batch 88 in epoch 0, gen_loss = 1.0324046899763386, disc_loss = 0.23252863018365388
Trained batch 89 in epoch 0, gen_loss = 1.0330892185370126, disc_loss = 0.2325288437306881
Trained batch 90 in epoch 0, gen_loss = 1.033049768798954, disc_loss = 0.2325302323960996
Trained batch 91 in epoch 0, gen_loss = 1.0303691541371138, disc_loss = 0.23230207541390605
Trained batch 92 in epoch 0, gen_loss = 1.0277204430231484, disc_loss = 0.23206365837525295
Trained batch 93 in epoch 0, gen_loss = 1.0263165134064696, disc_loss = 0.23176366241371377
Trained batch 94 in epoch 0, gen_loss = 1.0242017394617984, disc_loss = 0.2314121615729834
Trained batch 95 in epoch 0, gen_loss = 1.0229439077277978, disc_loss = 0.23111171795365712
Trained batch 96 in epoch 0, gen_loss = 1.0209600581336267, disc_loss = 0.2306956325456039
Trained batch 97 in epoch 0, gen_loss = 1.0208911932244593, disc_loss = 0.23052195680080628
Trained batch 98 in epoch 0, gen_loss = 1.0179269115130107, disc_loss = 0.23031871329353312
Trained batch 99 in epoch 0, gen_loss = 1.019821930527687, disc_loss = 0.23036526806652546
Trained batch 100 in epoch 0, gen_loss = 1.0160077916513575, disc_loss = 0.2309428153209167
Trained batch 101 in epoch 0, gen_loss = 1.0179832940008127, disc_loss = 0.2307206384253268
Trained batch 102 in epoch 0, gen_loss = 1.014857463466311, disc_loss = 0.23058394154587997
Trained batch 103 in epoch 0, gen_loss = 1.0130665777967527, disc_loss = 0.2305614736934121
Trained batch 104 in epoch 0, gen_loss = 1.0135855816659474, disc_loss = 0.23039437653053374
Trained batch 105 in epoch 0, gen_loss = 1.01245771882669, disc_loss = 0.23026970584156378
Trained batch 106 in epoch 0, gen_loss = 1.010025978088379, disc_loss = 0.23022510062589824
Trained batch 107 in epoch 0, gen_loss = 1.0111127385386713, disc_loss = 0.23049656294838147
Trained batch 108 in epoch 0, gen_loss = 1.009300404185549, disc_loss = 0.23010899734879853
Trained batch 109 in epoch 0, gen_loss = 1.007264614647085, disc_loss = 0.22982276617126032
Trained batch 110 in epoch 0, gen_loss = 1.007899020706211, disc_loss = 0.2297147790993656
Trained batch 111 in epoch 0, gen_loss = 1.0058571552591664, disc_loss = 0.22934216413913028
Trained batch 112 in epoch 0, gen_loss = 1.004801871502294, disc_loss = 0.2290567460972651
Trained batch 113 in epoch 0, gen_loss = 1.0038532510138394, disc_loss = 0.22852672015627226
Trained batch 114 in epoch 0, gen_loss = 1.0022547296855762, disc_loss = 0.22815825492143632
Trained batch 115 in epoch 0, gen_loss = 1.0044194038571983, disc_loss = 0.22806392680725146
Trained batch 116 in epoch 0, gen_loss = 1.0009199498046157, disc_loss = 0.22881181722777522
Trained batch 117 in epoch 0, gen_loss = 1.0024048802206071, disc_loss = 0.22860135611588672
Trained batch 118 in epoch 0, gen_loss = 0.9995963944106543, disc_loss = 0.2295813256827723
Trained batch 119 in epoch 0, gen_loss = 0.9997798929611842, disc_loss = 0.2293294770643115
Trained batch 120 in epoch 0, gen_loss = 0.9987829450733405, disc_loss = 0.22908537514692495
Trained batch 121 in epoch 0, gen_loss = 0.9967409465156618, disc_loss = 0.22895433259059172
Trained batch 122 in epoch 0, gen_loss = 0.9966873187359756, disc_loss = 0.22866666674371658
Trained batch 123 in epoch 0, gen_loss = 0.9953213175458293, disc_loss = 0.22826477662930567
Trained batch 124 in epoch 0, gen_loss = 0.9954124855995178, disc_loss = 0.22807773333787917
Trained batch 125 in epoch 0, gen_loss = 0.993974160580408, disc_loss = 0.22775988878002243
Trained batch 126 in epoch 0, gen_loss = 0.9933916705799853, disc_loss = 0.22713942855127214
Trained batch 127 in epoch 0, gen_loss = 0.9920126106590033, disc_loss = 0.22685829497640952
Trained batch 128 in epoch 0, gen_loss = 0.9930155942606371, disc_loss = 0.22670932928490084
Trained batch 129 in epoch 0, gen_loss = 0.9895017041609837, disc_loss = 0.2278759978711605
Trained batch 130 in epoch 0, gen_loss = 0.9906885810480773, disc_loss = 0.22784814983606339
Trained batch 131 in epoch 0, gen_loss = 0.990348451516845, disc_loss = 0.22800816093204598
Trained batch 132 in epoch 0, gen_loss = 0.9883759738807392, disc_loss = 0.22801578532260164
Trained batch 133 in epoch 0, gen_loss = 0.9877990786709002, disc_loss = 0.22734493478687842
Trained batch 134 in epoch 0, gen_loss = 0.9870448390642802, disc_loss = 0.22736040237877103
Trained batch 135 in epoch 0, gen_loss = 0.9869814225855995, disc_loss = 0.22772550862282515
Trained batch 136 in epoch 0, gen_loss = 0.9852238767338495, disc_loss = 0.2273760587520843
Trained batch 137 in epoch 0, gen_loss = 0.985156964564669, disc_loss = 0.2273002177800821
Trained batch 138 in epoch 0, gen_loss = 0.9853022887552385, disc_loss = 0.2265632150949334
Trained batch 139 in epoch 0, gen_loss = 0.9831180095672607, disc_loss = 0.22632020653358528
Trained batch 140 in epoch 0, gen_loss = 0.9834770074127414, disc_loss = 0.22587193902714034
Trained batch 141 in epoch 0, gen_loss = 0.98353528388789, disc_loss = 0.22543173897224414
Trained batch 142 in epoch 0, gen_loss = 0.9826972713837256, disc_loss = 0.22510785390655477
Trained batch 143 in epoch 0, gen_loss = 0.9821643651359611, disc_loss = 0.22466224913174906
Trained batch 144 in epoch 0, gen_loss = 0.9806317284189422, disc_loss = 0.22421905680977064
Trained batch 145 in epoch 0, gen_loss = 0.9835408203405877, disc_loss = 0.22522255467021302
Trained batch 146 in epoch 0, gen_loss = 0.9820427983796516, disc_loss = 0.2255214652946206
Trained batch 147 in epoch 0, gen_loss = 0.9806835884983475, disc_loss = 0.22522430439051744
Trained batch 148 in epoch 0, gen_loss = 0.9808386836275959, disc_loss = 0.22506550889487234
Trained batch 149 in epoch 0, gen_loss = 0.9799138744672139, disc_loss = 0.22474108909567198
Trained batch 150 in epoch 0, gen_loss = 0.9778994933658878, disc_loss = 0.22471821086098817
Trained batch 151 in epoch 0, gen_loss = 0.9782599371514822, disc_loss = 0.22462219691002056
Trained batch 152 in epoch 0, gen_loss = 0.9772238575555141, disc_loss = 0.22445899866570054
Trained batch 153 in epoch 0, gen_loss = 0.9761297137706311, disc_loss = 0.22417049279267137
Trained batch 154 in epoch 0, gen_loss = 0.9756839517624147, disc_loss = 0.22417040058682042
Trained batch 155 in epoch 0, gen_loss = 0.9740635458475504, disc_loss = 0.22415347058230486
Trained batch 156 in epoch 0, gen_loss = 0.9732737313410279, disc_loss = 0.2237477749111546
Trained batch 157 in epoch 0, gen_loss = 0.9733978555172305, disc_loss = 0.22327336239852483
Trained batch 158 in epoch 0, gen_loss = 0.974612092072109, disc_loss = 0.2222886122060272
Trained batch 159 in epoch 0, gen_loss = 0.9745429400354624, disc_loss = 0.22147901030257344
Trained batch 160 in epoch 0, gen_loss = 0.9753964291596264, disc_loss = 0.2208990233470194
Trained batch 161 in epoch 0, gen_loss = 0.9741377830505371, disc_loss = 0.22050464116496804
Trained batch 162 in epoch 0, gen_loss = 0.9829673854851284, disc_loss = 0.22202804684638977
Trained batch 163 in epoch 0, gen_loss = 0.980971394879062, disc_loss = 0.2224192154116747
Trained batch 164 in epoch 0, gen_loss = 0.9832412203152975, disc_loss = 0.22190323607488113
Trained batch 165 in epoch 0, gen_loss = 0.983787858701614, disc_loss = 0.22128975956913938
Trained batch 166 in epoch 0, gen_loss = 0.981946763164269, disc_loss = 0.22109637619135622
Trained batch 167 in epoch 0, gen_loss = 0.9804867882104147, disc_loss = 0.2208118877772774
Trained batch 168 in epoch 0, gen_loss = 0.9806250792283279, disc_loss = 0.2203934177668137
Trained batch 169 in epoch 0, gen_loss = 0.9810720429700963, disc_loss = 0.2199480315341669
Trained batch 170 in epoch 0, gen_loss = 0.9812527783432899, disc_loss = 0.21914994795071452
Trained batch 171 in epoch 0, gen_loss = 0.980539490663728, disc_loss = 0.21841716900641142
Trained batch 172 in epoch 0, gen_loss = 0.9807030882449509, disc_loss = 0.21787643729779071
Trained batch 173 in epoch 0, gen_loss = 0.9806268341924952, disc_loss = 0.21733044935711499
Trained batch 174 in epoch 0, gen_loss = 0.983043669632503, disc_loss = 0.21639396939958844
Trained batch 175 in epoch 0, gen_loss = 0.9820748042653907, disc_loss = 0.21593785836276683
Trained batch 176 in epoch 0, gen_loss = 0.9832811507128053, disc_loss = 0.21544667623810848
Trained batch 177 in epoch 0, gen_loss = 0.9820211151342714, disc_loss = 0.21523372929417686
Trained batch 178 in epoch 0, gen_loss = 0.981542714814234, disc_loss = 0.2148054225484752
Trained batch 179 in epoch 0, gen_loss = 0.9809432768159443, disc_loss = 0.2143479405177964
Trained batch 180 in epoch 0, gen_loss = 0.9813683846378853, disc_loss = 0.2139371699391149
Trained batch 181 in epoch 0, gen_loss = 0.9800667549882617, disc_loss = 0.21358538288009035
Trained batch 182 in epoch 0, gen_loss = 0.9841421989143871, disc_loss = 0.21484538464924025
Trained batch 183 in epoch 0, gen_loss = 0.9833939658558887, disc_loss = 0.21453514322638512
Trained batch 184 in epoch 0, gen_loss = 0.981357182038797, disc_loss = 0.21475940217842926
Trained batch 185 in epoch 0, gen_loss = 0.9823398173496287, disc_loss = 0.21513264249729855
Trained batch 186 in epoch 0, gen_loss = 0.982241991369482, disc_loss = 0.21508450120846856
Trained batch 187 in epoch 0, gen_loss = 0.9810711972891016, disc_loss = 0.2149437616004589
Trained batch 188 in epoch 0, gen_loss = 0.979566856351479, disc_loss = 0.2152705089282737
Trained batch 189 in epoch 0, gen_loss = 0.9784748770688709, disc_loss = 0.21525140067464427
Trained batch 190 in epoch 0, gen_loss = 0.9783220032122747, disc_loss = 0.2149936290779663
Trained batch 191 in epoch 0, gen_loss = 0.9772856226190925, disc_loss = 0.2149242916299651
Trained batch 192 in epoch 0, gen_loss = 0.9758876472557146, disc_loss = 0.21469610759631338
Trained batch 193 in epoch 0, gen_loss = 0.9753902612887707, disc_loss = 0.21444420163164435
Trained batch 194 in epoch 0, gen_loss = 0.9750418873933645, disc_loss = 0.21426061208431538
Trained batch 195 in epoch 0, gen_loss = 0.9737156571782365, disc_loss = 0.2142089409943746
Trained batch 196 in epoch 0, gen_loss = 0.974421500251983, disc_loss = 0.21458821286102236
Trained batch 197 in epoch 0, gen_loss = 0.9722692692520642, disc_loss = 0.21501026979901575
Trained batch 198 in epoch 0, gen_loss = 0.9719795021579494, disc_loss = 0.21481142496343833
Trained batch 199 in epoch 0, gen_loss = 0.9718061271309852, disc_loss = 0.21464432269334793
Trained batch 200 in epoch 0, gen_loss = 0.9699359978016336, disc_loss = 0.21477780027769097
Trained batch 201 in epoch 0, gen_loss = 0.9699280415431107, disc_loss = 0.21438507160337844
Trained batch 202 in epoch 0, gen_loss = 0.9711685321601153, disc_loss = 0.21408437809039807
Trained batch 203 in epoch 0, gen_loss = 0.9693037627958784, disc_loss = 0.213975109525171
Trained batch 204 in epoch 0, gen_loss = 0.9670380560363211, disc_loss = 0.21424398531274097
Trained batch 205 in epoch 0, gen_loss = 0.9699760041769269, disc_loss = 0.21480466707528217
Trained batch 206 in epoch 0, gen_loss = 0.9693351494517304, disc_loss = 0.2143158981045663
Trained batch 207 in epoch 0, gen_loss = 0.9678251236104048, disc_loss = 0.2140015707566188
Trained batch 208 in epoch 0, gen_loss = 0.9679660061329746, disc_loss = 0.21343121139341564
Trained batch 209 in epoch 0, gen_loss = 0.9686153031530834, disc_loss = 0.21293357631989887
Trained batch 210 in epoch 0, gen_loss = 0.9675964713096619, disc_loss = 0.212756380368183
Trained batch 211 in epoch 0, gen_loss = 0.9666418407885533, disc_loss = 0.21265387078219988
Trained batch 212 in epoch 0, gen_loss = 0.9665316620343168, disc_loss = 0.21247351421436794
Trained batch 213 in epoch 0, gen_loss = 0.9657727505002066, disc_loss = 0.21208474109663028
Trained batch 214 in epoch 0, gen_loss = 0.9680109514746555, disc_loss = 0.21215455920197243
Trained batch 215 in epoch 0, gen_loss = 0.9662697428354511, disc_loss = 0.21227804950817866
Trained batch 216 in epoch 0, gen_loss = 0.9662049995040014, disc_loss = 0.21185947885985748
Trained batch 217 in epoch 0, gen_loss = 0.9692029687789602, disc_loss = 0.21201962389803808
Trained batch 218 in epoch 0, gen_loss = 0.9676104846610326, disc_loss = 0.21228923815298298
Trained batch 219 in epoch 0, gen_loss = 0.9684912543405186, disc_loss = 0.21191960234533658
Trained batch 220 in epoch 0, gen_loss = 0.9689334013343397, disc_loss = 0.21137311879326315
Trained batch 221 in epoch 0, gen_loss = 0.9687874542700278, disc_loss = 0.21078519049931216
Trained batch 222 in epoch 0, gen_loss = 0.9676367009701751, disc_loss = 0.21063291321554525
Trained batch 223 in epoch 0, gen_loss = 0.9683441000857523, disc_loss = 0.21051278500817716
Trained batch 224 in epoch 0, gen_loss = 0.9700727232297262, disc_loss = 0.21004069735606512
Trained batch 225 in epoch 0, gen_loss = 0.9686599505686126, disc_loss = 0.21004438950820306
Trained batch 226 in epoch 0, gen_loss = 0.9685194715529286, disc_loss = 0.20957409242570138
Trained batch 227 in epoch 0, gen_loss = 0.969608190028291, disc_loss = 0.2092330822100242
Trained batch 228 in epoch 0, gen_loss = 0.9687873976199387, disc_loss = 0.20889443539766245
Trained batch 229 in epoch 0, gen_loss = 0.9698787914670032, disc_loss = 0.20825137094311091
Trained batch 230 in epoch 0, gen_loss = 0.9692968682293251, disc_loss = 0.20799619720611737
Trained batch 231 in epoch 0, gen_loss = 0.970618582491217, disc_loss = 0.20742728439127578
Trained batch 232 in epoch 0, gen_loss = 0.9717057472646492, disc_loss = 0.20706492029290344
Trained batch 233 in epoch 0, gen_loss = 0.9708689619333316, disc_loss = 0.2065525694280608
Trained batch 234 in epoch 0, gen_loss = 0.9735424219293797, disc_loss = 0.20609581771683186
Trained batch 235 in epoch 0, gen_loss = 0.9724083512516345, disc_loss = 0.2058527719987146
Trained batch 236 in epoch 0, gen_loss = 0.9748836479106533, disc_loss = 0.20704720722597864
Trained batch 237 in epoch 0, gen_loss = 0.973267142762657, disc_loss = 0.20702328830331312
Trained batch 238 in epoch 0, gen_loss = 0.9742462038993835, disc_loss = 0.20645302449933656
Trained batch 239 in epoch 0, gen_loss = 0.9745496727526188, disc_loss = 0.2060160182105998
Trained batch 240 in epoch 0, gen_loss = 0.9749142923295745, disc_loss = 0.20561518278977683
Trained batch 241 in epoch 0, gen_loss = 0.9738973135297949, disc_loss = 0.20540472295535497
Trained batch 242 in epoch 0, gen_loss = 0.9731112411973898, disc_loss = 0.20503848643950473
Trained batch 243 in epoch 0, gen_loss = 0.9725370876124648, disc_loss = 0.20483875372370736
Trained batch 244 in epoch 0, gen_loss = 0.9722142131961122, disc_loss = 0.2048124970222006
Trained batch 245 in epoch 0, gen_loss = 0.9717222609655644, disc_loss = 0.2045155636421064
Trained batch 246 in epoch 0, gen_loss = 0.9713980027055933, disc_loss = 0.20421393185491987
Trained batch 247 in epoch 0, gen_loss = 0.9736105246889976, disc_loss = 0.20366193621509499
Trained batch 248 in epoch 0, gen_loss = 0.9726426543959652, disc_loss = 0.20332013073575544
Trained batch 249 in epoch 0, gen_loss = 0.97427303647995, disc_loss = 0.20271258939802647
Trained batch 250 in epoch 0, gen_loss = 0.9767265875500987, disc_loss = 0.20247982699142034
Trained batch 251 in epoch 0, gen_loss = 0.9747271874830836, disc_loss = 0.20303356708101336
Trained batch 252 in epoch 0, gen_loss = 0.9751485907748754, disc_loss = 0.20248381983444624
Trained batch 253 in epoch 0, gen_loss = 0.975660078286186, disc_loss = 0.20226064495976984
Trained batch 254 in epoch 0, gen_loss = 0.9757258023701462, disc_loss = 0.20194378870667196
Trained batch 255 in epoch 0, gen_loss = 0.9749722831184044, disc_loss = 0.2016530547552975
Trained batch 256 in epoch 0, gen_loss = 0.9747902492372906, disc_loss = 0.20132204422053196
Trained batch 257 in epoch 0, gen_loss = 0.9765558957822563, disc_loss = 0.20101212905070118
Trained batch 258 in epoch 0, gen_loss = 0.9749958178021273, disc_loss = 0.2012900778426857
Trained batch 259 in epoch 0, gen_loss = 0.9760195744725374, disc_loss = 0.20107793789356948
Trained batch 260 in epoch 0, gen_loss = 0.9775221305560335, disc_loss = 0.200571544282852
Trained batch 261 in epoch 0, gen_loss = 0.9758658103132976, disc_loss = 0.2007307689068199
Trained batch 262 in epoch 0, gen_loss = 0.977186801202397, disc_loss = 0.2008199168390421
Trained batch 263 in epoch 0, gen_loss = 0.9765053902837363, disc_loss = 0.2008109432645142
Trained batch 264 in epoch 0, gen_loss = 0.9767119143369063, disc_loss = 0.2005385881465561
Trained batch 265 in epoch 0, gen_loss = 0.9747758376643174, disc_loss = 0.20093253659630628
Trained batch 266 in epoch 0, gen_loss = 0.9762065980988048, disc_loss = 0.2008570431686296
Trained batch 267 in epoch 0, gen_loss = 0.9758687004891794, disc_loss = 0.2006868780799099
Trained batch 268 in epoch 0, gen_loss = 0.9753162609378645, disc_loss = 0.20052284252964875
Trained batch 269 in epoch 0, gen_loss = 0.9740629994206959, disc_loss = 0.20042206166243112
Trained batch 270 in epoch 0, gen_loss = 0.9737515107512034, disc_loss = 0.20077504515922817
Trained batch 271 in epoch 0, gen_loss = 0.9736163249129758, disc_loss = 0.20048380499322185
Trained batch 272 in epoch 0, gen_loss = 0.9726643423676055, disc_loss = 0.20075778623585736
Trained batch 273 in epoch 0, gen_loss = 0.9715397593966366, disc_loss = 0.20073040285195312
Trained batch 274 in epoch 0, gen_loss = 0.9714172732830048, disc_loss = 0.20060793089595708
Trained batch 275 in epoch 0, gen_loss = 0.9713314671231352, disc_loss = 0.20039719439017167
Trained batch 276 in epoch 0, gen_loss = 0.9703583569948424, disc_loss = 0.20030427096068643
Trained batch 277 in epoch 0, gen_loss = 0.9715340020630857, disc_loss = 0.19997117685810697
Trained batch 278 in epoch 0, gen_loss = 0.9714059469734041, disc_loss = 0.19952357324060574
Trained batch 279 in epoch 0, gen_loss = 0.9709319205156395, disc_loss = 0.19934663992109045
Trained batch 280 in epoch 0, gen_loss = 0.9722878781289817, disc_loss = 0.1988213806814146
Trained batch 281 in epoch 0, gen_loss = 0.9722885304520316, disc_loss = 0.19827669655494656
Trained batch 282 in epoch 0, gen_loss = 0.9725597575570164, disc_loss = 0.19773040224827643
Trained batch 283 in epoch 0, gen_loss = 0.974444616533501, disc_loss = 0.19728418208763632
Trained batch 284 in epoch 0, gen_loss = 0.9729252979420779, disc_loss = 0.19756917357444764
Trained batch 285 in epoch 0, gen_loss = 0.9753861849333023, disc_loss = 0.19769539725738805
Trained batch 286 in epoch 0, gen_loss = 0.9741469660079438, disc_loss = 0.19780048063407793
Trained batch 287 in epoch 0, gen_loss = 0.9738136160497864, disc_loss = 0.19759371204094756
Trained batch 288 in epoch 0, gen_loss = 0.9742896551698137, disc_loss = 0.19769595161235043
Trained batch 289 in epoch 0, gen_loss = 0.9727549675209769, disc_loss = 0.19794300100926696
Trained batch 290 in epoch 0, gen_loss = 0.9723609135937445, disc_loss = 0.1980080961464197
Trained batch 291 in epoch 0, gen_loss = 0.9713878540755951, disc_loss = 0.19822740866100952
Trained batch 292 in epoch 0, gen_loss = 0.9705827969536439, disc_loss = 0.19815353347376344
Trained batch 293 in epoch 0, gen_loss = 0.9699654503136265, disc_loss = 0.19795753533134655
Trained batch 294 in epoch 0, gen_loss = 0.9699688338627249, disc_loss = 0.19782121873508066
Trained batch 295 in epoch 0, gen_loss = 0.9697275017765729, disc_loss = 0.19754440176325874
Trained batch 296 in epoch 0, gen_loss = 0.9696719603305713, disc_loss = 0.19752069876250195
Trained batch 297 in epoch 0, gen_loss = 0.9689239548556756, disc_loss = 0.1973815537339089
Trained batch 298 in epoch 0, gen_loss = 0.9686567640822866, disc_loss = 0.19742729836283718
Trained batch 299 in epoch 0, gen_loss = 0.9687850229938825, disc_loss = 0.19734180813034374
Trained batch 300 in epoch 0, gen_loss = 0.9677078431824909, disc_loss = 0.1975629398494068
Trained batch 301 in epoch 0, gen_loss = 0.9682249787035367, disc_loss = 0.19762936120988517
Trained batch 302 in epoch 0, gen_loss = 0.967399399174322, disc_loss = 0.1974829387153336
Trained batch 303 in epoch 0, gen_loss = 0.9683485347777605, disc_loss = 0.19760172549439103
Trained batch 304 in epoch 0, gen_loss = 0.9670653088170974, disc_loss = 0.1977069405258679
Trained batch 305 in epoch 0, gen_loss = 0.9681736046777052, disc_loss = 0.19775178483109068
Trained batch 306 in epoch 0, gen_loss = 0.9683779566412252, disc_loss = 0.19753404311131964
Trained batch 307 in epoch 0, gen_loss = 0.9671652538629322, disc_loss = 0.19753183685726933
Trained batch 308 in epoch 0, gen_loss = 0.9673794076859372, disc_loss = 0.19730090902075412
Trained batch 309 in epoch 0, gen_loss = 0.9676588009442052, disc_loss = 0.19705370284857288
Trained batch 310 in epoch 0, gen_loss = 0.9668810180720793, disc_loss = 0.19685918313130688
Trained batch 311 in epoch 0, gen_loss = 0.9664361550448797, disc_loss = 0.1965951998359882
Trained batch 312 in epoch 0, gen_loss = 0.9682357753998936, disc_loss = 0.1967105954028547
Trained batch 313 in epoch 0, gen_loss = 0.968161610566127, disc_loss = 0.19624746159003797
Trained batch 314 in epoch 0, gen_loss = 0.9667166466750796, disc_loss = 0.19651559789975484
Trained batch 315 in epoch 0, gen_loss = 0.9669891338371024, disc_loss = 0.19660842329075065
Trained batch 316 in epoch 0, gen_loss = 0.9663866277936881, disc_loss = 0.19671473927114289
Trained batch 317 in epoch 0, gen_loss = 0.9657806299579969, disc_loss = 0.19665635363110956
Trained batch 318 in epoch 0, gen_loss = 0.9657181567719737, disc_loss = 0.19662731549582885
Trained batch 319 in epoch 0, gen_loss = 0.9653403646312654, disc_loss = 0.1966460408642888
Trained batch 320 in epoch 0, gen_loss = 0.9649562682503852, disc_loss = 0.19654594686729515
Trained batch 321 in epoch 0, gen_loss = 0.9644945788642635, disc_loss = 0.1966874674888131
Trained batch 322 in epoch 0, gen_loss = 0.9634574250355593, disc_loss = 0.19678562898015828
Trained batch 323 in epoch 0, gen_loss = 0.9626386711074982, disc_loss = 0.19677696579400403
Trained batch 324 in epoch 0, gen_loss = 0.9633088725346786, disc_loss = 0.19662590036025415
Trained batch 325 in epoch 0, gen_loss = 0.9625208378752317, disc_loss = 0.19653409838310779
Trained batch 326 in epoch 0, gen_loss = 0.9621541005208951, disc_loss = 0.19634168597354074
Trained batch 327 in epoch 0, gen_loss = 0.9626759618702458, disc_loss = 0.19607023981104538
Trained batch 328 in epoch 0, gen_loss = 0.9626360958651569, disc_loss = 0.1958152925352195
Trained batch 329 in epoch 0, gen_loss = 0.9620125702836296, disc_loss = 0.19547596485777335
Trained batch 330 in epoch 0, gen_loss = 0.9638707339763641, disc_loss = 0.19525887240906137
Trained batch 331 in epoch 0, gen_loss = 0.9632258423301111, disc_loss = 0.1951908112155745
Trained batch 332 in epoch 0, gen_loss = 0.9625175129006933, disc_loss = 0.1950746559658208
Trained batch 333 in epoch 0, gen_loss = 0.9636034877178912, disc_loss = 0.19508291519955248
Trained batch 334 in epoch 0, gen_loss = 0.9627521866293096, disc_loss = 0.19491179890152233
Trained batch 335 in epoch 0, gen_loss = 0.9633552764675447, disc_loss = 0.19493469278815956
Trained batch 336 in epoch 0, gen_loss = 0.9629093414774988, disc_loss = 0.19461492922168103
Trained batch 337 in epoch 0, gen_loss = 0.9625268634078066, disc_loss = 0.194439525633345
Trained batch 338 in epoch 0, gen_loss = 0.9627724005936873, disc_loss = 0.1943030179557899
Trained batch 339 in epoch 0, gen_loss = 0.9629051912356825, disc_loss = 0.19397848548696323
Trained batch 340 in epoch 0, gen_loss = 0.9625259577528822, disc_loss = 0.19388171324719433
Trained batch 341 in epoch 0, gen_loss = 0.9620909109624506, disc_loss = 0.1936509143234345
Trained batch 342 in epoch 0, gen_loss = 0.9634629971953245, disc_loss = 0.19369960163581476
Trained batch 343 in epoch 0, gen_loss = 0.9622119672942994, disc_loss = 0.19387539183764263
Trained batch 344 in epoch 0, gen_loss = 0.9625411542429441, disc_loss = 0.19351708651452826
Trained batch 345 in epoch 0, gen_loss = 0.96449088163114, disc_loss = 0.19348062897865484
Trained batch 346 in epoch 0, gen_loss = 0.9638727512586357, disc_loss = 0.19328520144268826
Trained batch 347 in epoch 0, gen_loss = 0.9635461929371987, disc_loss = 0.19307114689172
Trained batch 348 in epoch 0, gen_loss = 0.964364483995219, disc_loss = 0.19296506760284346
Trained batch 349 in epoch 0, gen_loss = 0.9652182399375098, disc_loss = 0.1925455991923809
Trained batch 350 in epoch 0, gen_loss = 0.9637146139586413, disc_loss = 0.19294355773603136
Trained batch 351 in epoch 0, gen_loss = 0.9656001147390767, disc_loss = 0.19293465463190593
Trained batch 352 in epoch 0, gen_loss = 0.9663244132110823, disc_loss = 0.1927359087060261
Trained batch 353 in epoch 0, gen_loss = 0.9655081805704677, disc_loss = 0.19270879044960448
Trained batch 354 in epoch 0, gen_loss = 0.9662499321178651, disc_loss = 0.19226415816327216
Trained batch 355 in epoch 0, gen_loss = 0.9671265952540248, disc_loss = 0.19191244123189638
Trained batch 356 in epoch 0, gen_loss = 0.9674523822566709, disc_loss = 0.19155288299843043
Trained batch 357 in epoch 0, gen_loss = 0.9666533424368118, disc_loss = 0.1914030001941007
Trained batch 358 in epoch 0, gen_loss = 0.9666950447811722, disc_loss = 0.19115151628893398
Trained batch 359 in epoch 0, gen_loss = 0.9671787501209312, disc_loss = 0.1909266936075356
Trained batch 360 in epoch 0, gen_loss = 0.9660417809876048, disc_loss = 0.19095113277022527
Trained batch 361 in epoch 0, gen_loss = 0.9670447658603363, disc_loss = 0.19096878864570876
Trained batch 362 in epoch 0, gen_loss = 0.966136618504511, disc_loss = 0.1909288216597778
Trained batch 363 in epoch 0, gen_loss = 0.9659513055786981, disc_loss = 0.19079969072161795
Trained batch 364 in epoch 0, gen_loss = 0.9654814783024461, disc_loss = 0.19064295608295154
Trained batch 365 in epoch 0, gen_loss = 0.9654659170433472, disc_loss = 0.19050381017105827
Trained batch 366 in epoch 0, gen_loss = 0.9646876812468432, disc_loss = 0.1906886840345749
Trained batch 367 in epoch 0, gen_loss = 0.9637937415391207, disc_loss = 0.19080462855408373
Trained batch 368 in epoch 0, gen_loss = 0.964118125881283, disc_loss = 0.19078875757087538
Trained batch 369 in epoch 0, gen_loss = 0.9639516960124712, disc_loss = 0.1908603212519272
Trained batch 370 in epoch 0, gen_loss = 0.9633888591813913, disc_loss = 0.1907000582855988
Trained batch 371 in epoch 0, gen_loss = 0.9646918992361715, disc_loss = 0.1906121721871758
Trained batch 372 in epoch 0, gen_loss = 0.9641861117397492, disc_loss = 0.19042874547096103
Trained batch 373 in epoch 0, gen_loss = 0.9635153541112329, disc_loss = 0.1904978590972602
Trained batch 374 in epoch 0, gen_loss = 0.9643877901236216, disc_loss = 0.1904275798201561
Trained batch 375 in epoch 0, gen_loss = 0.9636152507935433, disc_loss = 0.19034528131893974
Trained batch 376 in epoch 0, gen_loss = 0.9638731565810641, disc_loss = 0.19009828371853033
Trained batch 377 in epoch 0, gen_loss = 0.9637297496751502, disc_loss = 0.18984323887834473
Trained batch 378 in epoch 0, gen_loss = 0.963978523707956, disc_loss = 0.18963235292318315
Trained batch 379 in epoch 0, gen_loss = 0.9638079763243073, disc_loss = 0.18953973194094081
Trained batch 380 in epoch 0, gen_loss = 0.962591617986599, disc_loss = 0.1899494807825001
Trained batch 381 in epoch 0, gen_loss = 0.9626504144587442, disc_loss = 0.18997190547473145
Trained batch 382 in epoch 0, gen_loss = 0.9627808421928017, disc_loss = 0.19000520448227154
Trained batch 383 in epoch 0, gen_loss = 0.9625939792798212, disc_loss = 0.1897984328873766
Trained batch 384 in epoch 0, gen_loss = 0.9622004398277828, disc_loss = 0.1897303524923015
Trained batch 385 in epoch 0, gen_loss = 0.9623735952408202, disc_loss = 0.1897101305193543
Trained batch 386 in epoch 0, gen_loss = 0.9612855600910285, disc_loss = 0.1901294604636902
Trained batch 387 in epoch 0, gen_loss = 0.9609529243148479, disc_loss = 0.1900802743381139
Trained batch 388 in epoch 0, gen_loss = 0.9600456366961904, disc_loss = 0.1902651024699824
Trained batch 389 in epoch 0, gen_loss = 0.9610801341441961, disc_loss = 0.19022413490292353
Trained batch 390 in epoch 0, gen_loss = 0.9605638615767974, disc_loss = 0.19005244511091496
Trained batch 391 in epoch 0, gen_loss = 0.9603703025804491, disc_loss = 0.18983893430962853
Trained batch 392 in epoch 0, gen_loss = 0.9603494812969033, disc_loss = 0.18970978180415757
Trained batch 393 in epoch 0, gen_loss = 0.9598394509045606, disc_loss = 0.18959525299405083
Trained batch 394 in epoch 0, gen_loss = 0.9602389260937896, disc_loss = 0.18957620527170882
Trained batch 395 in epoch 0, gen_loss = 0.959732468456331, disc_loss = 0.18954482072531575
Trained batch 396 in epoch 0, gen_loss = 0.9597046715336423, disc_loss = 0.18935311272252417
Trained batch 397 in epoch 0, gen_loss = 0.9603847556527535, disc_loss = 0.18915614504460712
Trained batch 398 in epoch 0, gen_loss = 0.9595561038730736, disc_loss = 0.18914420200619184
Trained batch 399 in epoch 0, gen_loss = 0.9597609397023916, disc_loss = 0.18928341556340456
Trained batch 400 in epoch 0, gen_loss = 0.9592637687995844, disc_loss = 0.18916036980110512
Trained batch 401 in epoch 0, gen_loss = 0.9604872310191245, disc_loss = 0.18908175455397042
Trained batch 402 in epoch 0, gen_loss = 0.9599472836732272, disc_loss = 0.18898932306701433
Trained batch 403 in epoch 0, gen_loss = 0.9599390619314543, disc_loss = 0.18875889686664732
Trained batch 404 in epoch 0, gen_loss = 0.961160358602618, disc_loss = 0.18876713933768097
Trained batch 405 in epoch 0, gen_loss = 0.9608954689626036, disc_loss = 0.18849090683122574
Trained batch 406 in epoch 0, gen_loss = 0.9612904511007688, disc_loss = 0.18816880080894696
Trained batch 407 in epoch 0, gen_loss = 0.9609603041089049, disc_loss = 0.18794038616960831
Trained batch 408 in epoch 0, gen_loss = 0.9612429530375745, disc_loss = 0.18788692744079896
Trained batch 409 in epoch 0, gen_loss = 0.961052621501248, disc_loss = 0.18768207833352613
Trained batch 410 in epoch 0, gen_loss = 0.9612365020597649, disc_loss = 0.18732181012884253
Trained batch 411 in epoch 0, gen_loss = 0.9619765871213478, disc_loss = 0.18712668085597384
Trained batch 412 in epoch 0, gen_loss = 0.9614814755847322, disc_loss = 0.18709601812768215
Trained batch 413 in epoch 0, gen_loss = 0.9610574934361638, disc_loss = 0.18688789234105227
Trained batch 414 in epoch 0, gen_loss = 0.9624435580638517, disc_loss = 0.18686078414320945
Trained batch 415 in epoch 0, gen_loss = 0.9615962150721595, disc_loss = 0.18697111015745366
Trained batch 416 in epoch 0, gen_loss = 0.962414849647801, disc_loss = 0.18671789803772235
Trained batch 417 in epoch 0, gen_loss = 0.9633018121337206, disc_loss = 0.18653819484538153
Trained batch 418 in epoch 0, gen_loss = 0.96290888301228, disc_loss = 0.18649863072077527
Trained batch 419 in epoch 0, gen_loss = 0.9627383082395509, disc_loss = 0.18634932222997858
Trained batch 420 in epoch 0, gen_loss = 0.9647166234155732, disc_loss = 0.18633891880122896
Trained batch 421 in epoch 0, gen_loss = 0.9643384877115629, disc_loss = 0.18612555616592627
Trained batch 422 in epoch 0, gen_loss = 0.9643215887230903, disc_loss = 0.18584479894445016
Trained batch 423 in epoch 0, gen_loss = 0.9654271528827694, disc_loss = 0.18619880360498461
Trained batch 424 in epoch 0, gen_loss = 0.9642371483410106, disc_loss = 0.18665993794798852
Trained batch 425 in epoch 0, gen_loss = 0.9636748914427601, disc_loss = 0.1865642895493569
Trained batch 426 in epoch 0, gen_loss = 0.9639318271319816, disc_loss = 0.18652184456761325
Trained batch 427 in epoch 0, gen_loss = 0.9637900861067192, disc_loss = 0.1864326354203241
Trained batch 428 in epoch 0, gen_loss = 0.9636173744301696, disc_loss = 0.18640853588779768
Trained batch 429 in epoch 0, gen_loss = 0.9634837332159973, disc_loss = 0.18619996197521688
Trained batch 430 in epoch 0, gen_loss = 0.9626549620639421, disc_loss = 0.18615442264626308
Trained batch 431 in epoch 0, gen_loss = 0.9624339491128922, disc_loss = 0.1860799081882255
Trained batch 432 in epoch 0, gen_loss = 0.962434935239391, disc_loss = 0.18612026138441942
Trained batch 433 in epoch 0, gen_loss = 0.9622060480755046, disc_loss = 0.18606428962527058
Trained batch 434 in epoch 0, gen_loss = 0.9618058766441784, disc_loss = 0.18597040542069523
Trained batch 435 in epoch 0, gen_loss = 0.9617443285677412, disc_loss = 0.18578901778428106
Trained batch 436 in epoch 0, gen_loss = 0.9614205135360735, disc_loss = 0.18572662151003047
Trained batch 437 in epoch 0, gen_loss = 0.961728519364579, disc_loss = 0.1855210180577214
Trained batch 438 in epoch 0, gen_loss = 0.9620151312041663, disc_loss = 0.1852574264524602
Trained batch 439 in epoch 0, gen_loss = 0.9618293172933838, disc_loss = 0.18502282035452397
Trained batch 440 in epoch 0, gen_loss = 0.9634495115334214, disc_loss = 0.184984811638691
Trained batch 441 in epoch 0, gen_loss = 0.9633997956552117, disc_loss = 0.18473234790381532
Trained batch 442 in epoch 0, gen_loss = 0.9644324044879887, disc_loss = 0.18436401979850592
Trained batch 443 in epoch 0, gen_loss = 0.96608541677664, disc_loss = 0.18406579474004003
Trained batch 444 in epoch 0, gen_loss = 0.9669287869099821, disc_loss = 0.1837473105346219
Trained batch 445 in epoch 0, gen_loss = 0.9675225292086067, disc_loss = 0.18343099542463306
Trained batch 446 in epoch 0, gen_loss = 0.968518538763059, disc_loss = 0.18308506604342387
Trained batch 447 in epoch 0, gen_loss = 0.9693265160811799, disc_loss = 0.18273797662862176
Trained batch 448 in epoch 0, gen_loss = 0.9694899049792896, disc_loss = 0.18253474878375409
Trained batch 449 in epoch 0, gen_loss = 0.9721384845839607, disc_loss = 0.18243197585145632
Trained batch 450 in epoch 0, gen_loss = 0.9712159726149227, disc_loss = 0.18268746948757617
Trained batch 451 in epoch 0, gen_loss = 0.9721931538223165, disc_loss = 0.18237823531427214
Trained batch 452 in epoch 0, gen_loss = 0.9728671833379379, disc_loss = 0.18205456193906583
Trained batch 453 in epoch 0, gen_loss = 0.973170123698953, disc_loss = 0.18172070077140426
Trained batch 454 in epoch 0, gen_loss = 0.9731759259988974, disc_loss = 0.18151503259336554
Trained batch 455 in epoch 0, gen_loss = 0.9734286969168144, disc_loss = 0.18123578188712136
Trained batch 456 in epoch 0, gen_loss = 0.9752727551324624, disc_loss = 0.18096118166847094
Trained batch 457 in epoch 0, gen_loss = 0.9749170881431696, disc_loss = 0.18085097364560745
Trained batch 458 in epoch 0, gen_loss = 0.9753269386966763, disc_loss = 0.18066027556791545
Trained batch 459 in epoch 0, gen_loss = 0.9754694831112157, disc_loss = 0.1803669296124059
Trained batch 460 in epoch 0, gen_loss = 0.9759158523118936, disc_loss = 0.18002894899753324
Trained batch 461 in epoch 0, gen_loss = 0.9761509215418911, disc_loss = 0.17976361305908464
Trained batch 462 in epoch 0, gen_loss = 0.9768583428267527, disc_loss = 0.17974407107571289
Trained batch 463 in epoch 0, gen_loss = 0.9763856184636724, disc_loss = 0.17963498560230023
Trained batch 464 in epoch 0, gen_loss = 0.9772057301254683, disc_loss = 0.17934234769594284
Trained batch 465 in epoch 0, gen_loss = 0.97718477313099, disc_loss = 0.1793432781161029
Trained batch 466 in epoch 0, gen_loss = 0.9775143028326667, disc_loss = 0.1790242781842674
Trained batch 467 in epoch 0, gen_loss = 0.9778665952448152, disc_loss = 0.17887068992783117
Trained batch 468 in epoch 0, gen_loss = 0.9772286500249591, disc_loss = 0.1789149990611112
Trained batch 469 in epoch 0, gen_loss = 0.9784665509741357, disc_loss = 0.17925544790923595
Trained batch 470 in epoch 0, gen_loss = 0.9775017026629924, disc_loss = 0.1794475778035677
Trained batch 471 in epoch 0, gen_loss = 0.9779397067377122, disc_loss = 0.17939370375726435
Trained batch 472 in epoch 0, gen_loss = 0.9793581592104148, disc_loss = 0.17924154957020005
Trained batch 473 in epoch 0, gen_loss = 0.9790990322451049, disc_loss = 0.17907970515767482
Trained batch 474 in epoch 0, gen_loss = 0.9786318327251233, disc_loss = 0.17900234762775272
Trained batch 475 in epoch 0, gen_loss = 0.9796348459580365, disc_loss = 0.17922200037933447
Trained batch 476 in epoch 0, gen_loss = 0.9798572343350457, disc_loss = 0.1789422370962002
Trained batch 477 in epoch 0, gen_loss = 0.9794473705431408, disc_loss = 0.17888723877278082
Trained batch 478 in epoch 0, gen_loss = 0.9795044745184435, disc_loss = 0.17861204433385167
Trained batch 479 in epoch 0, gen_loss = 0.9801278866827488, disc_loss = 0.17831598268821836
Trained batch 480 in epoch 0, gen_loss = 0.9806106920797463, disc_loss = 0.17834824419690765
Trained batch 481 in epoch 0, gen_loss = 0.9795999891159446, disc_loss = 0.1784881228789749
Trained batch 482 in epoch 0, gen_loss = 0.9798138092887081, disc_loss = 0.17821934357223798
Trained batch 483 in epoch 0, gen_loss = 0.9818558444784693, disc_loss = 0.17815921973621796
Trained batch 484 in epoch 0, gen_loss = 0.9818237897661544, disc_loss = 0.17794622306510346
Trained batch 485 in epoch 0, gen_loss = 0.9811768296937393, disc_loss = 0.17790369539993045
Trained batch 486 in epoch 0, gen_loss = 0.9815894556853316, disc_loss = 0.17781830006097377
Trained batch 487 in epoch 0, gen_loss = 0.9820825433755507, disc_loss = 0.177577631257963
Trained batch 488 in epoch 0, gen_loss = 0.9817381435986189, disc_loss = 0.17762785993044117
Trained batch 489 in epoch 0, gen_loss = 0.9813790549429096, disc_loss = 0.17750026331264146
Trained batch 490 in epoch 0, gen_loss = 0.981884005538315, disc_loss = 0.17731791282501339
Trained batch 491 in epoch 0, gen_loss = 0.9822471591878713, disc_loss = 0.17740151531449178
Trained batch 492 in epoch 0, gen_loss = 0.9813715836701964, disc_loss = 0.17762620354158157
Trained batch 493 in epoch 0, gen_loss = 0.9809305649175335, disc_loss = 0.1775530699474609
Trained batch 494 in epoch 0, gen_loss = 0.9810745516810754, disc_loss = 0.17742734096869073
Trained batch 495 in epoch 0, gen_loss = 0.981511844802768, disc_loss = 0.17727471638711229
Trained batch 496 in epoch 0, gen_loss = 0.9814838073263226, disc_loss = 0.17709508024051157
Trained batch 497 in epoch 0, gen_loss = 0.9811781638238325, disc_loss = 0.17698134833490514
Trained batch 498 in epoch 0, gen_loss = 0.9816844404221536, disc_loss = 0.1767955396570758
Trained batch 499 in epoch 0, gen_loss = 0.9815347092747688, disc_loss = 0.17679314808547497
Trained batch 500 in epoch 0, gen_loss = 0.9807194174525743, disc_loss = 0.17684209499946849
Trained batch 501 in epoch 0, gen_loss = 0.9807454645040026, disc_loss = 0.17673909217416053
Trained batch 502 in epoch 0, gen_loss = 0.9809989505567797, disc_loss = 0.17660909471940806
Trained batch 503 in epoch 0, gen_loss = 0.981182280573107, disc_loss = 0.17644675785586947
Trained batch 504 in epoch 0, gen_loss = 0.981061694940718, disc_loss = 0.17639642192585633
Trained batch 505 in epoch 0, gen_loss = 0.9804815981581277, disc_loss = 0.1763241008336365
Trained batch 506 in epoch 0, gen_loss = 0.9811254370024454, disc_loss = 0.17631236391777588
Trained batch 507 in epoch 0, gen_loss = 0.980761876901773, disc_loss = 0.17628931312814472
Trained batch 508 in epoch 0, gen_loss = 0.9803514448387918, disc_loss = 0.17620434745238667
Trained batch 509 in epoch 0, gen_loss = 0.9808103482512867, disc_loss = 0.1760173887014389
Trained batch 510 in epoch 0, gen_loss = 0.9812173013467844, disc_loss = 0.17610714994298035
Trained batch 511 in epoch 0, gen_loss = 0.980258786235936, disc_loss = 0.1763726855861023
Trained batch 512 in epoch 0, gen_loss = 0.9797690619966904, disc_loss = 0.17638486658620556
Trained batch 513 in epoch 0, gen_loss = 0.9807695543487712, disc_loss = 0.17706512303203925
Trained batch 514 in epoch 0, gen_loss = 0.9810214048450433, disc_loss = 0.1768112162073839
Trained batch 515 in epoch 0, gen_loss = 0.980452995776206, disc_loss = 0.17674171794639076
Trained batch 516 in epoch 0, gen_loss = 0.9800238216176946, disc_loss = 0.17666074454899683
Trained batch 517 in epoch 0, gen_loss = 0.9804689171921792, disc_loss = 0.17678346185725627
Trained batch 518 in epoch 0, gen_loss = 0.9800832459922011, disc_loss = 0.17676544361720883
Trained batch 519 in epoch 0, gen_loss = 0.9800151807757524, disc_loss = 0.17663236435216206
Trained batch 520 in epoch 0, gen_loss = 0.9800227467478351, disc_loss = 0.17679782755201015
Trained batch 521 in epoch 0, gen_loss = 0.9796723535006073, disc_loss = 0.1767057170413463
Trained batch 522 in epoch 0, gen_loss = 0.9793997344277785, disc_loss = 0.17665248889882068
Trained batch 523 in epoch 0, gen_loss = 0.9794400821205314, disc_loss = 0.1766146298298854
Trained batch 524 in epoch 0, gen_loss = 0.9792906401270912, disc_loss = 0.1765657294080371
Trained batch 525 in epoch 0, gen_loss = 0.9787999417618654, disc_loss = 0.17651783082875008
Trained batch 526 in epoch 0, gen_loss = 0.9784565812270148, disc_loss = 0.1763774649128742
Trained batch 527 in epoch 0, gen_loss = 0.9786658798429099, disc_loss = 0.17632396191810118
Trained batch 528 in epoch 0, gen_loss = 0.9787871424101702, disc_loss = 0.17627310572589955
Trained batch 529 in epoch 0, gen_loss = 0.978127429057967, disc_loss = 0.17621590798755862
Trained batch 530 in epoch 0, gen_loss = 0.9780899767821791, disc_loss = 0.17605362544906117
Trained batch 531 in epoch 0, gen_loss = 0.9789744475506302, disc_loss = 0.17598643037665607
Trained batch 532 in epoch 0, gen_loss = 0.978832449854874, disc_loss = 0.17582444540499298
Trained batch 533 in epoch 0, gen_loss = 0.9782048808724693, disc_loss = 0.17581255154942305
Trained batch 534 in epoch 0, gen_loss = 0.9781580585185613, disc_loss = 0.1757284313579586
Trained batch 535 in epoch 0, gen_loss = 0.97869593583381, disc_loss = 0.17578462015059013
Trained batch 536 in epoch 0, gen_loss = 0.9788709431609184, disc_loss = 0.17561646345551898
Trained batch 537 in epoch 0, gen_loss = 0.978501349576787, disc_loss = 0.17560648207715454
Trained batch 538 in epoch 0, gen_loss = 0.978697370067377, disc_loss = 0.17553299440136647
Trained batch 539 in epoch 0, gen_loss = 0.978784230682585, disc_loss = 0.1754978477816891
Trained batch 540 in epoch 0, gen_loss = 0.9784457004885576, disc_loss = 0.175366626250215
Trained batch 541 in epoch 0, gen_loss = 0.9782729331417718, disc_loss = 0.17529011838055625
Trained batch 542 in epoch 0, gen_loss = 0.9776914440246357, disc_loss = 0.17516653553941192
Trained batch 543 in epoch 0, gen_loss = 0.9785385504364967, disc_loss = 0.17507255511522732
Trained batch 544 in epoch 0, gen_loss = 0.9782512272169831, disc_loss = 0.17520406335045438
Trained batch 545 in epoch 0, gen_loss = 0.9775681958530412, disc_loss = 0.1751457315361325
Trained batch 546 in epoch 0, gen_loss = 0.9777999608564638, disc_loss = 0.1750337853346708
Trained batch 547 in epoch 0, gen_loss = 0.9782444556264112, disc_loss = 0.1750354671674053
Trained batch 548 in epoch 0, gen_loss = 0.9778668034706394, disc_loss = 0.17485447145170635
Trained batch 549 in epoch 0, gen_loss = 0.9778981935977936, disc_loss = 0.17474120329726825
Trained batch 550 in epoch 0, gen_loss = 0.9777014831016804, disc_loss = 0.17462996068915956
Trained batch 551 in epoch 0, gen_loss = 0.9781415695729463, disc_loss = 0.17454258923459312
Trained batch 552 in epoch 0, gen_loss = 0.9779717328319929, disc_loss = 0.1743774193643877
Trained batch 553 in epoch 0, gen_loss = 0.977918753017157, disc_loss = 0.1741552277390815
Trained batch 554 in epoch 0, gen_loss = 0.9781295818251532, disc_loss = 0.1742407743793887
Trained batch 555 in epoch 0, gen_loss = 0.9778231527093503, disc_loss = 0.17413234774344902
Trained batch 556 in epoch 0, gen_loss = 0.9783448307261646, disc_loss = 0.17389617550656655
Trained batch 557 in epoch 0, gen_loss = 0.9784224365560812, disc_loss = 0.1737034066803887
Trained batch 558 in epoch 0, gen_loss = 0.9781849070823683, disc_loss = 0.1736448842945487
Trained batch 559 in epoch 0, gen_loss = 0.9785165224756514, disc_loss = 0.17345519977887827
Trained batch 560 in epoch 0, gen_loss = 0.9787688667548956, disc_loss = 0.17320881626583676
Trained batch 561 in epoch 0, gen_loss = 0.9791332426020259, disc_loss = 0.1730526751139304
Trained batch 562 in epoch 0, gen_loss = 0.979210319358121, disc_loss = 0.17283683708913586
Trained batch 563 in epoch 0, gen_loss = 0.9789319332187058, disc_loss = 0.17277459290988267
Trained batch 564 in epoch 0, gen_loss = 0.9791147147659707, disc_loss = 0.17278755183900352
Trained batch 565 in epoch 0, gen_loss = 0.9788760239580916, disc_loss = 0.1726555499731968
Trained batch 566 in epoch 0, gen_loss = 0.9797993242425262, disc_loss = 0.1725522076918019
Trained batch 567 in epoch 0, gen_loss = 0.9796024277596407, disc_loss = 0.17240147550367343
Trained batch 568 in epoch 0, gen_loss = 0.9802672131828348, disc_loss = 0.1722414507244195
Trained batch 569 in epoch 0, gen_loss = 0.979682547586006, disc_loss = 0.17218258444285184
Trained batch 570 in epoch 0, gen_loss = 0.9806781036423718, disc_loss = 0.17209101227583484
Trained batch 571 in epoch 0, gen_loss = 0.9803124737072658, disc_loss = 0.17196523586379273
Trained batch 572 in epoch 0, gen_loss = 0.9807691199617236, disc_loss = 0.17173593653200184
Trained batch 573 in epoch 0, gen_loss = 0.9806896937225754, disc_loss = 0.17156487248370664
Trained batch 574 in epoch 0, gen_loss = 0.9810471925528154, disc_loss = 0.17163859931671102
Trained batch 575 in epoch 0, gen_loss = 0.9803969967696402, disc_loss = 0.17191522505729356
Trained batch 576 in epoch 0, gen_loss = 0.980147850162235, disc_loss = 0.17191294490955317
Trained batch 577 in epoch 0, gen_loss = 0.9811292536118451, disc_loss = 0.17213704010993758
Trained batch 578 in epoch 0, gen_loss = 0.980511109651885, disc_loss = 0.17218270351618065
Trained batch 579 in epoch 0, gen_loss = 0.9803213462747377, disc_loss = 0.17216476263028793
Trained batch 580 in epoch 0, gen_loss = 0.9804357790905924, disc_loss = 0.1725481695170144
Trained batch 581 in epoch 0, gen_loss = 0.980744398131813, disc_loss = 0.1723859061349401
Trained batch 582 in epoch 0, gen_loss = 0.9798539469008993, disc_loss = 0.17259731347549637
Trained batch 583 in epoch 0, gen_loss = 0.9799877453136118, disc_loss = 0.17258893484760024
Trained batch 584 in epoch 0, gen_loss = 0.9797563191153046, disc_loss = 0.17278890753021606
Trained batch 585 in epoch 0, gen_loss = 0.9792862993255004, disc_loss = 0.17271934073965095
Trained batch 586 in epoch 0, gen_loss = 0.9790090460590447, disc_loss = 0.1726488981338843
Trained batch 587 in epoch 0, gen_loss = 0.9793526053834124, disc_loss = 0.17254810603525567
Trained batch 588 in epoch 0, gen_loss = 0.9795880969392423, disc_loss = 0.1724895470932202
Trained batch 589 in epoch 0, gen_loss = 0.9797517687587415, disc_loss = 0.17226329681353045
Trained batch 590 in epoch 0, gen_loss = 0.9793576612286963, disc_loss = 0.17220077325169206
Trained batch 591 in epoch 0, gen_loss = 0.9801998266295807, disc_loss = 0.1720512894019988
Trained batch 592 in epoch 0, gen_loss = 0.9798933646530165, disc_loss = 0.17189764215738135
Trained batch 593 in epoch 0, gen_loss = 0.979739696991564, disc_loss = 0.1717727154129583
Trained batch 594 in epoch 0, gen_loss = 0.9800768017768859, disc_loss = 0.1716021284899291
Trained batch 595 in epoch 0, gen_loss = 0.9798682333038957, disc_loss = 0.17155955214953464
Trained batch 596 in epoch 0, gen_loss = 0.9802672508573612, disc_loss = 0.17133419590080204
Trained batch 597 in epoch 0, gen_loss = 0.9804230666280191, disc_loss = 0.17131895173253822
Trained batch 598 in epoch 0, gen_loss = 0.9807296679891608, disc_loss = 0.17112930827402512
Testing Epoch 0
Traceback (most recent call last):
  File "srgan_bones.py", line 314, in <module>
    loss_G = loss_content + oss_GAN #loss_content + 1e-3 * loss_GAN
NameError: name 'oss_GAN' is not defined