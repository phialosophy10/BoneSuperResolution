/work3/soeba/HALOS/halos/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.
  warnings.warn(
/work3/soeba/HALOS/halos/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Training Epoch 0
Trained batch 1144 in epoch 0, gen_loss = 0.9521800776496204, disc_loss = 0.02802662313201538
Testing Epoch 0
Training Epoch 1
Trained batch 1144 in epoch 1, gen_loss = 0.8906668300711953, disc_loss = 0.006547071901912594
Testing Epoch 1
Training Epoch 2
Trained batch 1144 in epoch 2, gen_loss = 0.8627847061927663, disc_loss = 0.0004843233018507571
Testing Epoch 2
Training Epoch 3
Trained batch 1144 in epoch 3, gen_loss = 0.8563081836075762, disc_loss = 0.0006641964741533126
Testing Epoch 3
Training Epoch 4
Trained batch 1144 in epoch 4, gen_loss = 0.8575963998465559, disc_loss = 0.0044821993446505755
Testing Epoch 4
Training Epoch 5
Trained batch 1144 in epoch 5, gen_loss = 0.8710134130898521, disc_loss = 0.019475298308543443
Testing Epoch 5
Training Epoch 6
Trained batch 1144 in epoch 6, gen_loss = 0.8624354377584166, disc_loss = 0.012006843662888114
Testing Epoch 6
Training Epoch 7
Trained batch 1144 in epoch 7, gen_loss = 0.6781954355114933, disc_loss = 0.140931572616263
Testing Epoch 7
Training Epoch 8
Trained batch 1144 in epoch 8, gen_loss = 0.7230132133159054, disc_loss = 0.10470934642730972
Testing Epoch 8
Training Epoch 9
Trained batch 1144 in epoch 9, gen_loss = 0.7129122749947044, disc_loss = 0.10578959420909007
Testing Epoch 9
Training Epoch 10
Trained batch 1144 in epoch 10, gen_loss = 0.7076335201877694, disc_loss = 0.11189884174550074
Testing Epoch 10
Training Epoch 11
Trained batch 1144 in epoch 11, gen_loss = 0.71397895258587, disc_loss = 0.10681145999125127
Testing Epoch 11
Training Epoch 12
Trained batch 1144 in epoch 12, gen_loss = 0.7409864083387966, disc_loss = 0.09241239218830281
Testing Epoch 12
Training Epoch 13
Trained batch 1144 in epoch 13, gen_loss = 0.7303308486678194, disc_loss = 0.09719126361898069
Testing Epoch 13
Training Epoch 14
Trained batch 1144 in epoch 14, gen_loss = 0.7306835227658134, disc_loss = 0.09455107165557458
Testing Epoch 14
Training Epoch 15
Trained batch 1144 in epoch 15, gen_loss = 0.7256745442032293, disc_loss = 0.09711992257238085
Testing Epoch 15
Training Epoch 16
Trained batch 1144 in epoch 16, gen_loss = 0.7466645830323082, disc_loss = 0.08562902442980802
Testing Epoch 16
Training Epoch 17
Trained batch 1144 in epoch 17, gen_loss = 0.7533325426443175, disc_loss = 0.0843170015182037
Testing Epoch 17
Training Epoch 18
Trained batch 1144 in epoch 18, gen_loss = 0.7593387334107311, disc_loss = 0.07663568192564243
Testing Epoch 18
Training Epoch 19
Trained batch 1144 in epoch 19, gen_loss = 0.7685251421282906, disc_loss = 0.0768955810678773
Testing Epoch 19
Training Epoch 20
Trained batch 1144 in epoch 20, gen_loss = 0.7817898939790684, disc_loss = 0.06894973762756351
Testing Epoch 20
/work3/soeba/HALOS/utils.py:112: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, axs = plt.subplots(batch_size, 4, figsize=(8,10))
Training Epoch 21
Trained batch 1144 in epoch 21, gen_loss = 0.7835194845386988, disc_loss = 0.06526230277745744
Testing Epoch 21
Training Epoch 22
Trained batch 1144 in epoch 22, gen_loss = 0.7841203467814682, disc_loss = 0.06502063736140598
Testing Epoch 22
Training Epoch 23
Trained batch 1144 in epoch 23, gen_loss = 0.7964780142473862, disc_loss = 0.058594960275647034
Testing Epoch 23
Training Epoch 24
Trained batch 1144 in epoch 24, gen_loss = 0.7886094183120145, disc_loss = 0.06225305078998533
Testing Epoch 24