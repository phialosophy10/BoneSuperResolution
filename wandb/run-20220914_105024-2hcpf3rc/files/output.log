/work3/soeba/HALOS/halos/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.
  warnings.warn(
/work3/soeba/HALOS/halos/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Training Epoch 0
Trained batch 0 in epoch 0, gen_loss = 0.8947851061820984, disc_loss = 0.7669030427932739
Trained batch 1 in epoch 0, gen_loss = 1.0948872864246368, disc_loss = 0.6513469219207764
Trained batch 2 in epoch 0, gen_loss = 1.0042240818341572, disc_loss = 0.5925628542900085
Trained batch 3 in epoch 0, gen_loss = 0.9881343692541122, disc_loss = 0.5998894721269608
Trained batch 4 in epoch 0, gen_loss = 0.9458374500274658, disc_loss = 0.5775132060050965
Trained batch 5 in epoch 0, gen_loss = 0.9331535001595815, disc_loss = 0.5478950440883636
Trained batch 6 in epoch 0, gen_loss = 0.8909345439502171, disc_loss = 0.5437134248869759
Trained batch 7 in epoch 0, gen_loss = 0.875044621527195, disc_loss = 0.5471639409661293
Trained batch 8 in epoch 0, gen_loss = 0.8543089893129137, disc_loss = 0.509744256734848
Trained batch 9 in epoch 0, gen_loss = 0.832119631767273, disc_loss = 0.48650471270084383
Trained batch 10 in epoch 0, gen_loss = 0.8255028833042491, disc_loss = 0.46710572188550775
Trained batch 11 in epoch 0, gen_loss = 0.8100252548853556, disc_loss = 0.4426850154995918
Trained batch 12 in epoch 0, gen_loss = 0.7928205728530884, disc_loss = 0.4218469789394966
Trained batch 13 in epoch 0, gen_loss = 0.7924854712826865, disc_loss = 0.40684719170842853
Trained batch 14 in epoch 0, gen_loss = 0.7832555135091146, disc_loss = 0.38997897307078044
Trained batch 15 in epoch 0, gen_loss = 0.7867988869547844, disc_loss = 0.37615179270505905
Trained batch 16 in epoch 0, gen_loss = 0.776485891903148, disc_loss = 0.36173943386358376
Trained batch 17 in epoch 0, gen_loss = 0.7659514182143741, disc_loss = 0.3534328705734677
Trained batch 18 in epoch 0, gen_loss = 0.7708574157012137, disc_loss = 0.34644524831520884
Trained batch 19 in epoch 0, gen_loss = 0.760456657409668, disc_loss = 0.33697776719927786
Trained batch 20 in epoch 0, gen_loss = 0.7611998660223824, disc_loss = 0.3287204539492017
Trained batch 21 in epoch 0, gen_loss = 0.7606091011654247, disc_loss = 0.32228550789031113
Trained batch 22 in epoch 0, gen_loss = 0.756176699762759, disc_loss = 0.31271328712287155
Trained batch 23 in epoch 0, gen_loss = 0.7487857143084208, disc_loss = 0.3054153605674704
Trained batch 24 in epoch 0, gen_loss = 0.741098186969757, disc_loss = 0.29754546195268633
Trained batch 25 in epoch 0, gen_loss = 0.7390354023529933, disc_loss = 0.28990652641424763
Trained batch 26 in epoch 0, gen_loss = 0.7368773332348576, disc_loss = 0.28379293117258286
Trained batch 27 in epoch 0, gen_loss = 0.7294669300317764, disc_loss = 0.2784575399543558
Trained batch 28 in epoch 0, gen_loss = 0.7240126153518414, disc_loss = 0.2778916189382816
Trained batch 29 in epoch 0, gen_loss = 0.7334013640880584, disc_loss = 0.27320363769928613
Trained batch 30 in epoch 0, gen_loss = 0.7310549501449831, disc_loss = 0.2682676245608637
Trained batch 31 in epoch 0, gen_loss = 0.7373520489782095, disc_loss = 0.26419124682433903
Trained batch 32 in epoch 0, gen_loss = 0.7338619448921897, disc_loss = 0.2595869463050004
Trained batch 33 in epoch 0, gen_loss = 0.7336320842013639, disc_loss = 0.2553433734266197
Trained batch 34 in epoch 0, gen_loss = 0.7298691477094378, disc_loss = 0.2501852791224207
Trained batch 35 in epoch 0, gen_loss = 0.7296954310602612, disc_loss = 0.24557269675036272
Trained batch 36 in epoch 0, gen_loss = 0.7282497608983839, disc_loss = 0.24112354541147077
Trained batch 37 in epoch 0, gen_loss = 0.7286220192909241, disc_loss = 0.2364700120922766
Trained batch 38 in epoch 0, gen_loss = 0.7292909484643203, disc_loss = 0.23196018754671782
Trained batch 39 in epoch 0, gen_loss = 0.7255349725484848, disc_loss = 0.23062851447612048
Trained batch 40 in epoch 0, gen_loss = 0.7248190016281314, disc_loss = 0.22716590962031993
Trained batch 41 in epoch 0, gen_loss = 0.7252961312021527, disc_loss = 0.2232160476878995
Trained batch 42 in epoch 0, gen_loss = 0.7229759097099304, disc_loss = 0.21939730271697044
Trained batch 43 in epoch 0, gen_loss = 0.7217840389771895, disc_loss = 0.2157882210713896
Trained batch 44 in epoch 0, gen_loss = 0.7225994772381252, disc_loss = 0.21268539271420903
Trained batch 45 in epoch 0, gen_loss = 0.7217817837777345, disc_loss = 0.2098674467238395
Trained batch 46 in epoch 0, gen_loss = 0.7204368165198792, disc_loss = 0.20780440507099984
Trained batch 47 in epoch 0, gen_loss = 0.7211259454488754, disc_loss = 0.2047355262717853
Trained batch 48 in epoch 0, gen_loss = 0.7179982552723009, disc_loss = 0.20177503500361832
Trained batch 49 in epoch 0, gen_loss = 0.7161706984043121, disc_loss = 0.1988947031646967
Trained batch 50 in epoch 0, gen_loss = 0.7157487530334323, disc_loss = 0.19595526845431796
Trained batch 51 in epoch 0, gen_loss = 0.7132307634903834, disc_loss = 0.193459080818754
Trained batch 52 in epoch 0, gen_loss = 0.7115582385153141, disc_loss = 0.1910575106739998
Trained batch 53 in epoch 0, gen_loss = 0.7131594993450023, disc_loss = 0.18857469706347696
Trained batch 54 in epoch 0, gen_loss = 0.7124506625262174, disc_loss = 0.1860993613573638
Trained batch 55 in epoch 0, gen_loss = 0.7118808935795512, disc_loss = 0.18449299842385308
Trained batch 56 in epoch 0, gen_loss = 0.7136950346461514, disc_loss = 0.18277551606297493
Trained batch 57 in epoch 0, gen_loss = 0.7143261494307682, disc_loss = 0.18048506442072063
Trained batch 58 in epoch 0, gen_loss = 0.7124420505459026, disc_loss = 0.17845990641389864
Trained batch 59 in epoch 0, gen_loss = 0.7107180605332056, disc_loss = 0.17680643952141206
Trained batch 60 in epoch 0, gen_loss = 0.7103482201451161, disc_loss = 0.17461293428891994
Trained batch 61 in epoch 0, gen_loss = 0.7101883330652791, disc_loss = 0.1724802257433053
Trained batch 62 in epoch 0, gen_loss = 0.7099037312325978, disc_loss = 0.17037258338597086
Trained batch 63 in epoch 0, gen_loss = 0.7069063587114215, disc_loss = 0.16862609091913328
Trained batch 64 in epoch 0, gen_loss = 0.7054590362768907, disc_loss = 0.16702490970492362
Trained batch 65 in epoch 0, gen_loss = 0.7073191783644937, disc_loss = 0.16566859292938854
Trained batch 66 in epoch 0, gen_loss = 0.7085602870628015, disc_loss = 0.16383490628048555
Trained batch 67 in epoch 0, gen_loss = 0.7070624372538399, disc_loss = 0.1620303882724222
Trained batch 68 in epoch 0, gen_loss = 0.7061683993408645, disc_loss = 0.16025100481035054
Trained batch 69 in epoch 0, gen_loss = 0.7061839801924569, disc_loss = 0.1586666968784162
Trained batch 70 in epoch 0, gen_loss = 0.7066669111520472, disc_loss = 0.15700062993966357
Trained batch 71 in epoch 0, gen_loss = 0.707522796259986, disc_loss = 0.15531725622713566
Trained batch 72 in epoch 0, gen_loss = 0.7085751752330832, disc_loss = 0.15363674911938302
Trained batch 73 in epoch 0, gen_loss = 0.7090188868947931, disc_loss = 0.15198143229291244
Trained batch 74 in epoch 0, gen_loss = 0.7084412240982055, disc_loss = 0.1504515959819158
Trained batch 75 in epoch 0, gen_loss = 0.7066968240235981, disc_loss = 0.1490918779255528
Trained batch 76 in epoch 0, gen_loss = 0.7058510989337773, disc_loss = 0.147666687121639
Trained batch 77 in epoch 0, gen_loss = 0.7052166622418624, disc_loss = 0.14632189741883522
Trained batch 78 in epoch 0, gen_loss = 0.7053033732160737, disc_loss = 0.14489959156777285
Trained batch 79 in epoch 0, gen_loss = 0.7063691884279251, disc_loss = 0.143696380360052
Trained batch 80 in epoch 0, gen_loss = 0.7055118120746848, disc_loss = 0.1427812482011907
Trained batch 81 in epoch 0, gen_loss = 0.7057029946548182, disc_loss = 0.14169129688383603
Trained batch 82 in epoch 0, gen_loss = 0.7062966629683253, disc_loss = 0.14044896724173822
Trained batch 83 in epoch 0, gen_loss = 0.7071016033490499, disc_loss = 0.1391523652488277
Trained batch 84 in epoch 0, gen_loss = 0.7062921958811142, disc_loss = 0.13780418925863855
Trained batch 85 in epoch 0, gen_loss = 0.7056542364663856, disc_loss = 0.13652316864233377
Trained batch 86 in epoch 0, gen_loss = 0.7035964151908611, disc_loss = 0.135325838629713
Trained batch 87 in epoch 0, gen_loss = 0.7023633122444153, disc_loss = 0.1347402788233012
Trained batch 88 in epoch 0, gen_loss = 0.701783440086279, disc_loss = 0.1337468804310212
Trained batch 89 in epoch 0, gen_loss = 0.7013911916149987, disc_loss = 0.13283320609480143
Trained batch 90 in epoch 0, gen_loss = 0.7009522312290066, disc_loss = 0.13184004483724032
Trained batch 91 in epoch 0, gen_loss = 0.6989626029263372, disc_loss = 0.13076921850037965
Trained batch 92 in epoch 0, gen_loss = 0.6985891724145541, disc_loss = 0.12997290786475904
Trained batch 93 in epoch 0, gen_loss = 0.6981033784277896, disc_loss = 0.1288259517837395
Trained batch 94 in epoch 0, gen_loss = 0.6987692136513559, disc_loss = 0.12784866947484644
Trained batch 95 in epoch 0, gen_loss = 0.698024882003665, disc_loss = 0.12679425042976314
Trained batch 96 in epoch 0, gen_loss = 0.6973889039963791, disc_loss = 0.12571005544327585
Trained batch 97 in epoch 0, gen_loss = 0.6960432170605173, disc_loss = 0.12466180277037986
Trained batch 98 in epoch 0, gen_loss = 0.6959787631275678, disc_loss = 0.12371248076434689
Trained batch 99 in epoch 0, gen_loss = 0.6942622780799865, disc_loss = 0.12263143316842616
Trained batch 100 in epoch 0, gen_loss = 0.6940492326670354, disc_loss = 0.12168889473386035
Trained batch 101 in epoch 0, gen_loss = 0.6929153466925901, disc_loss = 0.12067532319319892
Trained batch 102 in epoch 0, gen_loss = 0.6920692550325857, disc_loss = 0.11973029761168275
Trained batch 103 in epoch 0, gen_loss = 0.6907870116142126, disc_loss = 0.11874474299391015
Trained batch 104 in epoch 0, gen_loss = 0.6892171905154274, disc_loss = 0.11782610523736194
Trained batch 105 in epoch 0, gen_loss = 0.6881984157382317, disc_loss = 0.11688947991274719
Trained batch 106 in epoch 0, gen_loss = 0.6876993886778288, disc_loss = 0.11620205471065835
Trained batch 107 in epoch 0, gen_loss = 0.686961246309457, disc_loss = 0.1153322416788864
Trained batch 108 in epoch 0, gen_loss = 0.6863080202986341, disc_loss = 0.11458126546968835
Trained batch 109 in epoch 0, gen_loss = 0.6872569062493065, disc_loss = 0.11381727835502137
Trained batch 110 in epoch 0, gen_loss = 0.6877042930405419, disc_loss = 0.11306230280011355
Trained batch 111 in epoch 0, gen_loss = 0.6873709953257016, disc_loss = 0.11223108498545896
Trained batch 112 in epoch 0, gen_loss = 0.6873592940051999, disc_loss = 0.11141957089601628
Trained batch 113 in epoch 0, gen_loss = 0.6871831103375083, disc_loss = 0.11067296326846669
Trained batch 114 in epoch 0, gen_loss = 0.6869114927623583, disc_loss = 0.10988796512879755
Trained batch 115 in epoch 0, gen_loss = 0.6856588053292242, disc_loss = 0.10912094937219959
Trained batch 116 in epoch 0, gen_loss = 0.6854037866633163, disc_loss = 0.10844748781627825
Trained batch 117 in epoch 0, gen_loss = 0.685139621213331, disc_loss = 0.10768408960489145
Trained batch 118 in epoch 0, gen_loss = 0.6855023538365084, disc_loss = 0.10697399115017733
Trained batch 119 in epoch 0, gen_loss = 0.6846227531631788, disc_loss = 0.10623785984547188
Trained batch 120 in epoch 0, gen_loss = 0.6844689683480696, disc_loss = 0.10550985024272164
Trained batch 121 in epoch 0, gen_loss = 0.6835385155482371, disc_loss = 0.10485081193724373
Trained batch 122 in epoch 0, gen_loss = 0.6827122976140279, disc_loss = 0.10413106745184679
Trained batch 123 in epoch 0, gen_loss = 0.6818418276886786, disc_loss = 0.10343781139911903
Trained batch 124 in epoch 0, gen_loss = 0.6802973470687866, disc_loss = 0.10294762923568487
Trained batch 125 in epoch 0, gen_loss = 0.6806212876524244, disc_loss = 0.10236645150663597
Trained batch 126 in epoch 0, gen_loss = 0.679565573301841, disc_loss = 0.10191048157408716
Trained batch 127 in epoch 0, gen_loss = 0.6787562007084489, disc_loss = 0.1012789458836778
Trained batch 128 in epoch 0, gen_loss = 0.6793773585511732, disc_loss = 0.10079006899090469
Trained batch 129 in epoch 0, gen_loss = 0.67869075857676, disc_loss = 0.10026380201228537
Trained batch 130 in epoch 0, gen_loss = 0.6789291391845877, disc_loss = 0.09973329026252031
Trained batch 131 in epoch 0, gen_loss = 0.6782837550748478, disc_loss = 0.09909515860349391
Trained batch 132 in epoch 0, gen_loss = 0.6773203216997304, disc_loss = 0.09846492187309086
Trained batch 133 in epoch 0, gen_loss = 0.6774238939605542, disc_loss = 0.09786265086271424
Trained batch 134 in epoch 0, gen_loss = 0.6761665291256375, disc_loss = 0.09741752281509064
Trained batch 135 in epoch 0, gen_loss = 0.6750130692825598, disc_loss = 0.09702701325637891
Trained batch 136 in epoch 0, gen_loss = 0.6754685579425227, disc_loss = 0.09669263461047281
Trained batch 137 in epoch 0, gen_loss = 0.675124425387037, disc_loss = 0.09614120913750451
Trained batch 138 in epoch 0, gen_loss = 0.6744411587715149, disc_loss = 0.09565402748291012
Trained batch 139 in epoch 0, gen_loss = 0.6740843181099211, disc_loss = 0.09514544341447098
Trained batch 140 in epoch 0, gen_loss = 0.6734692382474318, disc_loss = 0.09458352061292381
Trained batch 141 in epoch 0, gen_loss = 0.673484657431992, disc_loss = 0.09404503864983857
Trained batch 142 in epoch 0, gen_loss = 0.6731490901300123, disc_loss = 0.09353758877963869
Trained batch 143 in epoch 0, gen_loss = 0.6729014470345445, disc_loss = 0.0930316978936187
Trained batch 144 in epoch 0, gen_loss = 0.6725585699081421, disc_loss = 0.09255069525077425
Trained batch 145 in epoch 0, gen_loss = 0.672093100743751, disc_loss = 0.09205115668169439
Trained batch 146 in epoch 0, gen_loss = 0.6717887018002621, disc_loss = 0.09157522519429524
Trained batch 147 in epoch 0, gen_loss = 0.6709696284017047, disc_loss = 0.09112927798383139
Trained batch 148 in epoch 0, gen_loss = 0.6714819305695143, disc_loss = 0.0906642125606937
Trained batch 149 in epoch 0, gen_loss = 0.6711344865957896, disc_loss = 0.09019377247740824
Trained batch 150 in epoch 0, gen_loss = 0.671444083286437, disc_loss = 0.0897138405541908
Trained batch 151 in epoch 0, gen_loss = 0.6706289766650451, disc_loss = 0.08920746463914647
Trained batch 152 in epoch 0, gen_loss = 0.669583876148548, disc_loss = 0.08869815300663117
Trained batch 153 in epoch 0, gen_loss = 0.6689644692006049, disc_loss = 0.08820357492387101
Trained batch 154 in epoch 0, gen_loss = 0.6686720059764001, disc_loss = 0.0877508528230171
Trained batch 155 in epoch 0, gen_loss = 0.6683273789210197, disc_loss = 0.08730096710869709
Trained batch 156 in epoch 0, gen_loss = 0.6681307728882808, disc_loss = 0.08686091541460934
Trained batch 157 in epoch 0, gen_loss = 0.6677912251104282, disc_loss = 0.08642102348252753
Trained batch 158 in epoch 0, gen_loss = 0.6667293062749898, disc_loss = 0.08595549820119855
Trained batch 159 in epoch 0, gen_loss = 0.6668188195675612, disc_loss = 0.08551721900003031
Trained batch 160 in epoch 0, gen_loss = 0.6667832980244797, disc_loss = 0.0850771977305875
Trained batch 161 in epoch 0, gen_loss = 0.6658984974578575, disc_loss = 0.08467523814006536
Trained batch 162 in epoch 0, gen_loss = 0.6651858797102618, disc_loss = 0.08424403335030642
Trained batch 163 in epoch 0, gen_loss = 0.6650891158638931, disc_loss = 0.08384507741756374
Trained batch 164 in epoch 0, gen_loss = 0.6647346326799104, disc_loss = 0.08347356780573274
Trained batch 165 in epoch 0, gen_loss = 0.6648322437182966, disc_loss = 0.08309326877997043
Trained batch 166 in epoch 0, gen_loss = 0.6652099083283705, disc_loss = 0.08268930022967254
Trained batch 167 in epoch 0, gen_loss = 0.6645685842349416, disc_loss = 0.08229158827037152
Trained batch 168 in epoch 0, gen_loss = 0.6639456297518939, disc_loss = 0.08190233143013434
Trained batch 169 in epoch 0, gen_loss = 0.6636755129870247, disc_loss = 0.08151125896941214
Trained batch 170 in epoch 0, gen_loss = 0.6640158215461419, disc_loss = 0.08111926240094922
Trained batch 171 in epoch 0, gen_loss = 0.6636532912420672, disc_loss = 0.0807476113875245
Trained batch 172 in epoch 0, gen_loss = 0.6636356759622607, disc_loss = 0.08039086251761872
Trained batch 173 in epoch 0, gen_loss = 0.6631765163492882, disc_loss = 0.08000627168636212
Trained batch 174 in epoch 0, gen_loss = 0.6629372542245048, disc_loss = 0.07965213256222861
Trained batch 175 in epoch 0, gen_loss = 0.6628654561936855, disc_loss = 0.07928075083658438
Trained batch 176 in epoch 0, gen_loss = 0.6621902894839055, disc_loss = 0.07901520899754796
Trained batch 177 in epoch 0, gen_loss = 0.6625188120295492, disc_loss = 0.07872824999765399
Trained batch 178 in epoch 0, gen_loss = 0.661779831241629, disc_loss = 0.07839957873373392
Trained batch 179 in epoch 0, gen_loss = 0.6614732907878028, disc_loss = 0.07805795347731974
Trained batch 180 in epoch 0, gen_loss = 0.6610613303948503, disc_loss = 0.07770004619468641
Trained batch 181 in epoch 0, gen_loss = 0.6603635231872181, disc_loss = 0.07734678739395279
Trained batch 182 in epoch 0, gen_loss = 0.659342617122202, disc_loss = 0.07705098747331751
Trained batch 183 in epoch 0, gen_loss = 0.6596151157889677, disc_loss = 0.07671140869269552
Trained batch 184 in epoch 0, gen_loss = 0.6586357250406936, disc_loss = 0.07636481353358643
Trained batch 185 in epoch 0, gen_loss = 0.6585119480407366, disc_loss = 0.07602972670468272
Trained batch 186 in epoch 0, gen_loss = 0.6578318618516871, disc_loss = 0.0756713290484672
Trained batch 187 in epoch 0, gen_loss = 0.6582098249742325, disc_loss = 0.07536776674951011
Trained batch 188 in epoch 0, gen_loss = 0.6576852509899745, disc_loss = 0.07502688681361852
Trained batch 189 in epoch 0, gen_loss = 0.6568945246307474, disc_loss = 0.07470997850851793
Trained batch 190 in epoch 0, gen_loss = 0.6566163003444672, disc_loss = 0.07438401565813893
Trained batch 191 in epoch 0, gen_loss = 0.6556522082537413, disc_loss = 0.07417077400411169
Trained batch 192 in epoch 0, gen_loss = 0.6552929306895005, disc_loss = 0.07389525935980322
Trained batch 193 in epoch 0, gen_loss = 0.6543185293059988, disc_loss = 0.07382107539505688
Trained batch 194 in epoch 0, gen_loss = 0.6545335042170989, disc_loss = 0.0735333559413751
Trained batch 195 in epoch 0, gen_loss = 0.6560884206270685, disc_loss = 0.07342586364140924
Trained batch 196 in epoch 0, gen_loss = 0.655708284850048, disc_loss = 0.07316033292542859
Trained batch 197 in epoch 0, gen_loss = 0.6558899677763081, disc_loss = 0.07294262980459014
Trained batch 198 in epoch 0, gen_loss = 0.6550778739116899, disc_loss = 0.07263883117623815
Trained batch 199 in epoch 0, gen_loss = 0.6540596272051334, disc_loss = 0.07255105944816023
Trained batch 200 in epoch 0, gen_loss = 0.6539412468523529, disc_loss = 0.07246762875986484
Trained batch 201 in epoch 0, gen_loss = 0.6538567571061673, disc_loss = 0.07225038918509784
Trained batch 202 in epoch 0, gen_loss = 0.6536087624251549, disc_loss = 0.0720162554942255
Trained batch 203 in epoch 0, gen_loss = 0.6537729938532791, disc_loss = 0.07176324768521476
Trained batch 204 in epoch 0, gen_loss = 0.6535975291961577, disc_loss = 0.07151966663304625
Trained batch 205 in epoch 0, gen_loss = 0.6536146322789701, disc_loss = 0.07126743805332525
Trained batch 206 in epoch 0, gen_loss = 0.6530804986827039, disc_loss = 0.07104110981873124
Trained batch 207 in epoch 0, gen_loss = 0.6530848785948294, disc_loss = 0.07078324300863852
Trained batch 208 in epoch 0, gen_loss = 0.6526524856615294, disc_loss = 0.07053169082743271
Trained batch 209 in epoch 0, gen_loss = 0.6525756894122987, disc_loss = 0.0702492247158218
Trained batch 210 in epoch 0, gen_loss = 0.6527160116564041, disc_loss = 0.06997300687541752
Trained batch 211 in epoch 0, gen_loss = 0.6525688524234969, disc_loss = 0.06969572720117867
Trained batch 212 in epoch 0, gen_loss = 0.6529521383869816, disc_loss = 0.0694366820777456
Trained batch 213 in epoch 0, gen_loss = 0.6524417502022235, disc_loss = 0.0691646337900833
Trained batch 214 in epoch 0, gen_loss = 0.6517382187898769, disc_loss = 0.06889097199796937
Trained batch 215 in epoch 0, gen_loss = 0.6517378366379826, disc_loss = 0.06861387330314352
Trained batch 216 in epoch 0, gen_loss = 0.6512225859450854, disc_loss = 0.06833796531030659
Trained batch 217 in epoch 0, gen_loss = 0.6511467015797939, disc_loss = 0.06806283091716127
Trained batch 218 in epoch 0, gen_loss = 0.6510020773432571, disc_loss = 0.06779285698499718
Trained batch 219 in epoch 0, gen_loss = 0.6502805475484241, disc_loss = 0.06753372796811163
Trained batch 220 in epoch 0, gen_loss = 0.6498910020109755, disc_loss = 0.06726784062702462
Trained batch 221 in epoch 0, gen_loss = 0.6495658998285327, disc_loss = 0.06699759062790724
Trained batch 222 in epoch 0, gen_loss = 0.6493387453491912, disc_loss = 0.06673766313655294
Trained batch 223 in epoch 0, gen_loss = 0.6495857460956488, disc_loss = 0.06648455672672883
Trained batch 224 in epoch 0, gen_loss = 0.6494121261437734, disc_loss = 0.06622703768519891
Trained batch 225 in epoch 0, gen_loss = 0.6492402027402304, disc_loss = 0.06598459221463882
Trained batch 226 in epoch 0, gen_loss = 0.6494810062620608, disc_loss = 0.06574579307169444
Trained batch 227 in epoch 0, gen_loss = 0.649094636110883, disc_loss = 0.06549937599297743
Trained batch 228 in epoch 0, gen_loss = 0.6490622656053852, disc_loss = 0.06525575499940972
Trained batch 229 in epoch 0, gen_loss = 0.6482572179773579, disc_loss = 0.06504776013974586
Trained batch 230 in epoch 0, gen_loss = 0.6479372490536083, disc_loss = 0.0648182488965814
Trained batch 231 in epoch 0, gen_loss = 0.6474821022872267, disc_loss = 0.06457419360482038
Trained batch 232 in epoch 0, gen_loss = 0.6476157039020195, disc_loss = 0.06439028868387761
Trained batch 233 in epoch 0, gen_loss = 0.6471369918595012, disc_loss = 0.06417600797194764
Trained batch 234 in epoch 0, gen_loss = 0.6462046804580283, disc_loss = 0.06394547319356748
Trained batch 235 in epoch 0, gen_loss = 0.6452091000342773, disc_loss = 0.06370524301832953
Trained batch 236 in epoch 0, gen_loss = 0.6451757213234398, disc_loss = 0.06349951702166144
Trained batch 237 in epoch 0, gen_loss = 0.6444899970743837, disc_loss = 0.06327644832033132
Trained batch 238 in epoch 0, gen_loss = 0.6438978419144283, disc_loss = 0.06304282665244892
Trained batch 239 in epoch 0, gen_loss = 0.6435802439848582, disc_loss = 0.06281160541305629
Trained batch 240 in epoch 0, gen_loss = 0.6433220992939106, disc_loss = 0.06258375498106804
Trained batch 241 in epoch 0, gen_loss = 0.6426272621332121, disc_loss = 0.06234920375657168
Trained batch 242 in epoch 0, gen_loss = 0.6423698492010925, disc_loss = 0.062126449280544935
Trained batch 243 in epoch 0, gen_loss = 0.6426743417978287, disc_loss = 0.061918269298955435
Trained batch 244 in epoch 0, gen_loss = 0.6424395060052677, disc_loss = 0.06169561625705386
Trained batch 245 in epoch 0, gen_loss = 0.642123839961804, disc_loss = 0.061485037803877055
Trained batch 246 in epoch 0, gen_loss = 0.6411009093045223, disc_loss = 0.06129856682703019
Trained batch 247 in epoch 0, gen_loss = 0.6404918831202292, disc_loss = 0.061141683057235975
Trained batch 248 in epoch 0, gen_loss = 0.6402713855107626, disc_loss = 0.0609410341107193
Trained batch 249 in epoch 0, gen_loss = 0.6401546764373779, disc_loss = 0.06073111972771585
Trained batch 250 in epoch 0, gen_loss = 0.6400146733717139, disc_loss = 0.060525785955374105
Trained batch 251 in epoch 0, gen_loss = 0.6403892487287521, disc_loss = 0.060323209514737956
Trained batch 252 in epoch 0, gen_loss = 0.6405385056503206, disc_loss = 0.06012074183120499
Trained batch 253 in epoch 0, gen_loss = 0.6405669830915496, disc_loss = 0.05991731307791971
Trained batch 254 in epoch 0, gen_loss = 0.6402849889269062, disc_loss = 0.059707739218777305
Trained batch 255 in epoch 0, gen_loss = 0.6397169887786731, disc_loss = 0.05949791158673179
Trained batch 256 in epoch 0, gen_loss = 0.6397601638554599, disc_loss = 0.059307216244463445
Trained batch 257 in epoch 0, gen_loss = 0.6393632559582244, disc_loss = 0.059101864145057956
Trained batch 258 in epoch 0, gen_loss = 0.6395255719602798, disc_loss = 0.0589080428017814
Trained batch 259 in epoch 0, gen_loss = 0.639833265886857, disc_loss = 0.05873304687332935
Trained batch 260 in epoch 0, gen_loss = 0.6401070407752333, disc_loss = 0.0585470928175382
Trained batch 261 in epoch 0, gen_loss = 0.6396467809686224, disc_loss = 0.058347386325787724
Trained batch 262 in epoch 0, gen_loss = 0.6395563681542646, disc_loss = 0.058167432002836535
Trained batch 263 in epoch 0, gen_loss = 0.6398764174770225, disc_loss = 0.05798219932088008
Trained batch 264 in epoch 0, gen_loss = 0.6393876531214084, disc_loss = 0.05778778416087042
Trained batch 265 in epoch 0, gen_loss = 0.6389938511122438, disc_loss = 0.057606944730295276
Trained batch 266 in epoch 0, gen_loss = 0.6384526129519001, disc_loss = 0.05742681876508596
Trained batch 267 in epoch 0, gen_loss = 0.6382353497085287, disc_loss = 0.057244738626446745
Trained batch 268 in epoch 0, gen_loss = 0.6376602610912465, disc_loss = 0.05706186004703151
Trained batch 269 in epoch 0, gen_loss = 0.6372174122819194, disc_loss = 0.056884950295918515
Trained batch 270 in epoch 0, gen_loss = 0.6365921619413524, disc_loss = 0.05671414417108157
Trained batch 271 in epoch 0, gen_loss = 0.6362807000165477, disc_loss = 0.056537772073437846
Trained batch 272 in epoch 0, gen_loss = 0.6363385373201126, disc_loss = 0.05638055121969609
Trained batch 273 in epoch 0, gen_loss = 0.6361233571367543, disc_loss = 0.05622537451096042
Trained batch 274 in epoch 0, gen_loss = 0.6363868957216089, disc_loss = 0.056051123331893576
Trained batch 275 in epoch 0, gen_loss = 0.6362651034757711, disc_loss = 0.05587819356984203
Trained batch 276 in epoch 0, gen_loss = 0.635778284352609, disc_loss = 0.05571589583954656
Trained batch 277 in epoch 0, gen_loss = 0.6356955817063078, disc_loss = 0.05554332286320466
Trained batch 278 in epoch 0, gen_loss = 0.6352137437217125, disc_loss = 0.055390813000713836
Trained batch 279 in epoch 0, gen_loss = 0.634940762498549, disc_loss = 0.055235899885052016
Trained batch 280 in epoch 0, gen_loss = 0.6351648423807477, disc_loss = 0.05506546521274327
Trained batch 281 in epoch 0, gen_loss = 0.6350600530281134, disc_loss = 0.0549057365778226
Trained batch 282 in epoch 0, gen_loss = 0.6345141168828566, disc_loss = 0.054738466180334455
Trained batch 283 in epoch 0, gen_loss = 0.6342530058513225, disc_loss = 0.054578329075757465
Trained batch 284 in epoch 0, gen_loss = 0.633974369053255, disc_loss = 0.05441315390921214
Trained batch 285 in epoch 0, gen_loss = 0.6336383166221472, disc_loss = 0.05424278772149522
Trained batch 286 in epoch 0, gen_loss = 0.6332695011685534, disc_loss = 0.05408058651213499
Trained batch 287 in epoch 0, gen_loss = 0.6326262008191811, disc_loss = 0.0539114882429001
Trained batch 288 in epoch 0, gen_loss = 0.6322282576437227, disc_loss = 0.053744805038980756
Trained batch 289 in epoch 0, gen_loss = 0.6320824198681733, disc_loss = 0.053616940573757065
Trained batch 290 in epoch 0, gen_loss = 0.631518906232008, disc_loss = 0.05346925517294517
Trained batch 291 in epoch 0, gen_loss = 0.6312018088894348, disc_loss = 0.05331484159799165
Trained batch 292 in epoch 0, gen_loss = 0.6312017921905062, disc_loss = 0.053158070523380826
Trained batch 293 in epoch 0, gen_loss = 0.630936515675921, disc_loss = 0.05300143147267553
Trained batch 294 in epoch 0, gen_loss = 0.6308344961222956, disc_loss = 0.05286711893533751
Trained batch 295 in epoch 0, gen_loss = 0.6308509519172681, disc_loss = 0.05271084583049791
Trained batch 296 in epoch 0, gen_loss = 0.6301453377461995, disc_loss = 0.052564640109261075
Trained batch 297 in epoch 0, gen_loss = 0.6297376186855688, disc_loss = 0.05241678607956375
Trained batch 298 in epoch 0, gen_loss = 0.6290704706640148, disc_loss = 0.052256496819390305
Trained batch 299 in epoch 0, gen_loss = 0.6283770072460174, disc_loss = 0.052109575330590206
Trained batch 300 in epoch 0, gen_loss = 0.6283984909025934, disc_loss = 0.051966065231500276
Trained batch 301 in epoch 0, gen_loss = 0.6284553617831098, disc_loss = 0.051821000948054904
Trained batch 302 in epoch 0, gen_loss = 0.6286160774750285, disc_loss = 0.051671747165876546
Trained batch 303 in epoch 0, gen_loss = 0.6281008100823352, disc_loss = 0.05151804923815163
Trained batch 304 in epoch 0, gen_loss = 0.6283482653195741, disc_loss = 0.05138213979538347
Trained batch 305 in epoch 0, gen_loss = 0.6280742694739423, disc_loss = 0.05124290040359388
Trained batch 306 in epoch 0, gen_loss = 0.627792254721303, disc_loss = 0.05109957242930605
Trained batch 307 in epoch 0, gen_loss = 0.6276869669363097, disc_loss = 0.05094988213991142
Trained batch 308 in epoch 0, gen_loss = 0.6277386833548931, disc_loss = 0.05082384101944014
Trained batch 309 in epoch 0, gen_loss = 0.6276147809720809, disc_loss = 0.05068296799886852
Trained batch 310 in epoch 0, gen_loss = 0.6276797958892258, disc_loss = 0.05053798194040344
Trained batch 311 in epoch 0, gen_loss = 0.6277390380318348, disc_loss = 0.05039728537369042
Trained batch 312 in epoch 0, gen_loss = 0.6273365424463924, disc_loss = 0.05024946873007824
Trained batch 313 in epoch 0, gen_loss = 0.6272349775217141, disc_loss = 0.05011697092307079
Trained batch 314 in epoch 0, gen_loss = 0.6271670305539692, disc_loss = 0.04998575283864897
Trained batch 315 in epoch 0, gen_loss = 0.626383973261978, disc_loss = 0.04988681420512922
Trained batch 316 in epoch 0, gen_loss = 0.6262334220416914, disc_loss = 0.049777496910186875
Trained batch 317 in epoch 0, gen_loss = 0.626200703517446, disc_loss = 0.04966548898362748
Trained batch 318 in epoch 0, gen_loss = 0.6261734850354329, disc_loss = 0.04952587465081235
Trained batch 319 in epoch 0, gen_loss = 0.6258264645934105, disc_loss = 0.04940738805307774
Trained batch 320 in epoch 0, gen_loss = 0.625639960773266, disc_loss = 0.04928576198734366
Trained batch 321 in epoch 0, gen_loss = 0.625215919121452, disc_loss = 0.04915188265796635
Trained batch 322 in epoch 0, gen_loss = 0.6247539387398829, disc_loss = 0.0490151062368447
Trained batch 323 in epoch 0, gen_loss = 0.6242443805123553, disc_loss = 0.04887693486159184
Trained batch 324 in epoch 0, gen_loss = 0.6239299884209266, disc_loss = 0.04875112027932818
Trained batch 325 in epoch 0, gen_loss = 0.6233383284573175, disc_loss = 0.04876920042496655
Trained batch 326 in epoch 0, gen_loss = 0.6230307585055676, disc_loss = 0.048683921665534244
Trained batch 327 in epoch 0, gen_loss = 0.6231351637440484, disc_loss = 0.048574301935221276
Trained batch 328 in epoch 0, gen_loss = 0.6232724442489241, disc_loss = 0.048460002755038555
Trained batch 329 in epoch 0, gen_loss = 0.6229403121001793, disc_loss = 0.048348645071470825
Trained batch 330 in epoch 0, gen_loss = 0.6227190414404221, disc_loss = 0.04822763966486488
Trained batch 331 in epoch 0, gen_loss = 0.6226413653018963, disc_loss = 0.04810260004274188
Trained batch 332 in epoch 0, gen_loss = 0.6224364734089768, disc_loss = 0.0479788570986451
Trained batch 333 in epoch 0, gen_loss = 0.6220381577571709, disc_loss = 0.04785138512208701
Trained batch 334 in epoch 0, gen_loss = 0.6216330797814611, disc_loss = 0.04772813844897631
Trained batch 335 in epoch 0, gen_loss = 0.6211169524384397, disc_loss = 0.04760297908935519
Trained batch 336 in epoch 0, gen_loss = 0.620607280855009, disc_loss = 0.04747440146962153
Trained batch 337 in epoch 0, gen_loss = 0.6206342249050648, disc_loss = 0.047356228046560075
Trained batch 338 in epoch 0, gen_loss = 0.6201972965821404, disc_loss = 0.047238516170740305
Trained batch 339 in epoch 0, gen_loss = 0.6201528607922442, disc_loss = 0.047121237266315695
Trained batch 340 in epoch 0, gen_loss = 0.6199893513557848, disc_loss = 0.047000553493068455
Trained batch 341 in epoch 0, gen_loss = 0.6196643107990075, disc_loss = 0.04687780987509466
Trained batch 342 in epoch 0, gen_loss = 0.6193287814845149, disc_loss = 0.046758508359953634
Trained batch 343 in epoch 0, gen_loss = 0.6188850369910861, disc_loss = 0.046633424864118096
Trained batch 344 in epoch 0, gen_loss = 0.6186784441920294, disc_loss = 0.04651015155368309
Trained batch 345 in epoch 0, gen_loss = 0.6183446800777678, disc_loss = 0.0463882970470246
Trained batch 346 in epoch 0, gen_loss = 0.6183301320337081, disc_loss = 0.0462744567403704
Trained batch 347 in epoch 0, gen_loss = 0.6180294892568697, disc_loss = 0.0461602669648141
Trained batch 348 in epoch 0, gen_loss = 0.6176267640296914, disc_loss = 0.04604068202470939
Trained batch 349 in epoch 0, gen_loss = 0.6172325662204198, disc_loss = 0.0459304616161223
Trained batch 350 in epoch 0, gen_loss = 0.6172955236543617, disc_loss = 0.04581981917121514
Trained batch 351 in epoch 0, gen_loss = 0.6173443767157468, disc_loss = 0.04570513795823155
Trained batch 352 in epoch 0, gen_loss = 0.6174969399617009, disc_loss = 0.04559308963645746
Trained batch 353 in epoch 0, gen_loss = 0.6173102020543847, disc_loss = 0.045477817412518426
Trained batch 354 in epoch 0, gen_loss = 0.6173053246148875, disc_loss = 0.04536361343187975
Trained batch 355 in epoch 0, gen_loss = 0.6169426565927066, disc_loss = 0.04524753160324743
Trained batch 356 in epoch 0, gen_loss = 0.6166059828939892, disc_loss = 0.04513226308486834
Trained batch 357 in epoch 0, gen_loss = 0.6164271212156924, disc_loss = 0.045016250451480416
Trained batch 358 in epoch 0, gen_loss = 0.6159653197091934, disc_loss = 0.04490661208419269
Trained batch 359 in epoch 0, gen_loss = 0.6161140163739522, disc_loss = 0.04480546808659306
Trained batch 360 in epoch 0, gen_loss = 0.6161717879805209, disc_loss = 0.044698080061509216
Trained batch 361 in epoch 0, gen_loss = 0.6160211884185095, disc_loss = 0.04458798159065641
Trained batch 362 in epoch 0, gen_loss = 0.615638373147686, disc_loss = 0.04447615153371733
Trained batch 363 in epoch 0, gen_loss = 0.6157452478841111, disc_loss = 0.04437029036337633
Trained batch 364 in epoch 0, gen_loss = 0.6153201103210449, disc_loss = 0.04426149323850564
Trained batch 365 in epoch 0, gen_loss = 0.6151337525883659, disc_loss = 0.0441516227746647
Trained batch 366 in epoch 0, gen_loss = 0.6151769274911698, disc_loss = 0.0440424903320042
Trained batch 367 in epoch 0, gen_loss = 0.614811503611829, disc_loss = 0.04393266881031551
Trained batch 368 in epoch 0, gen_loss = 0.6146445139475308, disc_loss = 0.04382340540746
Trained batch 369 in epoch 0, gen_loss = 0.6146598260950398, disc_loss = 0.04371689671298137
Trained batch 370 in epoch 0, gen_loss = 0.6145495011639402, disc_loss = 0.04360918269989726
Trained batch 371 in epoch 0, gen_loss = 0.6142194896776189, disc_loss = 0.04350376914828635
Trained batch 372 in epoch 0, gen_loss = 0.6139393692521563, disc_loss = 0.04339895477510949
Trained batch 373 in epoch 0, gen_loss = 0.6134964620525186, disc_loss = 0.04329887423029339
Trained batch 374 in epoch 0, gen_loss = 0.6130456407864888, disc_loss = 0.04319518674723804
Trained batch 375 in epoch 0, gen_loss = 0.6126413914434453, disc_loss = 0.04308932946816562
Trained batch 376 in epoch 0, gen_loss = 0.612734828292533, disc_loss = 0.042988938251040894
Trained batch 377 in epoch 0, gen_loss = 0.6122305241053697, disc_loss = 0.042889519811425574
Trained batch 378 in epoch 0, gen_loss = 0.6120567398052417, disc_loss = 0.04279405099774728
Trained batch 379 in epoch 0, gen_loss = 0.611857191904595, disc_loss = 0.04269811912488781
Trained batch 380 in epoch 0, gen_loss = 0.6117722522711816, disc_loss = 0.04259955304432807
Trained batch 381 in epoch 0, gen_loss = 0.6115953311402136, disc_loss = 0.04249846978425122
Trained batch 382 in epoch 0, gen_loss = 0.6110759224188235, disc_loss = 0.04239605530165136
Trained batch 383 in epoch 0, gen_loss = 0.6108069631736726, disc_loss = 0.04229587863725707
Trained batch 384 in epoch 0, gen_loss = 0.6103164264133998, disc_loss = 0.04219595576007548
Trained batch 385 in epoch 0, gen_loss = 0.6096969149582127, disc_loss = 0.04211319507874182
Trained batch 386 in epoch 0, gen_loss = 0.6094332707020663, disc_loss = 0.042027660052939404
Trained batch 387 in epoch 0, gen_loss = 0.6090784019877001, disc_loss = 0.04193036019329231
Trained batch 388 in epoch 0, gen_loss = 0.6087650338757621, disc_loss = 0.041833776156453936
Trained batch 389 in epoch 0, gen_loss = 0.6082736955239223, disc_loss = 0.04175486258195283
Trained batch 390 in epoch 0, gen_loss = 0.608182565787869, disc_loss = 0.04166594742978339
Trained batch 391 in epoch 0, gen_loss = 0.6078891550399819, disc_loss = 0.04157012146458087
Trained batch 392 in epoch 0, gen_loss = 0.6073958331543677, disc_loss = 0.04147430965096308
Trained batch 393 in epoch 0, gen_loss = 0.6074976042896358, disc_loss = 0.04138210221597934
Trained batch 394 in epoch 0, gen_loss = 0.6072177270545235, disc_loss = 0.04129081069774737
Trained batch 395 in epoch 0, gen_loss = 0.6070313266280926, disc_loss = 0.04119608925262729
Trained batch 396 in epoch 0, gen_loss = 0.6067407264066883, disc_loss = 0.04110414803236892
Trained batch 397 in epoch 0, gen_loss = 0.6066414202427745, disc_loss = 0.04101570269113978
Trained batch 398 in epoch 0, gen_loss = 0.606416491413475, disc_loss = 0.04092379293479212
Trained batch 399 in epoch 0, gen_loss = 0.6062933162599802, disc_loss = 0.040838780892663636
Trained batch 400 in epoch 0, gen_loss = 0.6059304038039467, disc_loss = 0.04074400464511776
Trained batch 401 in epoch 0, gen_loss = 0.6056814362012332, disc_loss = 0.040650721089173086
Trained batch 402 in epoch 0, gen_loss = 0.605745817754168, disc_loss = 0.040562265871494786
Trained batch 403 in epoch 0, gen_loss = 0.6054074417541523, disc_loss = 0.040471993292230043
Trained batch 404 in epoch 0, gen_loss = 0.6051609323348528, disc_loss = 0.04038568947832158
Trained batch 405 in epoch 0, gen_loss = 0.6048477596690502, disc_loss = 0.040297504673881705
Trained batch 406 in epoch 0, gen_loss = 0.6048045655579766, disc_loss = 0.0402073070889737
Trained batch 407 in epoch 0, gen_loss = 0.6046829081195242, disc_loss = 0.04011729136725668
Trained batch 408 in epoch 0, gen_loss = 0.6049025254756722, disc_loss = 0.04003121791644281
Trained batch 409 in epoch 0, gen_loss = 0.6047788667242702, disc_loss = 0.03994546940254911
Trained batch 410 in epoch 0, gen_loss = 0.604458605213467, disc_loss = 0.03985775455236525
Trained batch 411 in epoch 0, gen_loss = 0.6044405948334527, disc_loss = 0.03977241793599938
Trained batch 412 in epoch 0, gen_loss = 0.6044146947508574, disc_loss = 0.03968871347594907
Trained batch 413 in epoch 0, gen_loss = 0.6041404172994088, disc_loss = 0.039605529170712805
Trained batch 414 in epoch 0, gen_loss = 0.6038062938724655, disc_loss = 0.03951758353252817
Trained batch 415 in epoch 0, gen_loss = 0.603419787895221, disc_loss = 0.039429875110867076
Trained batch 416 in epoch 0, gen_loss = 0.6031390847347909, disc_loss = 0.03934554554236164
Trained batch 417 in epoch 0, gen_loss = 0.6027880362346412, disc_loss = 0.039257630053573425
Trained batch 418 in epoch 0, gen_loss = 0.6027533070568822, disc_loss = 0.03917263915011024
Trained batch 419 in epoch 0, gen_loss = 0.6024360121715636, disc_loss = 0.03909361374209679
Trained batch 420 in epoch 0, gen_loss = 0.6024112007680245, disc_loss = 0.03900998610574
Trained batch 421 in epoch 0, gen_loss = 0.6021535775107795, disc_loss = 0.03892735074278656
Trained batch 422 in epoch 0, gen_loss = 0.601953380778608, disc_loss = 0.038843512876910416
Trained batch 423 in epoch 0, gen_loss = 0.6014284575182312, disc_loss = 0.038758019253914686
Trained batch 424 in epoch 0, gen_loss = 0.6012903827078202, disc_loss = 0.038674336741952337
Trained batch 425 in epoch 0, gen_loss = 0.6009022131072523, disc_loss = 0.03859012020738717
Trained batch 426 in epoch 0, gen_loss = 0.6003436106289857, disc_loss = 0.03851543394717839
Trained batch 427 in epoch 0, gen_loss = 0.6000171707612332, disc_loss = 0.03844308616075595
Trained batch 428 in epoch 0, gen_loss = 0.5999863618737334, disc_loss = 0.038367443475959113
Trained batch 429 in epoch 0, gen_loss = 0.599899408429168, disc_loss = 0.038286506818867354
Trained batch 430 in epoch 0, gen_loss = 0.5994736172621875, disc_loss = 0.03820413199367905
Trained batch 431 in epoch 0, gen_loss = 0.5994470372658085, disc_loss = 0.03812429842138146
Trained batch 432 in epoch 0, gen_loss = 0.5993934321210787, disc_loss = 0.03804854439210155
Trained batch 433 in epoch 0, gen_loss = 0.5994240911462889, disc_loss = 0.037970283874503685
Trained batch 434 in epoch 0, gen_loss = 0.59942176184435, disc_loss = 0.03789291469401669
Trained batch 435 in epoch 0, gen_loss = 0.5993805279562233, disc_loss = 0.037814658141583884
Trained batch 436 in epoch 0, gen_loss = 0.5992833664406355, disc_loss = 0.03773566940692797
Trained batch 437 in epoch 0, gen_loss = 0.5989175408942514, disc_loss = 0.03765716098119254
Trained batch 438 in epoch 0, gen_loss = 0.5987602416638089, disc_loss = 0.03758227056293305
Trained batch 439 in epoch 0, gen_loss = 0.5985668656500903, disc_loss = 0.03750524136862209
Trained batch 440 in epoch 0, gen_loss = 0.5984761537878421, disc_loss = 0.0374269623883795
Trained batch 441 in epoch 0, gen_loss = 0.5985543439291182, disc_loss = 0.037354828603354984
Trained batch 442 in epoch 0, gen_loss = 0.5985948931821043, disc_loss = 0.03728086020667148
Trained batch 443 in epoch 0, gen_loss = 0.5981525894220885, disc_loss = 0.03720506175414886
Trained batch 444 in epoch 0, gen_loss = 0.5980834867177385, disc_loss = 0.03713019197084679
Trained batch 445 in epoch 0, gen_loss = 0.5979062529957347, disc_loss = 0.0370546344029738
Trained batch 446 in epoch 0, gen_loss = 0.5977563055303006, disc_loss = 0.036982884039368974
Trained batch 447 in epoch 0, gen_loss = 0.5975209676793644, disc_loss = 0.03690637238261323
Trained batch 448 in epoch 0, gen_loss = 0.5972852343174821, disc_loss = 0.036830706875003824
Trained batch 449 in epoch 0, gen_loss = 0.597016669511795, disc_loss = 0.03675409235888057
Trained batch 450 in epoch 0, gen_loss = 0.5969873701125186, disc_loss = 0.0366801754148169
Trained batch 451 in epoch 0, gen_loss = 0.5968786865736531, disc_loss = 0.03660571549512215
Trained batch 452 in epoch 0, gen_loss = 0.5969510621845591, disc_loss = 0.03653235957807723
Trained batch 453 in epoch 0, gen_loss = 0.5968685865664797, disc_loss = 0.036459596775829106
Trained batch 454 in epoch 0, gen_loss = 0.5965030734355633, disc_loss = 0.03638470729986963
Trained batch 455 in epoch 0, gen_loss = 0.5964667799702862, disc_loss = 0.036313333680849165
Trained batch 456 in epoch 0, gen_loss = 0.5963988210231522, disc_loss = 0.0362396919916443
Trained batch 457 in epoch 0, gen_loss = 0.59633101464359, disc_loss = 0.036168870469860274
Trained batch 458 in epoch 0, gen_loss = 0.5960943199580532, disc_loss = 0.036094884670690545
Trained batch 459 in epoch 0, gen_loss = 0.5955822884388592, disc_loss = 0.036029702957982766
Trained batch 460 in epoch 0, gen_loss = 0.5952556773923224, disc_loss = 0.03596307203595864
Trained batch 461 in epoch 0, gen_loss = 0.5949443798044544, disc_loss = 0.035896633894311754
Trained batch 462 in epoch 0, gen_loss = 0.5949459341123347, disc_loss = 0.03583002097041751
Trained batch 463 in epoch 0, gen_loss = 0.5948892578739544, disc_loss = 0.0357600877820193
Trained batch 464 in epoch 0, gen_loss = 0.5947259313316755, disc_loss = 0.035689180340838206
Trained batch 465 in epoch 0, gen_loss = 0.5943992075234523, disc_loss = 0.035624067034760225
Trained batch 466 in epoch 0, gen_loss = 0.5942863677757966, disc_loss = 0.035554282477911266
Trained batch 467 in epoch 0, gen_loss = 0.5941319526770176, disc_loss = 0.035484115834921025
Trained batch 468 in epoch 0, gen_loss = 0.5939969166255454, disc_loss = 0.03541829169038802
Trained batch 469 in epoch 0, gen_loss = 0.5937022268772125, disc_loss = 0.03534975595811897
Trained batch 470 in epoch 0, gen_loss = 0.5934074046505484, disc_loss = 0.03528077287931636
Trained batch 471 in epoch 0, gen_loss = 0.5933585236385718, disc_loss = 0.03521242098225738
Trained batch 472 in epoch 0, gen_loss = 0.5934905878585186, disc_loss = 0.035146834013243276
Trained batch 473 in epoch 0, gen_loss = 0.5934491693219052, disc_loss = 0.0350799322494734
Trained batch 474 in epoch 0, gen_loss = 0.5930779873697382, disc_loss = 0.03501060366140384
Trained batch 475 in epoch 0, gen_loss = 0.592846006217624, disc_loss = 0.034943790378013796
Trained batch 476 in epoch 0, gen_loss = 0.5927490363955747, disc_loss = 0.03487612754379866
Trained batch 477 in epoch 0, gen_loss = 0.592366628878785, disc_loss = 0.034807836766467745
Trained batch 478 in epoch 0, gen_loss = 0.5920533520443704, disc_loss = 0.034740380831996064
Trained batch 479 in epoch 0, gen_loss = 0.5919533056517442, disc_loss = 0.034673320478759706
Trained batch 480 in epoch 0, gen_loss = 0.5917610712581761, disc_loss = 0.03460543872227044
Trained batch 481 in epoch 0, gen_loss = 0.5916421822121529, disc_loss = 0.0345381441602437
Trained batch 482 in epoch 0, gen_loss = 0.5916154957829548, disc_loss = 0.03447239990558236
Trained batch 483 in epoch 0, gen_loss = 0.59148841782296, disc_loss = 0.034406474210833746
Trained batch 484 in epoch 0, gen_loss = 0.5911493874702257, disc_loss = 0.03434083434253854
Trained batch 485 in epoch 0, gen_loss = 0.5909707599583968, disc_loss = 0.03427502812545578
Trained batch 486 in epoch 0, gen_loss = 0.5906214563508788, disc_loss = 0.03421031121830064
Trained batch 487 in epoch 0, gen_loss = 0.5904854174520149, disc_loss = 0.03414487799335668
Trained batch 488 in epoch 0, gen_loss = 0.5902473168992314, disc_loss = 0.03408174131995025
Trained batch 489 in epoch 0, gen_loss = 0.5900000111180909, disc_loss = 0.034016301660627433
Trained batch 490 in epoch 0, gen_loss = 0.5896473170055149, disc_loss = 0.03395252396843857
Trained batch 491 in epoch 0, gen_loss = 0.5892608636399594, disc_loss = 0.03388930856755639
Trained batch 492 in epoch 0, gen_loss = 0.5891916779064503, disc_loss = 0.033826744617444685
Trained batch 493 in epoch 0, gen_loss = 0.5889155329601002, disc_loss = 0.033763340117715236
Trained batch 494 in epoch 0, gen_loss = 0.5887160721451346, disc_loss = 0.03370116672271656
Trained batch 495 in epoch 0, gen_loss = 0.5883482302869519, disc_loss = 0.03364074527381754
Trained batch 496 in epoch 0, gen_loss = 0.5881819837769752, disc_loss = 0.03357881333930062
Trained batch 497 in epoch 0, gen_loss = 0.5879181461880006, disc_loss = 0.033516359729617536
Trained batch 498 in epoch 0, gen_loss = 0.5878836464786339, disc_loss = 0.03345611406292371
Trained batch 499 in epoch 0, gen_loss = 0.5878071942329407, disc_loss = 0.033396878476254645
Trained batch 500 in epoch 0, gen_loss = 0.5879903492813339, disc_loss = 0.033338175175670615
Trained batch 501 in epoch 0, gen_loss = 0.5879189718766992, disc_loss = 0.03327818870900637
Trained batch 502 in epoch 0, gen_loss = 0.5876087245955381, disc_loss = 0.03321813063040583
Trained batch 503 in epoch 0, gen_loss = 0.5875735373369285, disc_loss = 0.03315737210158702
Trained batch 504 in epoch 0, gen_loss = 0.5874088094376101, disc_loss = 0.03309746014627416
Trained batch 505 in epoch 0, gen_loss = 0.5871952342775029, disc_loss = 0.033036219923326324
Trained batch 506 in epoch 0, gen_loss = 0.5869635633345421, disc_loss = 0.03297392155927343
Trained batch 507 in epoch 0, gen_loss = 0.5869591581657176, disc_loss = 0.032914774852170516
Trained batch 508 in epoch 0, gen_loss = 0.586686159398092, disc_loss = 0.03285747931854825
Trained batch 509 in epoch 0, gen_loss = 0.5866473628025429, disc_loss = 0.03280406567021547
Trained batch 510 in epoch 0, gen_loss = 0.5864575179822291, disc_loss = 0.03274914119966736
Trained batch 511 in epoch 0, gen_loss = 0.586308317957446, disc_loss = 0.03269021434653041
Trained batch 512 in epoch 0, gen_loss = 0.5863677033206873, disc_loss = 0.032631801114078124
Trained batch 513 in epoch 0, gen_loss = 0.5861904023338385, disc_loss = 0.0325728894495822
Trained batch 514 in epoch 0, gen_loss = 0.5858427600953185, disc_loss = 0.03251299969761551
Trained batch 515 in epoch 0, gen_loss = 0.5859071202287378, disc_loss = 0.03245585063353267
Trained batch 516 in epoch 0, gen_loss = 0.585850347633288, disc_loss = 0.03239926821200945
Trained batch 517 in epoch 0, gen_loss = 0.5856465179487546, disc_loss = 0.03234179173578224
Trained batch 518 in epoch 0, gen_loss = 0.5854642499159297, disc_loss = 0.0322848676797182
Trained batch 519 in epoch 0, gen_loss = 0.5853493862427198, disc_loss = 0.03222907155152195
Trained batch 520 in epoch 0, gen_loss = 0.5851917288582522, disc_loss = 0.032171445667221715
Trained batch 521 in epoch 0, gen_loss = 0.5850211721613032, disc_loss = 0.03211458965465141
Trained batch 522 in epoch 0, gen_loss = 0.5849406600112441, disc_loss = 0.03205922150621137
Trained batch 523 in epoch 0, gen_loss = 0.5847456313612807, disc_loss = 0.03200151648648787
Trained batch 524 in epoch 0, gen_loss = 0.5845572131588346, disc_loss = 0.03194602180338864
Trained batch 525 in epoch 0, gen_loss = 0.5844208096709995, disc_loss = 0.031890030596856765
Trained batch 526 in epoch 0, gen_loss = 0.5842904240967427, disc_loss = 0.03183883626744738
Trained batch 527 in epoch 0, gen_loss = 0.5840837440143029, disc_loss = 0.03178897168761592
Trained batch 528 in epoch 0, gen_loss = 0.5839861011122035, disc_loss = 0.03173611965404545
Trained batch 529 in epoch 0, gen_loss = 0.5838533038800617, disc_loss = 0.03168077794066191
Trained batch 530 in epoch 0, gen_loss = 0.5835703313350677, disc_loss = 0.03162660222459346
Trained batch 531 in epoch 0, gen_loss = 0.583199448547417, disc_loss = 0.03157251124131087
Trained batch 532 in epoch 0, gen_loss = 0.5830049274898157, disc_loss = 0.03151710978800896
Trained batch 533 in epoch 0, gen_loss = 0.5830907254071718, disc_loss = 0.031463241823103054
Trained batch 534 in epoch 0, gen_loss = 0.5828406432521678, disc_loss = 0.0314080013669418
Trained batch 535 in epoch 0, gen_loss = 0.5828076945200785, disc_loss = 0.03135427395503016
Trained batch 536 in epoch 0, gen_loss = 0.5824693660052351, disc_loss = 0.031300205555033406
Trained batch 537 in epoch 0, gen_loss = 0.5820675656808797, disc_loss = 0.031245990415479378
Trained batch 538 in epoch 0, gen_loss = 0.5819774965076588, disc_loss = 0.031192889366452696
Trained batch 539 in epoch 0, gen_loss = 0.5817435701136235, disc_loss = 0.031140429957519734
Trained batch 540 in epoch 0, gen_loss = 0.5814697081963368, disc_loss = 0.031094813530674194
Trained batch 541 in epoch 0, gen_loss = 0.5812602801406516, disc_loss = 0.03105107545006395
Trained batch 542 in epoch 0, gen_loss = 0.5808618473743207, disc_loss = 0.031001046883726138
Trained batch 543 in epoch 0, gen_loss = 0.5806342394036406, disc_loss = 0.03094848024984993
Trained batch 544 in epoch 0, gen_loss = 0.5803155368621197, disc_loss = 0.03089727780743237
Trained batch 545 in epoch 0, gen_loss = 0.5799972369120672, disc_loss = 0.03084869052035284
Trained batch 546 in epoch 0, gen_loss = 0.5798109841194188, disc_loss = 0.030795965317536273
Trained batch 547 in epoch 0, gen_loss = 0.5795773350953186, disc_loss = 0.03074341033688189
Trained batch 548 in epoch 0, gen_loss = 0.5796158474976899, disc_loss = 0.030693471481139774
Trained batch 549 in epoch 0, gen_loss = 0.5794517835161903, disc_loss = 0.03064266063378785
Trained batch 550 in epoch 0, gen_loss = 0.5792501424380093, disc_loss = 0.030592475890371267
Trained batch 551 in epoch 0, gen_loss = 0.5788761417286984, disc_loss = 0.030543316883056938
Trained batch 552 in epoch 0, gen_loss = 0.5787740349769592, disc_loss = 0.030500632220884248
Trained batch 553 in epoch 0, gen_loss = 0.5784915816913012, disc_loss = 0.030458425176086153
Trained batch 554 in epoch 0, gen_loss = 0.5784718000137055, disc_loss = 0.030411878407949598
Trained batch 555 in epoch 0, gen_loss = 0.5781679206507669, disc_loss = 0.030360628367718242
Trained batch 556 in epoch 0, gen_loss = 0.5780710953035303, disc_loss = 0.03031040042955629
Trained batch 557 in epoch 0, gen_loss = 0.5780093044469861, disc_loss = 0.030262157189748232
Trained batch 558 in epoch 0, gen_loss = 0.5778163198068444, disc_loss = 0.03021202114688096
Trained batch 559 in epoch 0, gen_loss = 0.5775727423706225, disc_loss = 0.030162730888072736
Trained batch 560 in epoch 0, gen_loss = 0.5773444564053507, disc_loss = 0.03011504354294881
Trained batch 561 in epoch 0, gen_loss = 0.5770369594737729, disc_loss = 0.030065166305964383
Trained batch 562 in epoch 0, gen_loss = 0.5770106574464225, disc_loss = 0.030018614942322074
Trained batch 563 in epoch 0, gen_loss = 0.5771749576038503, disc_loss = 0.029971408425105393
Trained batch 564 in epoch 0, gen_loss = 0.5770732395944342, disc_loss = 0.029921915828233864
Trained batch 565 in epoch 0, gen_loss = 0.5769102338030144, disc_loss = 0.029874053447426694
Trained batch 566 in epoch 0, gen_loss = 0.5766788046721638, disc_loss = 0.029825161978876715
Trained batch 567 in epoch 0, gen_loss = 0.5766252047893866, disc_loss = 0.02977810408710994
Trained batch 568 in epoch 0, gen_loss = 0.5765694147777892, disc_loss = 0.029730325880843445
Trained batch 569 in epoch 0, gen_loss = 0.5763130303014788, disc_loss = 0.029684102306297668
Trained batch 570 in epoch 0, gen_loss = 0.576313713490441, disc_loss = 0.029638109463446444
Trained batch 571 in epoch 0, gen_loss = 0.5762359439493059, disc_loss = 0.0295906681239758
Trained batch 572 in epoch 0, gen_loss = 0.5760413285013268, disc_loss = 0.029542559474409213
Trained batch 573 in epoch 0, gen_loss = 0.5759838326778977, disc_loss = 0.029495301276959724
Trained batch 574 in epoch 0, gen_loss = 0.5757834383197453, disc_loss = 0.029446537282277384
Trained batch 575 in epoch 0, gen_loss = 0.5755002694721851, disc_loss = 0.029398026155225427
Trained batch 576 in epoch 0, gen_loss = 0.5754944030808824, disc_loss = 0.029351157774830775
Trained batch 577 in epoch 0, gen_loss = 0.5753207289739464, disc_loss = 0.02930381691271027
Trained batch 578 in epoch 0, gen_loss = 0.5752207789705207, disc_loss = 0.029256555567043537
Trained batch 579 in epoch 0, gen_loss = 0.5752525130736417, disc_loss = 0.029211368555492112
Trained batch 580 in epoch 0, gen_loss = 0.5750652040241097, disc_loss = 0.029165166851755058
Trained batch 581 in epoch 0, gen_loss = 0.5750957474983025, disc_loss = 0.029118787101229903
Trained batch 582 in epoch 0, gen_loss = 0.5750116938485494, disc_loss = 0.02907152606855669
Trained batch 583 in epoch 0, gen_loss = 0.5748025805574574, disc_loss = 0.029024595695337257
Trained batch 584 in epoch 0, gen_loss = 0.5745193308235234, disc_loss = 0.02897813401451637
Trained batch 585 in epoch 0, gen_loss = 0.5742732951665495, disc_loss = 0.02893182053647891
Trained batch 586 in epoch 0, gen_loss = 0.5741007952003706, disc_loss = 0.028884849321887254
Trained batch 587 in epoch 0, gen_loss = 0.5736424612755678, disc_loss = 0.028847605279459915
Trained batch 588 in epoch 0, gen_loss = 0.5732484038900236, disc_loss = 0.028810337855099545
Trained batch 589 in epoch 0, gen_loss = 0.5732029332952985, disc_loss = 0.028766618671795446
Trained batch 590 in epoch 0, gen_loss = 0.5730823822029553, disc_loss = 0.02872300776810576
Trained batch 591 in epoch 0, gen_loss = 0.5728772813705979, disc_loss = 0.02867962778632441
Trained batch 592 in epoch 0, gen_loss = 0.5728765711514986, disc_loss = 0.02864364726338567
Trained batch 593 in epoch 0, gen_loss = 0.5730081670713746, disc_loss = 0.0286074547055538
Trained batch 594 in epoch 0, gen_loss = 0.5728881331551977, disc_loss = 0.028576965855058877
Trained batch 595 in epoch 0, gen_loss = 0.5728091584056015, disc_loss = 0.028538912801531414
Trained batch 596 in epoch 0, gen_loss = 0.5727461068773988, disc_loss = 0.028499247500410857
Trained batch 597 in epoch 0, gen_loss = 0.5726259840671035, disc_loss = 0.028456497262699224
Trained batch 598 in epoch 0, gen_loss = 0.5725120431891267, disc_loss = 0.028413124242564276
Trained batch 599 in epoch 0, gen_loss = 0.5723082280655702, disc_loss = 0.02836917311496412
Trained batch 600 in epoch 0, gen_loss = 0.5722976020985158, disc_loss = 0.028325898968730858
Trained batch 601 in epoch 0, gen_loss = 0.5721470726486853, disc_loss = 0.02828401474721507
Trained batch 602 in epoch 0, gen_loss = 0.5719878872235616, disc_loss = 0.02824286645846704
Trained batch 603 in epoch 0, gen_loss = 0.5719247016290955, disc_loss = 0.028200775993192286
Trained batch 604 in epoch 0, gen_loss = 0.5718189445408908, disc_loss = 0.028157463594347485
Trained batch 605 in epoch 0, gen_loss = 0.5716815480206272, disc_loss = 0.028113350670219157
Trained batch 606 in epoch 0, gen_loss = 0.5715152267274589, disc_loss = 0.02806993454957114
Trained batch 607 in epoch 0, gen_loss = 0.5713509577571562, disc_loss = 0.02803019557177322
Trained batch 608 in epoch 0, gen_loss = 0.5714292401457067, disc_loss = 0.02798757485032987
Trained batch 609 in epoch 0, gen_loss = 0.5713189490017344, disc_loss = 0.02794550370852478
Trained batch 610 in epoch 0, gen_loss = 0.5713238460049886, disc_loss = 0.02790409508092655
Trained batch 611 in epoch 0, gen_loss = 0.5712582863915979, disc_loss = 0.027861697024063152
Trained batch 612 in epoch 0, gen_loss = 0.5711130049158659, disc_loss = 0.027819278892311974
Trained batch 613 in epoch 0, gen_loss = 0.5711096448987625, disc_loss = 0.027776498799720408
Trained batch 614 in epoch 0, gen_loss = 0.570843387910021, disc_loss = 0.02773402852717999
Trained batch 615 in epoch 0, gen_loss = 0.5705104976311907, disc_loss = 0.0276911567984294
Trained batch 616 in epoch 0, gen_loss = 0.5703692528958065, disc_loss = 0.027649679779327162
Trained batch 617 in epoch 0, gen_loss = 0.5702081028022427, disc_loss = 0.027608098621400297
Trained batch 618 in epoch 0, gen_loss = 0.570106024936636, disc_loss = 0.027566881276560293
Trained batch 619 in epoch 0, gen_loss = 0.570384214530068, disc_loss = 0.027530093529569585
Trained batch 620 in epoch 0, gen_loss = 0.57025078582111, disc_loss = 0.027490682439352182
Trained batch 621 in epoch 0, gen_loss = 0.5701217000219982, disc_loss = 0.02745159449848613
Trained batch 622 in epoch 0, gen_loss = 0.5699015688838776, disc_loss = 0.027414376537067395
Trained batch 623 in epoch 0, gen_loss = 0.5698984405264641, disc_loss = 0.02737788469303035
Trained batch 624 in epoch 0, gen_loss = 0.5697474552154541, disc_loss = 0.02733718880433589
Trained batch 625 in epoch 0, gen_loss = 0.5695575286214725, disc_loss = 0.027296002263005395
Trained batch 626 in epoch 0, gen_loss = 0.5696769370987085, disc_loss = 0.027258426276855527
Trained batch 627 in epoch 0, gen_loss = 0.5695962118115395, disc_loss = 0.027219190001806576
Trained batch 628 in epoch 0, gen_loss = 0.5695613118537091, disc_loss = 0.0271801026236334
Trained batch 629 in epoch 0, gen_loss = 0.5693812810712391, disc_loss = 0.02714081932956885
Trained batch 630 in epoch 0, gen_loss = 0.5693554797471042, disc_loss = 0.027102097056595026
Trained batch 631 in epoch 0, gen_loss = 0.5691800106175339, disc_loss = 0.02706466498766301
Trained batch 632 in epoch 0, gen_loss = 0.5690771205730348, disc_loss = 0.027026704703251674
Trained batch 633 in epoch 0, gen_loss = 0.5688298224274668, disc_loss = 0.026987253332235894
Trained batch 634 in epoch 0, gen_loss = 0.5685765178654137, disc_loss = 0.02694799198134707
Trained batch 635 in epoch 0, gen_loss = 0.5683812545905323, disc_loss = 0.026908928815988567
Trained batch 636 in epoch 0, gen_loss = 0.5682582386434546, disc_loss = 0.026868987899767806
Trained batch 637 in epoch 0, gen_loss = 0.5679907521764312, disc_loss = 0.026828862242232774
Trained batch 638 in epoch 0, gen_loss = 0.5677135484132781, disc_loss = 0.02678997952076395
Trained batch 639 in epoch 0, gen_loss = 0.5674984745681286, disc_loss = 0.026751420774417056
Trained batch 640 in epoch 0, gen_loss = 0.5672597097532239, disc_loss = 0.02671264977065059
Trained batch 641 in epoch 0, gen_loss = 0.5670212056499404, disc_loss = 0.026673368029071547
Trained batch 642 in epoch 0, gen_loss = 0.5668977673743413, disc_loss = 0.02663392091204715
Trained batch 643 in epoch 0, gen_loss = 0.5669198176412849, disc_loss = 0.026595341914075987
Trained batch 644 in epoch 0, gen_loss = 0.5666722244070482, disc_loss = 0.02655583986536015
Trained batch 645 in epoch 0, gen_loss = 0.5664913489169011, disc_loss = 0.026516971851074808
Trained batch 646 in epoch 0, gen_loss = 0.5663430084124599, disc_loss = 0.02647823786580362
Trained batch 647 in epoch 0, gen_loss = 0.5659970844877354, disc_loss = 0.026440611158003224
Trained batch 648 in epoch 0, gen_loss = 0.5659262758833234, disc_loss = 0.02640448585877307
Trained batch 649 in epoch 0, gen_loss = 0.5658272393391682, disc_loss = 0.026368184676752067
Trained batch 650 in epoch 0, gen_loss = 0.5657149753354478, disc_loss = 0.026330523544810885
Trained batch 651 in epoch 0, gen_loss = 0.5654565077617856, disc_loss = 0.026292216767124156
Trained batch 652 in epoch 0, gen_loss = 0.5654671508904439, disc_loss = 0.02625604932838951
Trained batch 653 in epoch 0, gen_loss = 0.56538242903689, disc_loss = 0.0262199471815381
Trained batch 654 in epoch 0, gen_loss = 0.5654767536935006, disc_loss = 0.0261848708513399
Trained batch 655 in epoch 0, gen_loss = 0.5653982442326662, disc_loss = 0.026148477275167285
Trained batch 656 in epoch 0, gen_loss = 0.5655376251611173, disc_loss = 0.02611220246935731
Trained batch 657 in epoch 0, gen_loss = 0.5653830080952688, disc_loss = 0.026074753840854475
Trained batch 658 in epoch 0, gen_loss = 0.5651310471554989, disc_loss = 0.026037058964893164
Trained batch 659 in epoch 0, gen_loss = 0.565392403530352, disc_loss = 0.02600276041370252
Trained batch 660 in epoch 0, gen_loss = 0.5651234985299622, disc_loss = 0.02596553711329112
Trained batch 661 in epoch 0, gen_loss = 0.5649990868892555, disc_loss = 0.025928721214705106
Trained batch 662 in epoch 0, gen_loss = 0.5649170900721715, disc_loss = 0.02589219302408886
Trained batch 663 in epoch 0, gen_loss = 0.5649102955877062, disc_loss = 0.025855689741010462
Trained batch 664 in epoch 0, gen_loss = 0.5646457571284216, disc_loss = 0.025820320248218686
Trained batch 665 in epoch 0, gen_loss = 0.5643540011780398, disc_loss = 0.025784107760138886
Trained batch 666 in epoch 0, gen_loss = 0.5642780970329645, disc_loss = 0.025748432544269496
Trained batch 667 in epoch 0, gen_loss = 0.5642633431418214, disc_loss = 0.02571344564262426
Trained batch 668 in epoch 0, gen_loss = 0.5642690878188842, disc_loss = 0.025678399533502514
Trained batch 669 in epoch 0, gen_loss = 0.5641198917556165, disc_loss = 0.025642670834013052
Trained batch 670 in epoch 0, gen_loss = 0.5639260358408915, disc_loss = 0.025606500120186726
Trained batch 671 in epoch 0, gen_loss = 0.5637916349140661, disc_loss = 0.02557144749187587
Trained batch 672 in epoch 0, gen_loss = 0.5635722949990192, disc_loss = 0.025536490318519
Trained batch 673 in epoch 0, gen_loss = 0.5636423380509329, disc_loss = 0.02550099748988202
Trained batch 674 in epoch 0, gen_loss = 0.563480553317953, disc_loss = 0.025469197103132803
Trained batch 675 in epoch 0, gen_loss = 0.5632872510062167, disc_loss = 0.025439160920062362
Trained batch 676 in epoch 0, gen_loss = 0.5632617260539902, disc_loss = 0.02540577235191433
Trained batch 677 in epoch 0, gen_loss = 0.5631439286698986, disc_loss = 0.02537187823619155
Trained batch 678 in epoch 0, gen_loss = 0.5630921372959996, disc_loss = 0.025336645463507605
Trained batch 679 in epoch 0, gen_loss = 0.5628945011426421, disc_loss = 0.025301004181143498
Trained batch 680 in epoch 0, gen_loss = 0.5627200121659014, disc_loss = 0.025266043025905897
Trained batch 681 in epoch 0, gen_loss = 0.5627571700517733, disc_loss = 0.025231421024359685
Trained batch 682 in epoch 0, gen_loss = 0.5627171729267254, disc_loss = 0.025196974874797277
Trained batch 683 in epoch 0, gen_loss = 0.5626176941203095, disc_loss = 0.025162281817756593
Trained batch 684 in epoch 0, gen_loss = 0.5624403844349576, disc_loss = 0.02512862908307218
Trained batch 685 in epoch 0, gen_loss = 0.5622212495383299, disc_loss = 0.02509495829443825
Trained batch 686 in epoch 0, gen_loss = 0.5622292148407543, disc_loss = 0.025061686496484856
Trained batch 687 in epoch 0, gen_loss = 0.5619249931255053, disc_loss = 0.025027340601386536
Trained batch 688 in epoch 0, gen_loss = 0.5618732501529644, disc_loss = 0.024992956177545076
Trained batch 689 in epoch 0, gen_loss = 0.5616997935633729, disc_loss = 0.02495861926396121
Trained batch 690 in epoch 0, gen_loss = 0.5616681013369526, disc_loss = 0.02492472412456878
Trained batch 691 in epoch 0, gen_loss = 0.5618941988042324, disc_loss = 0.0248936485419066
Trained batch 692 in epoch 0, gen_loss = 0.5618417537401593, disc_loss = 0.02486350143450604
Trained batch 693 in epoch 0, gen_loss = 0.5616473307355336, disc_loss = 0.02483387487257431
Trained batch 694 in epoch 0, gen_loss = 0.5613705226843305, disc_loss = 0.024804081674330205
Trained batch 695 in epoch 0, gen_loss = 0.561236628855782, disc_loss = 0.02477300384894131
Trained batch 696 in epoch 0, gen_loss = 0.5610956698442292, disc_loss = 0.02473966670797141
Trained batch 697 in epoch 0, gen_loss = 0.5610050708482464, disc_loss = 0.02470678656180536
Trained batch 698 in epoch 0, gen_loss = 0.5609632425212724, disc_loss = 0.024674284224263866
Trained batch 699 in epoch 0, gen_loss = 0.5607982222097261, disc_loss = 0.024641323220982614
Trained batch 700 in epoch 0, gen_loss = 0.5608266747201901, disc_loss = 0.02460944241068237
Trained batch 701 in epoch 0, gen_loss = 0.5607643419096613, disc_loss = 0.02457736552582836
Trained batch 702 in epoch 0, gen_loss = 0.560634743989277, disc_loss = 0.02454507485585841
Trained batch 703 in epoch 0, gen_loss = 0.5604473008266226, disc_loss = 0.02451222284898077
Trained batch 704 in epoch 0, gen_loss = 0.5602480316838474, disc_loss = 0.024480040546949185
Trained batch 705 in epoch 0, gen_loss = 0.5600113041340122, disc_loss = 0.02444727020948286
Trained batch 706 in epoch 0, gen_loss = 0.5599514014144261, disc_loss = 0.0244146107060076
Trained batch 707 in epoch 0, gen_loss = 0.5597294045791115, disc_loss = 0.02438297182651063
Trained batch 708 in epoch 0, gen_loss = 0.5596531790553767, disc_loss = 0.02435067562913136
Trained batch 709 in epoch 0, gen_loss = 0.5595479284373808, disc_loss = 0.024318118466713636
Trained batch 710 in epoch 0, gen_loss = 0.5593112642587321, disc_loss = 0.024285671954650992
Trained batch 711 in epoch 0, gen_loss = 0.559110825358147, disc_loss = 0.0242535494355579
Trained batch 712 in epoch 0, gen_loss = 0.5589458355144565, disc_loss = 0.02422176272171415
Trained batch 713 in epoch 0, gen_loss = 0.5587372478483772, disc_loss = 0.024190196940115802
Trained batch 714 in epoch 0, gen_loss = 0.5586181494739506, disc_loss = 0.024158319819985622
Trained batch 715 in epoch 0, gen_loss = 0.5585649157536097, disc_loss = 0.024126601070829007
Trained batch 716 in epoch 0, gen_loss = 0.558443243741324, disc_loss = 0.024094816962012832
Trained batch 717 in epoch 0, gen_loss = 0.5581843058544945, disc_loss = 0.024063016554213605
Trained batch 718 in epoch 0, gen_loss = 0.5580474813060734, disc_loss = 0.0240315539056114
Trained batch 719 in epoch 0, gen_loss = 0.5580614242288802, disc_loss = 0.024001435351318733
Trained batch 720 in epoch 0, gen_loss = 0.5579522450902756, disc_loss = 0.023972400986775496
Trained batch 721 in epoch 0, gen_loss = 0.5577587652487107, disc_loss = 0.02394269493720017
Trained batch 722 in epoch 0, gen_loss = 0.5576384414198653, disc_loss = 0.02391174575571553
Trained batch 723 in epoch 0, gen_loss = 0.5574954584323241, disc_loss = 0.02388060660209527
Trained batch 724 in epoch 0, gen_loss = 0.5575251386083405, disc_loss = 0.023849825572768418
Trained batch 725 in epoch 0, gen_loss = 0.5573889321696331, disc_loss = 0.02381861020670434
Trained batch 726 in epoch 0, gen_loss = 0.557320232814411, disc_loss = 0.02378776160989884
Trained batch 727 in epoch 0, gen_loss = 0.5570674989331554, disc_loss = 0.023757004814387483
Trained batch 728 in epoch 0, gen_loss = 0.5568112024145689, disc_loss = 0.023727044413653194
Trained batch 729 in epoch 0, gen_loss = 0.5565013558897254, disc_loss = 0.023697923504020252
Trained batch 730 in epoch 0, gen_loss = 0.5566242385334583, disc_loss = 0.02366915063247105
Trained batch 731 in epoch 0, gen_loss = 0.5565403923392296, disc_loss = 0.023639669519075562
Trained batch 732 in epoch 0, gen_loss = 0.5562935031208465, disc_loss = 0.023610559402676593
Trained batch 733 in epoch 0, gen_loss = 0.5562305448409323, disc_loss = 0.023582157581519304
Trained batch 734 in epoch 0, gen_loss = 0.5561479472384161, disc_loss = 0.023553746981470256
Trained batch 735 in epoch 0, gen_loss = 0.5559796849265695, disc_loss = 0.02352423925090672
Trained batch 736 in epoch 0, gen_loss = 0.5558823830722307, disc_loss = 0.023496592587006645
Trained batch 737 in epoch 0, gen_loss = 0.5556413568859178, disc_loss = 0.023467798050061415
Trained batch 738 in epoch 0, gen_loss = 0.5554972736616096, disc_loss = 0.023438357235958322
Trained batch 739 in epoch 0, gen_loss = 0.555356271121953, disc_loss = 0.023408690930463726
Trained batch 740 in epoch 0, gen_loss = 0.5555105293810609, disc_loss = 0.023381160359921414
Trained batch 741 in epoch 0, gen_loss = 0.5553618290755306, disc_loss = 0.02335680068443552
Trained batch 742 in epoch 0, gen_loss = 0.5554250633171076, disc_loss = 0.02333117996121244
Trained batch 743 in epoch 0, gen_loss = 0.5553571128797147, disc_loss = 0.02330314595598392
Trained batch 744 in epoch 0, gen_loss = 0.5552854371150868, disc_loss = 0.023275279045423754
Trained batch 745 in epoch 0, gen_loss = 0.5551489474945989, disc_loss = 0.023251745777907493
Trained batch 746 in epoch 0, gen_loss = 0.5551580256885953, disc_loss = 0.02323479681051993
Trained batch 747 in epoch 0, gen_loss = 0.5551320577369017, disc_loss = 0.023219226217513683
Trained batch 748 in epoch 0, gen_loss = 0.5550473187571374, disc_loss = 0.023199369096836307
Trained batch 749 in epoch 0, gen_loss = 0.5549322297175725, disc_loss = 0.023174569558197012
Trained batch 750 in epoch 0, gen_loss = 0.5547250041790237, disc_loss = 0.023146577160055196
Trained batch 751 in epoch 0, gen_loss = 0.554614492672238, disc_loss = 0.02311805901851411
Trained batch 752 in epoch 0, gen_loss = 0.5544997543531902, disc_loss = 0.023090227101842907
Trained batch 753 in epoch 0, gen_loss = 0.5544393261405454, disc_loss = 0.023063743492404358
Trained batch 754 in epoch 0, gen_loss = 0.5544800092447673, disc_loss = 0.023036505503191407
Trained batch 755 in epoch 0, gen_loss = 0.5543118541439375, disc_loss = 0.023007782115311504
Trained batch 756 in epoch 0, gen_loss = 0.5541186479548954, disc_loss = 0.022979690965210905
Trained batch 757 in epoch 0, gen_loss = 0.5539058845952193, disc_loss = 0.022951685052363835
Trained batch 758 in epoch 0, gen_loss = 0.553713905677808, disc_loss = 0.022923183898602234
Trained batch 759 in epoch 0, gen_loss = 0.5536534076066394, disc_loss = 0.022895323217227613
Trained batch 760 in epoch 0, gen_loss = 0.5535828249918177, disc_loss = 0.02286748678433255
Trained batch 761 in epoch 0, gen_loss = 0.5534297419892834, disc_loss = 0.02283982969783833
Trained batch 762 in epoch 0, gen_loss = 0.5533801001224418, disc_loss = 0.022812356232197985
Trained batch 763 in epoch 0, gen_loss = 0.5532319108072999, disc_loss = 0.02278605050648714
Trained batch 764 in epoch 0, gen_loss = 0.553114097959855, disc_loss = 0.022760129592144022
Trained batch 765 in epoch 0, gen_loss = 0.5529403473705287, disc_loss = 0.022733877836082217
Trained batch 766 in epoch 0, gen_loss = 0.5528896325492486, disc_loss = 0.02270860068082343
Trained batch 767 in epoch 0, gen_loss = 0.55282378262685, disc_loss = 0.022682726785205887
Trained batch 768 in epoch 0, gen_loss = 0.552658183095669, disc_loss = 0.022656652983156927
Trained batch 769 in epoch 0, gen_loss = 0.5525604241467141, disc_loss = 0.022629408097172808
Trained batch 770 in epoch 0, gen_loss = 0.5524653240787071, disc_loss = 0.02260164234662808
Trained batch 771 in epoch 0, gen_loss = 0.5522523975202457, disc_loss = 0.02257408122571277
Trained batch 772 in epoch 0, gen_loss = 0.5522318032472593, disc_loss = 0.02254682180368104
Trained batch 773 in epoch 0, gen_loss = 0.552067671211807, disc_loss = 0.02251997769320533
Trained batch 774 in epoch 0, gen_loss = 0.5519514103858701, disc_loss = 0.022493331676889813
Trained batch 775 in epoch 0, gen_loss = 0.551902566527583, disc_loss = 0.02246702299963385
Trained batch 776 in epoch 0, gen_loss = 0.5517703077661178, disc_loss = 0.022439325714071534
Trained batch 777 in epoch 0, gen_loss = 0.551597308338761, disc_loss = 0.022412131166092784
Trained batch 778 in epoch 0, gen_loss = 0.5513642076044242, disc_loss = 0.022384801499942493
Trained batch 779 in epoch 0, gen_loss = 0.5513795431607809, disc_loss = 0.022358389877272435
Trained batch 780 in epoch 0, gen_loss = 0.5512341973341992, disc_loss = 0.022331905659024633
Trained batch 781 in epoch 0, gen_loss = 0.5512739481294856, disc_loss = 0.02230585658850739
Trained batch 782 in epoch 0, gen_loss = 0.5512579627344587, disc_loss = 0.022279126902966073
Trained batch 783 in epoch 0, gen_loss = 0.5512170301636263, disc_loss = 0.022252710481181416
Trained batch 784 in epoch 0, gen_loss = 0.5510936764774809, disc_loss = 0.022226659385902677
Trained batch 785 in epoch 0, gen_loss = 0.5509858263357905, disc_loss = 0.022199937342381716
Trained batch 786 in epoch 0, gen_loss = 0.5508164834642956, disc_loss = 0.02217273426057565
Trained batch 787 in epoch 0, gen_loss = 0.550708847640432, disc_loss = 0.022145601049515296
Trained batch 788 in epoch 0, gen_loss = 0.5506224820369097, disc_loss = 0.022118591133496842
Trained batch 789 in epoch 0, gen_loss = 0.5504316597799711, disc_loss = 0.022092404652864352
Trained batch 790 in epoch 0, gen_loss = 0.5503546047376471, disc_loss = 0.02206606085396898
Trained batch 791 in epoch 0, gen_loss = 0.5503602861379734, disc_loss = 0.022039535958108238
Trained batch 792 in epoch 0, gen_loss = 0.5501540028507742, disc_loss = 0.022012922183588773
Trained batch 793 in epoch 0, gen_loss = 0.5502094809219279, disc_loss = 0.021987510443109697
Trained batch 794 in epoch 0, gen_loss = 0.5500141289249156, disc_loss = 0.021961386553700075
Trained batch 795 in epoch 0, gen_loss = 0.5498375271882244, disc_loss = 0.02193498499385823
Trained batch 796 in epoch 0, gen_loss = 0.5497047527596224, disc_loss = 0.02190891802733077
Trained batch 797 in epoch 0, gen_loss = 0.5497264922934965, disc_loss = 0.021883152498595796
Trained batch 798 in epoch 0, gen_loss = 0.5495448531361485, disc_loss = 0.02185805228625358
Trained batch 799 in epoch 0, gen_loss = 0.549504675231874, disc_loss = 0.021832918367654202
Trained batch 800 in epoch 0, gen_loss = 0.549278741583842, disc_loss = 0.021807569357640368
Trained batch 801 in epoch 0, gen_loss = 0.5492611689832145, disc_loss = 0.021782541603353363
Trained batch 802 in epoch 0, gen_loss = 0.5493067763884961, disc_loss = 0.021759337535029993
Trained batch 803 in epoch 0, gen_loss = 0.5492529197044633, disc_loss = 0.021734331624361742
Trained batch 804 in epoch 0, gen_loss = 0.5492610339422404, disc_loss = 0.02170909317203086
Trained batch 805 in epoch 0, gen_loss = 0.5492388241122438, disc_loss = 0.021685631393911962
Trained batch 806 in epoch 0, gen_loss = 0.5491947215212026, disc_loss = 0.021661936134013984
Trained batch 807 in epoch 0, gen_loss = 0.5491139579306145, disc_loss = 0.021637165407540095
Trained batch 808 in epoch 0, gen_loss = 0.5489906939057396, disc_loss = 0.02161198811312304
Trained batch 809 in epoch 0, gen_loss = 0.5489149158751523, disc_loss = 0.02158709369279318
Trained batch 810 in epoch 0, gen_loss = 0.5488120050303886, disc_loss = 0.021561980590355755
Trained batch 811 in epoch 0, gen_loss = 0.5486524645697894, disc_loss = 0.021536550991907554
Trained batch 812 in epoch 0, gen_loss = 0.5485083085804408, disc_loss = 0.02151110172208724
Trained batch 813 in epoch 0, gen_loss = 0.5484655051691233, disc_loss = 0.02148578894617498
Trained batch 814 in epoch 0, gen_loss = 0.548486665238632, disc_loss = 0.021460856817679858
Trained batch 815 in epoch 0, gen_loss = 0.5484373153497776, disc_loss = 0.02143563022514618
Trained batch 816 in epoch 0, gen_loss = 0.5483218701043356, disc_loss = 0.021410673536984944
Trained batch 817 in epoch 0, gen_loss = 0.5481352218060155, disc_loss = 0.02138553917294705
Trained batch 818 in epoch 0, gen_loss = 0.5481909519295698, disc_loss = 0.021361176085303583
Trained batch 819 in epoch 0, gen_loss = 0.548026765882969, disc_loss = 0.021337622786991344
Trained batch 820 in epoch 0, gen_loss = 0.5479276908808881, disc_loss = 0.02131374681640153
Trained batch 821 in epoch 0, gen_loss = 0.5477580049974785, disc_loss = 0.021289323826791797
Trained batch 822 in epoch 0, gen_loss = 0.5476132636009415, disc_loss = 0.021265475899841124
Trained batch 823 in epoch 0, gen_loss = 0.5475064331611383, disc_loss = 0.021242608983370305
Trained batch 824 in epoch 0, gen_loss = 0.5475014258153511, disc_loss = 0.02122020754539831
Trained batch 825 in epoch 0, gen_loss = 0.5474891625795757, disc_loss = 0.021199187491321374
Trained batch 826 in epoch 0, gen_loss = 0.5475110419572048, disc_loss = 0.021177921319175445
Trained batch 827 in epoch 0, gen_loss = 0.5475262239359427, disc_loss = 0.021156187667252704
Trained batch 828 in epoch 0, gen_loss = 0.5474703053082143, disc_loss = 0.021132688996195567
Trained batch 829 in epoch 0, gen_loss = 0.5472368952021541, disc_loss = 0.02110830069952981
Trained batch 830 in epoch 0, gen_loss = 0.5472432484695627, disc_loss = 0.02108456629389229
Trained batch 831 in epoch 0, gen_loss = 0.5471951291681483, disc_loss = 0.021060960355778383
Trained batch 832 in epoch 0, gen_loss = 0.5472692339217105, disc_loss = 0.021037705593347205
Trained batch 833 in epoch 0, gen_loss = 0.5472219517048028, disc_loss = 0.02101420889527881
Trained batch 834 in epoch 0, gen_loss = 0.5471157954481547, disc_loss = 0.020990715673525152
Trained batch 835 in epoch 0, gen_loss = 0.5469958687013987, disc_loss = 0.020967026528611037
Trained batch 836 in epoch 0, gen_loss = 0.5468412050565653, disc_loss = 0.020943430158976695
Trained batch 837 in epoch 0, gen_loss = 0.546800108079694, disc_loss = 0.02091965203495476
Trained batch 838 in epoch 0, gen_loss = 0.5467063749788487, disc_loss = 0.020895828611505494
Trained batch 839 in epoch 0, gen_loss = 0.5465640946158341, disc_loss = 0.020872269802757294
Trained batch 840 in epoch 0, gen_loss = 0.5465578480438726, disc_loss = 0.02084909407423331
Trained batch 841 in epoch 0, gen_loss = 0.5466913993242517, disc_loss = 0.020826968017176366
Trained batch 842 in epoch 0, gen_loss = 0.546632010989059, disc_loss = 0.020805107236357596
Trained batch 843 in epoch 0, gen_loss = 0.5465451096188966, disc_loss = 0.0207824580580624
Trained batch 844 in epoch 0, gen_loss = 0.5463366521886115, disc_loss = 0.02075995260569923
Trained batch 845 in epoch 0, gen_loss = 0.546254203568959, disc_loss = 0.020737301610615523
Trained batch 846 in epoch 0, gen_loss = 0.5461591346697374, disc_loss = 0.02071455424111897
Trained batch 847 in epoch 0, gen_loss = 0.5460979214184126, disc_loss = 0.02069144523632548
Trained batch 848 in epoch 0, gen_loss = 0.5459990762218008, disc_loss = 0.02066818283133382
Trained batch 849 in epoch 0, gen_loss = 0.5459700163322336, disc_loss = 0.020645084350082256
Trained batch 850 in epoch 0, gen_loss = 0.5459807570126306, disc_loss = 0.020622269504281695
Trained batch 851 in epoch 0, gen_loss = 0.5459116478341286, disc_loss = 0.020599884132648592
Trained batch 852 in epoch 0, gen_loss = 0.5458728068039816, disc_loss = 0.0205774846418372
Trained batch 853 in epoch 0, gen_loss = 0.5457643790113842, disc_loss = 0.020554698820417286
Trained batch 854 in epoch 0, gen_loss = 0.5457273545669533, disc_loss = 0.020532101425628193
Trained batch 855 in epoch 0, gen_loss = 0.5455401106345876, disc_loss = 0.020509490406749417
Trained batch 856 in epoch 0, gen_loss = 0.5454007935565717, disc_loss = 0.020486789547347613
Trained batch 857 in epoch 0, gen_loss = 0.5453484812469194, disc_loss = 0.020464290403514027
Trained batch 858 in epoch 0, gen_loss = 0.5453156555836914, disc_loss = 0.02044254540482306
Trained batch 859 in epoch 0, gen_loss = 0.5452332051340923, disc_loss = 0.020420517488639954
Trained batch 860 in epoch 0, gen_loss = 0.5451546055583866, disc_loss = 0.020398627393557196
Trained batch 861 in epoch 0, gen_loss = 0.5449212867325807, disc_loss = 0.020376840749555085
Trained batch 862 in epoch 0, gen_loss = 0.5447789328399428, disc_loss = 0.020354972429304282
Trained batch 863 in epoch 0, gen_loss = 0.5447117635566328, disc_loss = 0.020332870371863397
Trained batch 864 in epoch 0, gen_loss = 0.544539695185733, disc_loss = 0.020310533730746542
Trained batch 865 in epoch 0, gen_loss = 0.544427539335938, disc_loss = 0.02028839854185048
Trained batch 866 in epoch 0, gen_loss = 0.544451619239552, disc_loss = 0.02026778805149384
Trained batch 867 in epoch 0, gen_loss = 0.5443626658364376, disc_loss = 0.02024772511195651
Trained batch 868 in epoch 0, gen_loss = 0.544226702015474, disc_loss = 0.020227415183673197
Trained batch 869 in epoch 0, gen_loss = 0.5441694211685795, disc_loss = 0.020206034227057586
Trained batch 870 in epoch 0, gen_loss = 0.5441381810863858, disc_loss = 0.020184347099888145
Trained batch 871 in epoch 0, gen_loss = 0.5439671998450516, disc_loss = 0.020163293794871397
Trained batch 872 in epoch 0, gen_loss = 0.5440102593049424, disc_loss = 0.020142747351101237
Trained batch 873 in epoch 0, gen_loss = 0.5438471781239084, disc_loss = 0.020121782817582128
Trained batch 874 in epoch 0, gen_loss = 0.5437169255188533, disc_loss = 0.02010060240201918
Trained batch 875 in epoch 0, gen_loss = 0.5435623513985443, disc_loss = 0.02007979848699859
Trained batch 876 in epoch 0, gen_loss = 0.5434171581553761, disc_loss = 0.02005942946517874
Trained batch 877 in epoch 0, gen_loss = 0.5433766436264564, disc_loss = 0.020039732479195986
Trained batch 878 in epoch 0, gen_loss = 0.5432852195730524, disc_loss = 0.020020453557406528
Trained batch 879 in epoch 0, gen_loss = 0.5431544943966649, disc_loss = 0.020000446810908297
Trained batch 880 in epoch 0, gen_loss = 0.543095612431222, disc_loss = 0.019979429489995853
Trained batch 881 in epoch 0, gen_loss = 0.5430269453868845, disc_loss = 0.01995798561025396
Trained batch 882 in epoch 0, gen_loss = 0.5429048770753876, disc_loss = 0.01993713590786091
Trained batch 883 in epoch 0, gen_loss = 0.5428948766168427, disc_loss = 0.019915864607664268
Trained batch 884 in epoch 0, gen_loss = 0.5428814192612966, disc_loss = 0.019894434636035185
Trained batch 885 in epoch 0, gen_loss = 0.5427441932008713, disc_loss = 0.019872989367236936
Trained batch 886 in epoch 0, gen_loss = 0.5426816064045421, disc_loss = 0.019852321013467535
Trained batch 887 in epoch 0, gen_loss = 0.5425511258612344, disc_loss = 0.01983172226761969
Trained batch 888 in epoch 0, gen_loss = 0.54241295070026, disc_loss = 0.019810572012992318
Trained batch 889 in epoch 0, gen_loss = 0.5423477107889196, disc_loss = 0.019789346164024404
Trained batch 890 in epoch 0, gen_loss = 0.5422492506595038, disc_loss = 0.01976842770687964
Trained batch 891 in epoch 0, gen_loss = 0.5420493856740639, disc_loss = 0.019748857371574852
Trained batch 892 in epoch 0, gen_loss = 0.5420901405250639, disc_loss = 0.019730291063250704
Trained batch 893 in epoch 0, gen_loss = 0.5421000138691875, disc_loss = 0.019711073295924794
Trained batch 894 in epoch 0, gen_loss = 0.541928657639626, disc_loss = 0.01969050485328589
Trained batch 895 in epoch 0, gen_loss = 0.5417709286723819, disc_loss = 0.019669810361067772
Trained batch 896 in epoch 0, gen_loss = 0.5416805486017247, disc_loss = 0.01964907012160851
Trained batch 897 in epoch 0, gen_loss = 0.5415922214499029, disc_loss = 0.019628313650456954
Trained batch 898 in epoch 0, gen_loss = 0.541536250687282, disc_loss = 0.019608387255443425
Trained batch 899 in epoch 0, gen_loss = 0.5414956455098258, disc_loss = 0.019588554081662247
Trained batch 900 in epoch 0, gen_loss = 0.541414507948995, disc_loss = 0.019568018189638638
Trained batch 901 in epoch 0, gen_loss = 0.5414202739288432, disc_loss = 0.019548082046629965
Trained batch 902 in epoch 0, gen_loss = 0.5412232635887226, disc_loss = 0.01952931935102746
Trained batch 903 in epoch 0, gen_loss = 0.5410587038614053, disc_loss = 0.019510957315838195
Trained batch 904 in epoch 0, gen_loss = 0.5407701055977227, disc_loss = 0.019503385609900836
Trained batch 905 in epoch 0, gen_loss = 0.5406617187374738, disc_loss = 0.019496966725180365
Trained batch 906 in epoch 0, gen_loss = 0.540657819500306, disc_loss = 0.01947932699289637
Trained batch 907 in epoch 0, gen_loss = 0.5406334066706082, disc_loss = 0.019462186273132013
Trained batch 908 in epoch 0, gen_loss = 0.5407029366729283, disc_loss = 0.019458529872643557
Trained batch 909 in epoch 0, gen_loss = 0.540758893254039, disc_loss = 0.01947032552197125
Trained batch 910 in epoch 0, gen_loss = 0.5408385696316917, disc_loss = 0.019459032129780538
Trained batch 911 in epoch 0, gen_loss = 0.5407747175675213, disc_loss = 0.019460372449350217
Trained batch 912 in epoch 0, gen_loss = 0.5407796166225174, disc_loss = 0.019446325447840813
Trained batch 913 in epoch 0, gen_loss = 0.5406825247361311, disc_loss = 0.019446475779040587
Trained batch 914 in epoch 0, gen_loss = 0.5405983257814835, disc_loss = 0.019489116324722524
Trained batch 915 in epoch 0, gen_loss = 0.5404423479820443, disc_loss = 0.01949210833543586
Testing Epoch 0
Traceback (most recent call last):
  File "esrgan_bones.py", line 318, in <module>
    gen_hr = generator(imgs_lr)
  File "/work3/soeba/HALOS/halos/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "esrgan_bones.py", line 228, in forward
    trunk = self.trunk_conv(self.RRDB_trunk(fea))
  File "/work3/soeba/HALOS/halos/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/work3/soeba/HALOS/halos/lib/python3.8/site-packages/torch/nn/modules/container.py", line 139, in forward
    input = module(input)
  File "/work3/soeba/HALOS/halos/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "esrgan_bones.py", line 206, in forward
    out = self.RDB3(out)
  File "/work3/soeba/HALOS/halos/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "esrgan_bones.py", line 189, in forward
    x4 = self.lrelu(self.conv4(torch.cat((x, x1, x2, x3), 1)))
  File "/work3/soeba/HALOS/halos/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/work3/soeba/HALOS/halos/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 457, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/work3/soeba/HALOS/halos/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 453, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 31.75 GiB total capacity; 28.70 GiB already allocated; 3.69 MiB free; 30.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Training Epoch 1