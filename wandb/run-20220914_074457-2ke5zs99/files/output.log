/work3/soeba/HALOS/halos/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.
  warnings.warn(
/work3/soeba/HALOS/halos/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Training Epoch 0
Trained batch 0 in epoch 0, gen_loss = 1.6882851123809814, disc_loss = 0.4694347083568573
Trained batch 1 in epoch 0, gen_loss = 1.3225319981575012, disc_loss = 0.5651452392339706
Trained batch 2 in epoch 0, gen_loss = 1.132317801316579, disc_loss = 0.6194352805614471
Trained batch 3 in epoch 0, gen_loss = 1.0947790294885635, disc_loss = 0.5441776886582375
Trained batch 4 in epoch 0, gen_loss = 1.015294075012207, disc_loss = 0.4933931827545166
Trained batch 5 in epoch 0, gen_loss = 0.9721970558166504, disc_loss = 0.4546819031238556
Trained batch 6 in epoch 0, gen_loss = 0.9276086602892194, disc_loss = 0.4222880814756666
Trained batch 7 in epoch 0, gen_loss = 0.8842533677816391, disc_loss = 0.4000983498990536
Trained batch 8 in epoch 0, gen_loss = 0.8538624975416396, disc_loss = 0.3842744595474667
Trained batch 9 in epoch 0, gen_loss = 0.8259390950202942, disc_loss = 0.36756872832775117
Trained batch 10 in epoch 0, gen_loss = 0.8201364007863131, disc_loss = 0.35441730510104785
Trained batch 11 in epoch 0, gen_loss = 0.8104317386945089, disc_loss = 0.3413805514574051
Trained batch 12 in epoch 0, gen_loss = 0.7973522131259625, disc_loss = 0.3277257268245404
Trained batch 13 in epoch 0, gen_loss = 0.7888347847121102, disc_loss = 0.3185629663722856
Trained batch 14 in epoch 0, gen_loss = 0.7788282712300618, disc_loss = 0.31100143591562907
Trained batch 15 in epoch 0, gen_loss = 0.7740676887333393, disc_loss = 0.30378222465515137
Trained batch 16 in epoch 0, gen_loss = 0.7668040184413686, disc_loss = 0.29620469755986156
Trained batch 17 in epoch 0, gen_loss = 0.7701439592573378, disc_loss = 0.2891191865007083
Trained batch 18 in epoch 0, gen_loss = 0.7670565812211287, disc_loss = 0.28092378770050247
Trained batch 19 in epoch 0, gen_loss = 0.7642616599798202, disc_loss = 0.2756127819418907
Trained batch 20 in epoch 0, gen_loss = 0.7587455567859468, disc_loss = 0.2683997704159646
Trained batch 21 in epoch 0, gen_loss = 0.7574925205924294, disc_loss = 0.2654009336097674
Trained batch 22 in epoch 0, gen_loss = 0.751976401909538, disc_loss = 0.25989201826893765
Trained batch 23 in epoch 0, gen_loss = 0.7526762162645658, disc_loss = 0.2545760202532013
Trained batch 24 in epoch 0, gen_loss = 0.7513754057884217, disc_loss = 0.24829373240470887
Trained batch 25 in epoch 0, gen_loss = 0.7478184081040896, disc_loss = 0.24176500737667084
Trained batch 26 in epoch 0, gen_loss = 0.7479154114369992, disc_loss = 0.2398476528900641
Trained batch 27 in epoch 0, gen_loss = 0.745961627789906, disc_loss = 0.23520303278097085
Trained batch 28 in epoch 0, gen_loss = 0.7441741392530244, disc_loss = 0.2308108403251089
Trained batch 29 in epoch 0, gen_loss = 0.7416326582431794, disc_loss = 0.2258266953130563
Trained batch 30 in epoch 0, gen_loss = 0.7426734220597052, disc_loss = 0.22073764666434256
Trained batch 31 in epoch 0, gen_loss = 0.736256955191493, disc_loss = 0.21707282634451985
Trained batch 32 in epoch 0, gen_loss = 0.7345032800327648, disc_loss = 0.2131375422080358
Trained batch 33 in epoch 0, gen_loss = 0.7313161895555609, disc_loss = 0.20906060714932048
Trained batch 34 in epoch 0, gen_loss = 0.7327225685119629, disc_loss = 0.20561468665088925
Trained batch 35 in epoch 0, gen_loss = 0.7318250570032332, disc_loss = 0.20224040891561243
Trained batch 36 in epoch 0, gen_loss = 0.7285804603550885, disc_loss = 0.1994116783544824
Trained batch 37 in epoch 0, gen_loss = 0.7284714924661737, disc_loss = 0.19613218817271685
Trained batch 38 in epoch 0, gen_loss = 0.7238058478404314, disc_loss = 0.19322432978795126
Trained batch 39 in epoch 0, gen_loss = 0.7246627599000931, disc_loss = 0.19200886990875005
Trained batch 40 in epoch 0, gen_loss = 0.7265394315487002, disc_loss = 0.1894688317325057
Trained batch 41 in epoch 0, gen_loss = 0.7231830386888414, disc_loss = 0.1862910802343062
Trained batch 42 in epoch 0, gen_loss = 0.7224222102830576, disc_loss = 0.18351318817152534
Trained batch 43 in epoch 0, gen_loss = 0.721526406028054, disc_loss = 0.18174446077848022
Trained batch 44 in epoch 0, gen_loss = 0.7198774682150947, disc_loss = 0.17926613349053594
Trained batch 45 in epoch 0, gen_loss = 0.7174493709336156, disc_loss = 0.17681689714284046
Trained batch 46 in epoch 0, gen_loss = 0.7158992049541879, disc_loss = 0.17451371767736495
Trained batch 47 in epoch 0, gen_loss = 0.7160476793845495, disc_loss = 0.17243548459373415
Trained batch 48 in epoch 0, gen_loss = 0.7159919787426384, disc_loss = 0.1698979565835729
Trained batch 49 in epoch 0, gen_loss = 0.7119345200061798, disc_loss = 0.1675690183788538
Trained batch 50 in epoch 0, gen_loss = 0.7104139935736563, disc_loss = 0.16496649932335405
Trained batch 51 in epoch 0, gen_loss = 0.7089320214895102, disc_loss = 0.16250913217663765
Trained batch 52 in epoch 0, gen_loss = 0.7053495443092203, disc_loss = 0.16115292105472312
Trained batch 53 in epoch 0, gen_loss = 0.7057987937220821, disc_loss = 0.16081072017550468
Trained batch 54 in epoch 0, gen_loss = 0.7041852355003357, disc_loss = 0.15919804803349755
Trained batch 55 in epoch 0, gen_loss = 0.7021211917911258, disc_loss = 0.1577488829248718
Trained batch 56 in epoch 0, gen_loss = 0.7014608445920443, disc_loss = 0.15587802767230755
Trained batch 57 in epoch 0, gen_loss = 0.7016196805855324, disc_loss = 0.15410505402190933
Trained batch 58 in epoch 0, gen_loss = 0.7005822173619675, disc_loss = 0.15220547884197558
Trained batch 59 in epoch 0, gen_loss = 0.6993262887001037, disc_loss = 0.15042647290974856
Trained batch 60 in epoch 0, gen_loss = 0.6987965400101709, disc_loss = 0.14870388305089513
Trained batch 61 in epoch 0, gen_loss = 0.6976749973912393, disc_loss = 0.147014420840048
Trained batch 62 in epoch 0, gen_loss = 0.6956802833647955, disc_loss = 0.14578052645637876
Trained batch 63 in epoch 0, gen_loss = 0.6945547116920352, disc_loss = 0.14484432409517467
Trained batch 64 in epoch 0, gen_loss = 0.6922352754152739, disc_loss = 0.14323866355877657
Trained batch 65 in epoch 0, gen_loss = 0.6920669692935366, disc_loss = 0.1415669738004605
Trained batch 66 in epoch 0, gen_loss = 0.6897123456001282, disc_loss = 0.14001404154878944
Trained batch 67 in epoch 0, gen_loss = 0.6895695959820467, disc_loss = 0.1387101706555661
Trained batch 68 in epoch 0, gen_loss = 0.6887512794439343, disc_loss = 0.137475389285364
Trained batch 69 in epoch 0, gen_loss = 0.6865638775484902, disc_loss = 0.1361416962529932
Trained batch 70 in epoch 0, gen_loss = 0.6865971449395301, disc_loss = 0.13499021740026876
Trained batch 71 in epoch 0, gen_loss = 0.6869998921950659, disc_loss = 0.13353410940099922
Trained batch 72 in epoch 0, gen_loss = 0.685417651313625, disc_loss = 0.1323264454274553
Trained batch 73 in epoch 0, gen_loss = 0.684821605682373, disc_loss = 0.13111448929821318
Trained batch 74 in epoch 0, gen_loss = 0.6825519712766012, disc_loss = 0.13107928591469922
Trained batch 75 in epoch 0, gen_loss = 0.6828206134469885, disc_loss = 0.1315652137671254
Trained batch 76 in epoch 0, gen_loss = 0.6821742545474659, disc_loss = 0.130523709918965
Trained batch 77 in epoch 0, gen_loss = 0.6813123187957666, disc_loss = 0.12958606635817352
Trained batch 78 in epoch 0, gen_loss = 0.6810513478291186, disc_loss = 0.12888426746277115
Trained batch 79 in epoch 0, gen_loss = 0.6798958986997604, disc_loss = 0.12776181718800217
Trained batch 80 in epoch 0, gen_loss = 0.677788750624951, disc_loss = 0.1266935696526442
Trained batch 81 in epoch 0, gen_loss = 0.6795878744706875, disc_loss = 0.1259388504549861
Trained batch 82 in epoch 0, gen_loss = 0.6791171517716833, disc_loss = 0.124906750216362
Trained batch 83 in epoch 0, gen_loss = 0.6799295941988627, disc_loss = 0.12391149214956731
Trained batch 84 in epoch 0, gen_loss = 0.6815057207556332, disc_loss = 0.1229305560755379
Trained batch 85 in epoch 0, gen_loss = 0.6820148077122, disc_loss = 0.1219744671699266
Trained batch 86 in epoch 0, gen_loss = 0.6820253209135998, disc_loss = 0.12100342470595891
Trained batch 87 in epoch 0, gen_loss = 0.6811560609123923, disc_loss = 0.11996705260720443
Trained batch 88 in epoch 0, gen_loss = 0.6806700872571281, disc_loss = 0.11890674125026451
Trained batch 89 in epoch 0, gen_loss = 0.6790133125252193, disc_loss = 0.11785547468397352
Trained batch 90 in epoch 0, gen_loss = 0.678253805244362, disc_loss = 0.11691761180594727
Trained batch 91 in epoch 0, gen_loss = 0.6784719274095867, disc_loss = 0.11597625664232866
Trained batch 92 in epoch 0, gen_loss = 0.6775319595490733, disc_loss = 0.11493009066469567
Trained batch 93 in epoch 0, gen_loss = 0.6774918126299027, disc_loss = 0.11400316542688202
Trained batch 94 in epoch 0, gen_loss = 0.6771037384083397, disc_loss = 0.11313623015425707
Trained batch 95 in epoch 0, gen_loss = 0.6768541634082794, disc_loss = 0.11219739287237947
Trained batch 96 in epoch 0, gen_loss = 0.676766189103274, disc_loss = 0.11125512461456441
Trained batch 97 in epoch 0, gen_loss = 0.6752596400222, disc_loss = 0.11136617505808874
Trained batch 98 in epoch 0, gen_loss = 0.6743279587138783, disc_loss = 0.11204711033614596
Trained batch 99 in epoch 0, gen_loss = 0.6751476693153381, disc_loss = 0.1112878324277699
Trained batch 100 in epoch 0, gen_loss = 0.6742139972082459, disc_loss = 0.11090485834618016
Trained batch 101 in epoch 0, gen_loss = 0.6746257637061325, disc_loss = 0.11010367193204515
Trained batch 102 in epoch 0, gen_loss = 0.6754445572501248, disc_loss = 0.10931700289032413
Trained batch 103 in epoch 0, gen_loss = 0.6733912011751761, disc_loss = 0.10866887514622739
Trained batch 104 in epoch 0, gen_loss = 0.6729777892430623, disc_loss = 0.10788656959221476
Trained batch 105 in epoch 0, gen_loss = 0.6737377458023575, disc_loss = 0.1071021382187335
Trained batch 106 in epoch 0, gen_loss = 0.6731899531088142, disc_loss = 0.10635210852199625
Trained batch 107 in epoch 0, gen_loss = 0.6720960581744159, disc_loss = 0.10556268710987987
Trained batch 108 in epoch 0, gen_loss = 0.6719654650863157, disc_loss = 0.10480241581729245
Trained batch 109 in epoch 0, gen_loss = 0.6711637903343547, disc_loss = 0.10400339407338337
Trained batch 110 in epoch 0, gen_loss = 0.6711129852243372, disc_loss = 0.10320588817970978
Trained batch 111 in epoch 0, gen_loss = 0.6711409320788724, disc_loss = 0.10244183850175302
Trained batch 112 in epoch 0, gen_loss = 0.6705150646446025, disc_loss = 0.10167138030406385
Trained batch 113 in epoch 0, gen_loss = 0.6692566751388082, disc_loss = 0.10089195739445195
Trained batch 114 in epoch 0, gen_loss = 0.6667335230371226, disc_loss = 0.10039637105782395
Trained batch 115 in epoch 0, gen_loss = 0.6648530032614182, disc_loss = 0.09978623682600928
Trained batch 116 in epoch 0, gen_loss = 0.6650091819783561, disc_loss = 0.09931182113882059
Trained batch 117 in epoch 0, gen_loss = 0.6647810534400455, disc_loss = 0.09870098652002418
Trained batch 118 in epoch 0, gen_loss = 0.665037520542866, disc_loss = 0.09802941942759671
Trained batch 119 in epoch 0, gen_loss = 0.664406419545412, disc_loss = 0.09732355084270239
Trained batch 120 in epoch 0, gen_loss = 0.6644488344015169, disc_loss = 0.0967349269050212
Trained batch 121 in epoch 0, gen_loss = 0.6643987749932242, disc_loss = 0.09616079782975502
Trained batch 122 in epoch 0, gen_loss = 0.6627234656636308, disc_loss = 0.09559965135181338
Trained batch 123 in epoch 0, gen_loss = 0.6634389695621306, disc_loss = 0.09497056999093582
Trained batch 124 in epoch 0, gen_loss = 0.6623652997016907, disc_loss = 0.09445935101807118
Trained batch 125 in epoch 0, gen_loss = 0.6614577131611961, disc_loss = 0.09387013458070301
Trained batch 126 in epoch 0, gen_loss = 0.6603985874671635, disc_loss = 0.09334796274036873
Trained batch 127 in epoch 0, gen_loss = 0.6599380346015096, disc_loss = 0.0928801505942829
Trained batch 128 in epoch 0, gen_loss = 0.6586903229702351, disc_loss = 0.0922955936181915
Trained batch 129 in epoch 0, gen_loss = 0.6575286459464293, disc_loss = 0.09169112294912338
Trained batch 130 in epoch 0, gen_loss = 0.6565695730329469, disc_loss = 0.09108866095002599
Trained batch 131 in epoch 0, gen_loss = 0.6575522285067674, disc_loss = 0.09060669041972494
Trained batch 132 in epoch 0, gen_loss = 0.6576658395448125, disc_loss = 0.09029296226099245
Trained batch 133 in epoch 0, gen_loss = 0.6560603738275926, disc_loss = 0.09205567398205844
Trained batch 134 in epoch 0, gen_loss = 0.6556842633971461, disc_loss = 0.09188332597000731
Trained batch 135 in epoch 0, gen_loss = 0.6552917731597143, disc_loss = 0.09179577538880575
Trained batch 136 in epoch 0, gen_loss = 0.6539705909081619, disc_loss = 0.09141061648753655
Trained batch 137 in epoch 0, gen_loss = 0.6527885040943173, disc_loss = 0.09169068881917906
Trained batch 138 in epoch 0, gen_loss = 0.6527538100163713, disc_loss = 0.09169254798388524
Trained batch 139 in epoch 0, gen_loss = 0.6533470918025289, disc_loss = 0.09162577016146055
Trained batch 140 in epoch 0, gen_loss = 0.653728592691692, disc_loss = 0.09118019523270798
Trained batch 141 in epoch 0, gen_loss = 0.65244010321691, disc_loss = 0.0911758447944803
Trained batch 142 in epoch 0, gen_loss = 0.6522627929290692, disc_loss = 0.09079169566007135
Trained batch 143 in epoch 0, gen_loss = 0.6520774544527134, disc_loss = 0.09034545220977937
Trained batch 144 in epoch 0, gen_loss = 0.6521097715558677, disc_loss = 0.08991957446239118
Trained batch 145 in epoch 0, gen_loss = 0.6508769303152006, disc_loss = 0.08969902135559345
Trained batch 146 in epoch 0, gen_loss = 0.6510167539525195, disc_loss = 0.08932904088806336
Trained batch 147 in epoch 0, gen_loss = 0.6517011277578972, disc_loss = 0.08888986183808663
Trained batch 148 in epoch 0, gen_loss = 0.6512880237310524, disc_loss = 0.08853715523802394
Trained batch 149 in epoch 0, gen_loss = 0.6501144727071126, disc_loss = 0.08807400552555919
Trained batch 150 in epoch 0, gen_loss = 0.6486454831053879, disc_loss = 0.08774152790991874
Trained batch 151 in epoch 0, gen_loss = 0.6486848565308672, disc_loss = 0.08731498148316812
Trained batch 152 in epoch 0, gen_loss = 0.6481994522942437, disc_loss = 0.08684738192804693
Trained batch 153 in epoch 0, gen_loss = 0.6464049936114967, disc_loss = 0.08649890014127672
Trained batch 154 in epoch 0, gen_loss = 0.6466070055961609, disc_loss = 0.08610951387954335
Trained batch 155 in epoch 0, gen_loss = 0.6462568583396765, disc_loss = 0.08564358389076705
Trained batch 156 in epoch 0, gen_loss = 0.645749917835187, disc_loss = 0.08519498885840557
Trained batch 157 in epoch 0, gen_loss = 0.64563388537757, disc_loss = 0.08478064174304091
Trained batch 158 in epoch 0, gen_loss = 0.6461922466380041, disc_loss = 0.08434745275163613
Trained batch 159 in epoch 0, gen_loss = 0.6467533811926842, disc_loss = 0.08389415311976336
Trained batch 160 in epoch 0, gen_loss = 0.6462957740570447, disc_loss = 0.08348113456336052
Trained batch 161 in epoch 0, gen_loss = 0.6452273987693551, disc_loss = 0.08305907131598135
Trained batch 162 in epoch 0, gen_loss = 0.6444484946186557, disc_loss = 0.08261198018523265
Trained batch 163 in epoch 0, gen_loss = 0.6441171416422216, disc_loss = 0.08222030986259442
Trained batch 164 in epoch 0, gen_loss = 0.6436953595190337, disc_loss = 0.0819378595178326
Trained batch 165 in epoch 0, gen_loss = 0.6437004319874637, disc_loss = 0.08162855048921991
Trained batch 166 in epoch 0, gen_loss = 0.6434167088148837, disc_loss = 0.08122893771390893
Trained batch 167 in epoch 0, gen_loss = 0.6426401507286799, disc_loss = 0.08081313030284253
Trained batch 168 in epoch 0, gen_loss = 0.6420790450812797, disc_loss = 0.08042970593445577
Trained batch 169 in epoch 0, gen_loss = 0.6418329060077668, disc_loss = 0.08003990709891214
Trained batch 170 in epoch 0, gen_loss = 0.642304725814284, disc_loss = 0.07968069372796699
Trained batch 171 in epoch 0, gen_loss = 0.6425609879715498, disc_loss = 0.07932560067342291
Trained batch 172 in epoch 0, gen_loss = 0.6423721396165087, disc_loss = 0.07895179619377403
Trained batch 173 in epoch 0, gen_loss = 0.6422313506575836, disc_loss = 0.07858476310368927
Trained batch 174 in epoch 0, gen_loss = 0.6421692630222865, disc_loss = 0.07818980881678207
Trained batch 175 in epoch 0, gen_loss = 0.6418632820925929, disc_loss = 0.07784014883641661
Trained batch 176 in epoch 0, gen_loss = 0.6417446405874134, disc_loss = 0.0774933285432431
Trained batch 177 in epoch 0, gen_loss = 0.6410507104370031, disc_loss = 0.0771192766024909
Trained batch 178 in epoch 0, gen_loss = 0.6402145599186754, disc_loss = 0.07680065464228392
Trained batch 179 in epoch 0, gen_loss = 0.6397194513016277, disc_loss = 0.07644035242394441
Trained batch 180 in epoch 0, gen_loss = 0.6395819053794798, disc_loss = 0.07609480950796144
Trained batch 181 in epoch 0, gen_loss = 0.6396298898117883, disc_loss = 0.07577428455044935
Trained batch 182 in epoch 0, gen_loss = 0.6391766007806434, disc_loss = 0.07544123838385923
Trained batch 183 in epoch 0, gen_loss = 0.6385109037484812, disc_loss = 0.07509970724704149
Trained batch 184 in epoch 0, gen_loss = 0.6376747471255225, disc_loss = 0.0747825534762563
Trained batch 185 in epoch 0, gen_loss = 0.63806411543841, disc_loss = 0.07446799447299332
Trained batch 186 in epoch 0, gen_loss = 0.6381962383813399, disc_loss = 0.07413418767326337
Trained batch 187 in epoch 0, gen_loss = 0.6375626609363454, disc_loss = 0.07380814029973873
Trained batch 188 in epoch 0, gen_loss = 0.6377640008611023, disc_loss = 0.07356877478145102
Trained batch 189 in epoch 0, gen_loss = 0.6371562655034818, disc_loss = 0.0732358845244897
Trained batch 190 in epoch 0, gen_loss = 0.6368479814516936, disc_loss = 0.07290508673667752
Trained batch 191 in epoch 0, gen_loss = 0.6369374960971376, disc_loss = 0.07261996477124437
Trained batch 192 in epoch 0, gen_loss = 0.6366768644572539, disc_loss = 0.07230286364361567
Trained batch 193 in epoch 0, gen_loss = 0.6364442166901126, disc_loss = 0.07199087221482672
Trained batch 194 in epoch 0, gen_loss = 0.6363083654489272, disc_loss = 0.07167634577132188
Trained batch 195 in epoch 0, gen_loss = 0.636737911220716, disc_loss = 0.0713953827317728
Trained batch 196 in epoch 0, gen_loss = 0.6368675632827778, disc_loss = 0.07109057801175238
Trained batch 197 in epoch 0, gen_loss = 0.6364013277220003, disc_loss = 0.07079010652472274
Trained batch 198 in epoch 0, gen_loss = 0.6359973916755849, disc_loss = 0.07047660330581905
Trained batch 199 in epoch 0, gen_loss = 0.6359466476738453, disc_loss = 0.0701703122863546
Trained batch 200 in epoch 0, gen_loss = 0.6359423557620737, disc_loss = 0.06986830251486
Trained batch 201 in epoch 0, gen_loss = 0.6356750371137468, disc_loss = 0.0695660584467915
Trained batch 202 in epoch 0, gen_loss = 0.6351178651079169, disc_loss = 0.06926605829354283
Trained batch 203 in epoch 0, gen_loss = 0.6352005441691361, disc_loss = 0.06896052093646836
Trained batch 204 in epoch 0, gen_loss = 0.6345191643005464, disc_loss = 0.06866414247580417
Trained batch 205 in epoch 0, gen_loss = 0.6337585741455115, disc_loss = 0.06837456648066206
Trained batch 206 in epoch 0, gen_loss = 0.6336107616839202, disc_loss = 0.06809972850200922
Trained batch 207 in epoch 0, gen_loss = 0.6337669491767883, disc_loss = 0.06782171907028757
Trained batch 208 in epoch 0, gen_loss = 0.6339758201078936, disc_loss = 0.06755171124890233
Trained batch 209 in epoch 0, gen_loss = 0.6343347821916853, disc_loss = 0.06729782724398232
Trained batch 210 in epoch 0, gen_loss = 0.6346836400823006, disc_loss = 0.06702780037659321
Trained batch 211 in epoch 0, gen_loss = 0.634199746655968, disc_loss = 0.06674337791683117
Trained batch 212 in epoch 0, gen_loss = 0.6338918514095002, disc_loss = 0.06646912316726127
Trained batch 213 in epoch 0, gen_loss = 0.6339049464631303, disc_loss = 0.06619477673503353
Trained batch 214 in epoch 0, gen_loss = 0.6334096021430436, disc_loss = 0.06591674632414482
Trained batch 215 in epoch 0, gen_loss = 0.6332524678221455, disc_loss = 0.06565450837930527
Trained batch 216 in epoch 0, gen_loss = 0.6322504375387447, disc_loss = 0.0653961587864648
Trained batch 217 in epoch 0, gen_loss = 0.631788017552927, disc_loss = 0.06513809163012294
Trained batch 218 in epoch 0, gen_loss = 0.630699793225554, disc_loss = 0.06489093245121919
Trained batch 219 in epoch 0, gen_loss = 0.6309170256961476, disc_loss = 0.06465653852263296
Trained batch 220 in epoch 0, gen_loss = 0.6307485893840703, disc_loss = 0.0643959405612858
Trained batch 221 in epoch 0, gen_loss = 0.6300581656060777, disc_loss = 0.06414525062558954
Trained batch 222 in epoch 0, gen_loss = 0.6295469681243725, disc_loss = 0.06388414410389792
Trained batch 223 in epoch 0, gen_loss = 0.6286481076053211, disc_loss = 0.06362173702759069
Trained batch 224 in epoch 0, gen_loss = 0.6285359117719862, disc_loss = 0.06337649060413242
Trained batch 225 in epoch 0, gen_loss = 0.6284281345067826, disc_loss = 0.06313416494295594
Trained batch 226 in epoch 0, gen_loss = 0.6285018125294589, disc_loss = 0.0628904590797549
Trained batch 227 in epoch 0, gen_loss = 0.627886723400208, disc_loss = 0.06264259003862542
Trained batch 228 in epoch 0, gen_loss = 0.628109950816267, disc_loss = 0.062405039440417524
Trained batch 229 in epoch 0, gen_loss = 0.6277164369821548, disc_loss = 0.06216790121980011
Trained batch 230 in epoch 0, gen_loss = 0.6271881467594213, disc_loss = 0.0619296634718937
Trained batch 231 in epoch 0, gen_loss = 0.6269950120356577, disc_loss = 0.06169976969048831
Trained batch 232 in epoch 0, gen_loss = 0.6262557111328764, disc_loss = 0.06146188119882038
Trained batch 233 in epoch 0, gen_loss = 0.6259197043812174, disc_loss = 0.061223824926389336
Trained batch 234 in epoch 0, gen_loss = 0.6259530773822297, disc_loss = 0.06098746697002269
Trained batch 235 in epoch 0, gen_loss = 0.62534321869834, disc_loss = 0.060753284673348576
Trained batch 236 in epoch 0, gen_loss = 0.625356525811465, disc_loss = 0.060531981701998014
Trained batch 237 in epoch 0, gen_loss = 0.6248031520292538, disc_loss = 0.060306461228589923
Trained batch 238 in epoch 0, gen_loss = 0.62447671274261, disc_loss = 0.060079064991745985
Trained batch 239 in epoch 0, gen_loss = 0.6237385675311089, disc_loss = 0.05985217017199223
Trained batch 240 in epoch 0, gen_loss = 0.6233983457830437, disc_loss = 0.059635476846850756
Trained batch 241 in epoch 0, gen_loss = 0.6229847674527444, disc_loss = 0.059429595054489026
Trained batch 242 in epoch 0, gen_loss = 0.6225000413847558, disc_loss = 0.05922244028514059
Trained batch 243 in epoch 0, gen_loss = 0.6222532557659461, disc_loss = 0.05902177913755667
Trained batch 244 in epoch 0, gen_loss = 0.6216686082129576, disc_loss = 0.058808145128494624
Trained batch 245 in epoch 0, gen_loss = 0.6216391849566282, disc_loss = 0.05861134390248274
Trained batch 246 in epoch 0, gen_loss = 0.6216815276908488, disc_loss = 0.05841741740311447
Trained batch 247 in epoch 0, gen_loss = 0.6211782497984748, disc_loss = 0.05821410829066149
Trained batch 248 in epoch 0, gen_loss = 0.6213882873096619, disc_loss = 0.05803456256936592
Trained batch 249 in epoch 0, gen_loss = 0.62185695540905, disc_loss = 0.05786262809485197
Trained batch 250 in epoch 0, gen_loss = 0.6222623134276781, disc_loss = 0.05766446048772311
Trained batch 251 in epoch 0, gen_loss = 0.6218381343143327, disc_loss = 0.05746619574843891
Trained batch 252 in epoch 0, gen_loss = 0.6216159845764929, disc_loss = 0.057267591162131946
Trained batch 253 in epoch 0, gen_loss = 0.6210178965189326, disc_loss = 0.05707346721310315
Trained batch 254 in epoch 0, gen_loss = 0.6201418752763785, disc_loss = 0.056882223348115005
Trained batch 255 in epoch 0, gen_loss = 0.6195798839908093, disc_loss = 0.05668643579338095
Trained batch 256 in epoch 0, gen_loss = 0.6190021972711912, disc_loss = 0.05649934147017127
Trained batch 257 in epoch 0, gen_loss = 0.6186395213585492, disc_loss = 0.05631136248075916
Trained batch 258 in epoch 0, gen_loss = 0.6182030466992883, disc_loss = 0.056129873453362567
Trained batch 259 in epoch 0, gen_loss = 0.6177764745858999, disc_loss = 0.055939916707575324
Trained batch 260 in epoch 0, gen_loss = 0.6171665994371948, disc_loss = 0.055751052363936245
Trained batch 261 in epoch 0, gen_loss = 0.6168379598218976, disc_loss = 0.055559048135871536
Trained batch 262 in epoch 0, gen_loss = 0.616345750288365, disc_loss = 0.055368103060389764
Trained batch 263 in epoch 0, gen_loss = 0.6161220649426634, disc_loss = 0.05517636624757539
Trained batch 264 in epoch 0, gen_loss = 0.6163251008627549, disc_loss = 0.05500322995742537
Trained batch 265 in epoch 0, gen_loss = 0.6163505999217356, disc_loss = 0.05482857878480975
Trained batch 266 in epoch 0, gen_loss = 0.6163981338118792, disc_loss = 0.054645109874604456
Trained batch 267 in epoch 0, gen_loss = 0.6159343742834988, disc_loss = 0.054455153820445454
Trained batch 268 in epoch 0, gen_loss = 0.6157926350942775, disc_loss = 0.05426816531927862
Trained batch 269 in epoch 0, gen_loss = 0.6152756839990616, disc_loss = 0.05409785003867
Trained batch 270 in epoch 0, gen_loss = 0.6146376382600778, disc_loss = 0.05395115570103358
Trained batch 271 in epoch 0, gen_loss = 0.6148781582493993, disc_loss = 0.053785279856289406
Trained batch 272 in epoch 0, gen_loss = 0.614365659572266, disc_loss = 0.05362269711926334
Trained batch 273 in epoch 0, gen_loss = 0.6143820707815407, disc_loss = 0.05346331311420395
Trained batch 274 in epoch 0, gen_loss = 0.6136435441537337, disc_loss = 0.05341174995103343
Trained batch 275 in epoch 0, gen_loss = 0.6134569282117097, disc_loss = 0.05333957515835789
Trained batch 276 in epoch 0, gen_loss = 0.6127767072257583, disc_loss = 0.05322170087862558
Trained batch 277 in epoch 0, gen_loss = 0.6128056606800436, disc_loss = 0.053076833594597125
Trained batch 278 in epoch 0, gen_loss = 0.6125300674028294, disc_loss = 0.052928041647552215
Trained batch 279 in epoch 0, gen_loss = 0.6121059524161475, disc_loss = 0.05306657469648469
Trained batch 280 in epoch 0, gen_loss = 0.612547504732193, disc_loss = 0.05317753229333333
Trained batch 281 in epoch 0, gen_loss = 0.6124121538290741, disc_loss = 0.053121215334419714
Trained batch 282 in epoch 0, gen_loss = 0.6120911990374642, disc_loss = 0.05312503448542694
Trained batch 283 in epoch 0, gen_loss = 0.612215637111328, disc_loss = 0.052990572439329565
Trained batch 284 in epoch 0, gen_loss = 0.6121199971751163, disc_loss = 0.05285602584236154
Trained batch 285 in epoch 0, gen_loss = 0.6120041116967901, disc_loss = 0.052699036450919834
Trained batch 286 in epoch 0, gen_loss = 0.6117280391450542, disc_loss = 0.0525448372699515
Trained batch 287 in epoch 0, gen_loss = 0.6111273768668374, disc_loss = 0.0523833733887942
Trained batch 288 in epoch 0, gen_loss = 0.6108064625708702, disc_loss = 0.05223757443727223
Trained batch 289 in epoch 0, gen_loss = 0.610550079777323, disc_loss = 0.05207902119446803
Trained batch 290 in epoch 0, gen_loss = 0.6103164917210123, disc_loss = 0.051927648819951
Trained batch 291 in epoch 0, gen_loss = 0.6097593557548849, disc_loss = 0.05183362059075105
Trained batch 292 in epoch 0, gen_loss = 0.6096357378739953, disc_loss = 0.051752085408907855
Trained batch 293 in epoch 0, gen_loss = 0.6097274454069787, disc_loss = 0.051608227746922294
Trained batch 294 in epoch 0, gen_loss = 0.6096236551212052, disc_loss = 0.05146146821000187
Trained batch 295 in epoch 0, gen_loss = 0.6095068989371931, disc_loss = 0.05131702108947384
Trained batch 296 in epoch 0, gen_loss = 0.6093361155954675, disc_loss = 0.051178941104140896
Trained batch 297 in epoch 0, gen_loss = 0.6091651641482475, disc_loss = 0.05103646407775806
Trained batch 298 in epoch 0, gen_loss = 0.6087834002780277, disc_loss = 0.05088525579971289
Trained batch 299 in epoch 0, gen_loss = 0.6082410503427188, disc_loss = 0.05073894375547146
Trained batch 300 in epoch 0, gen_loss = 0.6081292879739869, disc_loss = 0.050603671827851686
Trained batch 301 in epoch 0, gen_loss = 0.6079094614216823, disc_loss = 0.05046536941127543
Trained batch 302 in epoch 0, gen_loss = 0.6078063510235386, disc_loss = 0.050324717286832805
Trained batch 303 in epoch 0, gen_loss = 0.6071214535714764, disc_loss = 0.050243808081416465
Trained batch 304 in epoch 0, gen_loss = 0.6068939143516978, disc_loss = 0.050148319296508294
Trained batch 305 in epoch 0, gen_loss = 0.6065716321756637, disc_loss = 0.050015516922310044
Trained batch 306 in epoch 0, gen_loss = 0.6069868928252291, disc_loss = 0.049878448034797455
Trained batch 307 in epoch 0, gen_loss = 0.6070974547561113, disc_loss = 0.04973983231330194
Trained batch 308 in epoch 0, gen_loss = 0.6070596165834508, disc_loss = 0.04960843192603331
Trained batch 309 in epoch 0, gen_loss = 0.606748424903039, disc_loss = 0.04946692007548747
Trained batch 310 in epoch 0, gen_loss = 0.6066135973984023, disc_loss = 0.049325444411420505
Trained batch 311 in epoch 0, gen_loss = 0.6062428014209638, disc_loss = 0.049182535390196465
Trained batch 312 in epoch 0, gen_loss = 0.6063209401723295, disc_loss = 0.04904400671463305
Trained batch 313 in epoch 0, gen_loss = 0.606749129238402, disc_loss = 0.048914258361639824
Trained batch 314 in epoch 0, gen_loss = 0.6066157928534917, disc_loss = 0.04877278068960304
Trained batch 315 in epoch 0, gen_loss = 0.6065516410560547, disc_loss = 0.0486330816270253
Trained batch 316 in epoch 0, gen_loss = 0.606209827804415, disc_loss = 0.04849202732103128
Trained batch 317 in epoch 0, gen_loss = 0.6057701056483407, disc_loss = 0.04835442774680168
Trained batch 318 in epoch 0, gen_loss = 0.6059425725458558, disc_loss = 0.048232535706821324
Trained batch 319 in epoch 0, gen_loss = 0.6057110292837024, disc_loss = 0.04811092434465536
Trained batch 320 in epoch 0, gen_loss = 0.6057040529830433, disc_loss = 0.047978753761484104
Trained batch 321 in epoch 0, gen_loss = 0.6050788609322554, disc_loss = 0.04784056833606815
Trained batch 322 in epoch 0, gen_loss = 0.6048386948581082, disc_loss = 0.04770799666346157
Trained batch 323 in epoch 0, gen_loss = 0.604946703546577, disc_loss = 0.047581264084452235
Trained batch 324 in epoch 0, gen_loss = 0.6051050990361434, disc_loss = 0.047458927636392985
Trained batch 325 in epoch 0, gen_loss = 0.6051882961593522, disc_loss = 0.04733611015171567
Trained batch 326 in epoch 0, gen_loss = 0.6049244065714903, disc_loss = 0.047207835027150556
Trained batch 327 in epoch 0, gen_loss = 0.6047581266157511, disc_loss = 0.0470932119479119
Trained batch 328 in epoch 0, gen_loss = 0.6042112938176535, disc_loss = 0.046963044329713405
Trained batch 329 in epoch 0, gen_loss = 0.6045686956607934, disc_loss = 0.04684589819375877
Trained batch 330 in epoch 0, gen_loss = 0.6046699836535036, disc_loss = 0.04672147396014964
Trained batch 331 in epoch 0, gen_loss = 0.6045791637825678, disc_loss = 0.04659381610725681
Trained batch 332 in epoch 0, gen_loss = 0.6045312018723817, disc_loss = 0.046466207166003796
Trained batch 333 in epoch 0, gen_loss = 0.6049988997910551, disc_loss = 0.04634733178895413
Trained batch 334 in epoch 0, gen_loss = 0.6048685022254489, disc_loss = 0.04622339604547553
Trained batch 335 in epoch 0, gen_loss = 0.6045946118732294, disc_loss = 0.04609680756194783
Trained batch 336 in epoch 0, gen_loss = 0.6042022013522751, disc_loss = 0.04597298985137121
Trained batch 337 in epoch 0, gen_loss = 0.6040176045612471, disc_loss = 0.04585223186061271
Trained batch 338 in epoch 0, gen_loss = 0.6037229327677274, disc_loss = 0.04573264406378598
Trained batch 339 in epoch 0, gen_loss = 0.6035809581770616, disc_loss = 0.04561295994584832
Trained batch 340 in epoch 0, gen_loss = 0.603323972120313, disc_loss = 0.045491051877906155
Trained batch 341 in epoch 0, gen_loss = 0.6031285174060286, disc_loss = 0.045376491709748466
Trained batch 342 in epoch 0, gen_loss = 0.6029347643560293, disc_loss = 0.04527677493930167
Trained batch 343 in epoch 0, gen_loss = 0.6028275520995606, disc_loss = 0.045168055409598055
Trained batch 344 in epoch 0, gen_loss = 0.6029044382814048, disc_loss = 0.04505929340999843
Trained batch 345 in epoch 0, gen_loss = 0.6029879466302133, disc_loss = 0.044946630804320375
Trained batch 346 in epoch 0, gen_loss = 0.6025514964239742, disc_loss = 0.04483366556739386
Trained batch 347 in epoch 0, gen_loss = 0.6024787831066669, disc_loss = 0.04471727504245081
Trained batch 348 in epoch 0, gen_loss = 0.6022388609739976, disc_loss = 0.04460043160186115
Trained batch 349 in epoch 0, gen_loss = 0.6018763261181967, disc_loss = 0.04448906132685287
Trained batch 350 in epoch 0, gen_loss = 0.6015551662512993, disc_loss = 0.044373710556194566
Trained batch 351 in epoch 0, gen_loss = 0.6012280304831538, disc_loss = 0.044258937222449196
Trained batch 352 in epoch 0, gen_loss = 0.6009628315991788, disc_loss = 0.044154555902212185
Trained batch 353 in epoch 0, gen_loss = 0.6008306491678044, disc_loss = 0.04405394465252126
Trained batch 354 in epoch 0, gen_loss = 0.6005033028797364, disc_loss = 0.04395113994800289
Trained batch 355 in epoch 0, gen_loss = 0.6004949000276877, disc_loss = 0.04384032023149762
Trained batch 356 in epoch 0, gen_loss = 0.600016690185424, disc_loss = 0.043734940809800345
Trained batch 357 in epoch 0, gen_loss = 0.5999496588327365, disc_loss = 0.043633613034344
Trained batch 358 in epoch 0, gen_loss = 0.5995229476839719, disc_loss = 0.043522815256376954
Trained batch 359 in epoch 0, gen_loss = 0.5995293710794714, disc_loss = 0.043414759015043575
Trained batch 360 in epoch 0, gen_loss = 0.59973157781313, disc_loss = 0.043307512856689684
Trained batch 361 in epoch 0, gen_loss = 0.599790132095142, disc_loss = 0.04319816428038564
Trained batch 362 in epoch 0, gen_loss = 0.5995599897604015, disc_loss = 0.04309053443944384
Trained batch 363 in epoch 0, gen_loss = 0.5996622517704964, disc_loss = 0.042984845012472434
Trained batch 364 in epoch 0, gen_loss = 0.5995739746583651, disc_loss = 0.04287726218454948
Trained batch 365 in epoch 0, gen_loss = 0.5995943934702482, disc_loss = 0.042776211815672455
Trained batch 366 in epoch 0, gen_loss = 0.5995379932407462, disc_loss = 0.042682481687947374
Trained batch 367 in epoch 0, gen_loss = 0.5994335292312114, disc_loss = 0.04257714416282555
Trained batch 368 in epoch 0, gen_loss = 0.5991468823699124, disc_loss = 0.04247350528202749
Trained batch 369 in epoch 0, gen_loss = 0.5989343847777392, disc_loss = 0.042370397215583236
Trained batch 370 in epoch 0, gen_loss = 0.5986966259395979, disc_loss = 0.04226512162672542
Trained batch 371 in epoch 0, gen_loss = 0.5985189238863606, disc_loss = 0.0421611258493466
Trained batch 372 in epoch 0, gen_loss = 0.5983368273394997, disc_loss = 0.04205692052184517
Trained batch 373 in epoch 0, gen_loss = 0.5984835635850774, disc_loss = 0.04195723425843479
Trained batch 374 in epoch 0, gen_loss = 0.5979947598775228, disc_loss = 0.04185556688470145
Trained batch 375 in epoch 0, gen_loss = 0.5979915155058212, disc_loss = 0.04175632080481824
Trained batch 376 in epoch 0, gen_loss = 0.5980341069261971, disc_loss = 0.04165659011069893
Trained batch 377 in epoch 0, gen_loss = 0.5978942152368959, disc_loss = 0.04156335420775961
Trained batch 378 in epoch 0, gen_loss = 0.5978223057095168, disc_loss = 0.041473301619630414
Trained batch 379 in epoch 0, gen_loss = 0.5973166115974126, disc_loss = 0.04137335532210081
Trained batch 380 in epoch 0, gen_loss = 0.5972503375193579, disc_loss = 0.041279611183242415
Trained batch 381 in epoch 0, gen_loss = 0.5970727704582414, disc_loss = 0.04118631923320113
Trained batch 382 in epoch 0, gen_loss = 0.5966751816689813, disc_loss = 0.04109019100940823
Trained batch 383 in epoch 0, gen_loss = 0.5968252712239822, disc_loss = 0.04099376386087291
Trained batch 384 in epoch 0, gen_loss = 0.5968949952682892, disc_loss = 0.04089548616041127
Trained batch 385 in epoch 0, gen_loss = 0.5971194343554541, disc_loss = 0.040803460438658574
Trained batch 386 in epoch 0, gen_loss = 0.5968435768625225, disc_loss = 0.04070908223033356
Trained batch 387 in epoch 0, gen_loss = 0.5965778333778234, disc_loss = 0.04061435801445113
Trained batch 388 in epoch 0, gen_loss = 0.5964236897643856, disc_loss = 0.040519378795635656
Trained batch 389 in epoch 0, gen_loss = 0.596342764985867, disc_loss = 0.04042286206490527
Trained batch 390 in epoch 0, gen_loss = 0.596220527082453, disc_loss = 0.040328769044250326
Trained batch 391 in epoch 0, gen_loss = 0.5958749813844963, disc_loss = 0.04023245366275957
Trained batch 392 in epoch 0, gen_loss = 0.5959665724491042, disc_loss = 0.04013971622521413
Trained batch 393 in epoch 0, gen_loss = 0.595850071855608, disc_loss = 0.04004631041334044
Trained batch 394 in epoch 0, gen_loss = 0.59556189700018, disc_loss = 0.03995252320059587
Trained batch 395 in epoch 0, gen_loss = 0.5958503135527023, disc_loss = 0.039869785156557244
Trained batch 396 in epoch 0, gen_loss = 0.5956466369124443, disc_loss = 0.03977738992928032
Trained batch 397 in epoch 0, gen_loss = 0.5954997394252662, disc_loss = 0.03968582022525651
Trained batch 398 in epoch 0, gen_loss = 0.5953964054733888, disc_loss = 0.03959602194156656
Trained batch 399 in epoch 0, gen_loss = 0.5952538289129734, disc_loss = 0.03950521349848714
Trained batch 400 in epoch 0, gen_loss = 0.5950199365615845, disc_loss = 0.039414110996451535
Trained batch 401 in epoch 0, gen_loss = 0.5951313240018057, disc_loss = 0.039329006954036023
Trained batch 402 in epoch 0, gen_loss = 0.5947337664534377, disc_loss = 0.03924493739270506
Trained batch 403 in epoch 0, gen_loss = 0.5948607839717723, disc_loss = 0.03916309343531064
Trained batch 404 in epoch 0, gen_loss = 0.5949366483423445, disc_loss = 0.03907767378115728
Trained batch 405 in epoch 0, gen_loss = 0.5947537901513095, disc_loss = 0.03898962721769959
Trained batch 406 in epoch 0, gen_loss = 0.5946029627498889, disc_loss = 0.03890186714818495
Trained batch 407 in epoch 0, gen_loss = 0.5946461024383703, disc_loss = 0.03881609105555268
Trained batch 408 in epoch 0, gen_loss = 0.5941689254164987, disc_loss = 0.038733926428650436
Trained batch 409 in epoch 0, gen_loss = 0.5941562318947257, disc_loss = 0.03865664982941092
Trained batch 410 in epoch 0, gen_loss = 0.5939321246025336, disc_loss = 0.03857374609472942
Trained batch 411 in epoch 0, gen_loss = 0.5935185974084057, disc_loss = 0.03848601015085734
Trained batch 412 in epoch 0, gen_loss = 0.593124218555686, disc_loss = 0.038399473293831
Trained batch 413 in epoch 0, gen_loss = 0.5927976109987296, disc_loss = 0.038312246632123376
Trained batch 414 in epoch 0, gen_loss = 0.5925485259079072, disc_loss = 0.03822714582770075
Trained batch 415 in epoch 0, gen_loss = 0.5923800720618322, disc_loss = 0.038142782646620896
Trained batch 416 in epoch 0, gen_loss = 0.5919161192375979, disc_loss = 0.038056734856664776
Trained batch 417 in epoch 0, gen_loss = 0.592182008344591, disc_loss = 0.03797518821994654
Trained batch 418 in epoch 0, gen_loss = 0.5921772006445682, disc_loss = 0.03789278248331659
Trained batch 419 in epoch 0, gen_loss = 0.5918425881436893, disc_loss = 0.03780979621501285
Trained batch 420 in epoch 0, gen_loss = 0.5919202037350298, disc_loss = 0.03773052137701235
Trained batch 421 in epoch 0, gen_loss = 0.5921534718636653, disc_loss = 0.03765123605262488
Trained batch 422 in epoch 0, gen_loss = 0.5922006307341529, disc_loss = 0.0375689755972193
Trained batch 423 in epoch 0, gen_loss = 0.5918910935661703, disc_loss = 0.03748791565775941
Trained batch 424 in epoch 0, gen_loss = 0.5915723187081954, disc_loss = 0.03740904786350096
Trained batch 425 in epoch 0, gen_loss = 0.5916837310427231, disc_loss = 0.03732923052859593
Trained batch 426 in epoch 0, gen_loss = 0.5912647393185305, disc_loss = 0.037247917984025575
Trained batch 427 in epoch 0, gen_loss = 0.5911685858513708, disc_loss = 0.03717164800308697
Trained batch 428 in epoch 0, gen_loss = 0.5908972333361219, disc_loss = 0.037091363187757756
Trained batch 429 in epoch 0, gen_loss = 0.5904390797365543, disc_loss = 0.03701272034627753
Trained batch 430 in epoch 0, gen_loss = 0.590362299138598, disc_loss = 0.03693512556674672
Trained batch 431 in epoch 0, gen_loss = 0.5901667490187619, disc_loss = 0.036858363120161275
Trained batch 432 in epoch 0, gen_loss = 0.5896878877082543, disc_loss = 0.03677933050403436
Trained batch 433 in epoch 0, gen_loss = 0.5894646097987478, disc_loss = 0.03670303121129746
Trained batch 434 in epoch 0, gen_loss = 0.5890158247673648, disc_loss = 0.03662618057663156
Trained batch 435 in epoch 0, gen_loss = 0.5889902511321077, disc_loss = 0.03654690717032965
Trained batch 436 in epoch 0, gen_loss = 0.5889710331126808, disc_loss = 0.03647162535097834
Trained batch 437 in epoch 0, gen_loss = 0.5887447211840381, disc_loss = 0.036394905999566525
Trained batch 438 in epoch 0, gen_loss = 0.5886042863470003, disc_loss = 0.03631777061491034
Trained batch 439 in epoch 0, gen_loss = 0.5882041903381998, disc_loss = 0.03624258633882908
Trained batch 440 in epoch 0, gen_loss = 0.58788347419968, disc_loss = 0.036172348892426036
Trained batch 441 in epoch 0, gen_loss = 0.5878805495225466, disc_loss = 0.03610143755463571
Trained batch 442 in epoch 0, gen_loss = 0.5876336133372434, disc_loss = 0.03602544832228353
Trained batch 443 in epoch 0, gen_loss = 0.5874569442223858, disc_loss = 0.035951703485323803
Trained batch 444 in epoch 0, gen_loss = 0.5869925477531519, disc_loss = 0.03588573115124378
Trained batch 445 in epoch 0, gen_loss = 0.586637923402102, disc_loss = 0.03581889078695528
Trained batch 446 in epoch 0, gen_loss = 0.5861991879924032, disc_loss = 0.035745827068097605
Trained batch 447 in epoch 0, gen_loss = 0.5861290364659258, disc_loss = 0.03567340178363208
Trained batch 448 in epoch 0, gen_loss = 0.5857289277498334, disc_loss = 0.035598464628561234
Trained batch 449 in epoch 0, gen_loss = 0.5858103967375226, disc_loss = 0.03552544571614514
Trained batch 450 in epoch 0, gen_loss = 0.5856206203643605, disc_loss = 0.03545152634937737
Trained batch 451 in epoch 0, gen_loss = 0.5857610403832081, disc_loss = 0.03538122929352443
Trained batch 452 in epoch 0, gen_loss = 0.5855332120233288, disc_loss = 0.03530909812487004
Trained batch 453 in epoch 0, gen_loss = 0.5853504035047498, disc_loss = 0.035235833908976895
Trained batch 454 in epoch 0, gen_loss = 0.585303048314629, disc_loss = 0.03516418244996732
Trained batch 455 in epoch 0, gen_loss = 0.5852143118125305, disc_loss = 0.03509237051874056
Trained batch 456 in epoch 0, gen_loss = 0.5851208235685361, disc_loss = 0.035021042855277136
Trained batch 457 in epoch 0, gen_loss = 0.584994394901538, disc_loss = 0.03495121621130254
Trained batch 458 in epoch 0, gen_loss = 0.5850091091558045, disc_loss = 0.03488349562413455
Trained batch 459 in epoch 0, gen_loss = 0.5846334687393645, disc_loss = 0.03481336975338585
Trained batch 460 in epoch 0, gen_loss = 0.5846933522736432, disc_loss = 0.03474379743756464
Trained batch 461 in epoch 0, gen_loss = 0.5845668017219156, disc_loss = 0.03467579981265791
Trained batch 462 in epoch 0, gen_loss = 0.5843574755521367, disc_loss = 0.03460647029144444
Trained batch 463 in epoch 0, gen_loss = 0.584115758923621, disc_loss = 0.034537557104566324
Trained batch 464 in epoch 0, gen_loss = 0.5837034745882916, disc_loss = 0.03446819839588497
Trained batch 465 in epoch 0, gen_loss = 0.5831720015521725, disc_loss = 0.034402809384111076
Trained batch 466 in epoch 0, gen_loss = 0.5828071131951293, disc_loss = 0.034334991951383315
Trained batch 467 in epoch 0, gen_loss = 0.582524708703033, disc_loss = 0.0342688203145527
Trained batch 468 in epoch 0, gen_loss = 0.5822124035754946, disc_loss = 0.034202330563066485
Trained batch 469 in epoch 0, gen_loss = 0.5821191561348895, disc_loss = 0.034138686047431005
Trained batch 470 in epoch 0, gen_loss = 0.5818452768011964, disc_loss = 0.034071799863976154
Trained batch 471 in epoch 0, gen_loss = 0.5816344919346147, disc_loss = 0.03400766003484269
Trained batch 472 in epoch 0, gen_loss = 0.581407543531684, disc_loss = 0.033941915776111956
Trained batch 473 in epoch 0, gen_loss = 0.5812139677724758, disc_loss = 0.03387593780398636
Trained batch 474 in epoch 0, gen_loss = 0.5814098845657549, disc_loss = 0.033816234493432074
Trained batch 475 in epoch 0, gen_loss = 0.5812567364142722, disc_loss = 0.0337516463419399
Trained batch 476 in epoch 0, gen_loss = 0.5811376349856019, disc_loss = 0.03368912628008369
Trained batch 477 in epoch 0, gen_loss = 0.5808953024976922, disc_loss = 0.03362507042601689
Trained batch 478 in epoch 0, gen_loss = 0.5806570680878108, disc_loss = 0.03355820392796066
Trained batch 479 in epoch 0, gen_loss = 0.5805024593447645, disc_loss = 0.03349536442425839
Trained batch 480 in epoch 0, gen_loss = 0.5801927250289124, disc_loss = 0.03343328618247976
Trained batch 481 in epoch 0, gen_loss = 0.5804159873748715, disc_loss = 0.03338022893695625
Trained batch 482 in epoch 0, gen_loss = 0.5804544333098591, disc_loss = 0.033319188814801136
Trained batch 483 in epoch 0, gen_loss = 0.5804185341458675, disc_loss = 0.03326199296356779
Trained batch 484 in epoch 0, gen_loss = 0.5805048088437503, disc_loss = 0.033203346408740377
Trained batch 485 in epoch 0, gen_loss = 0.5802656233678629, disc_loss = 0.03314126697019195
Trained batch 486 in epoch 0, gen_loss = 0.5800342318091304, disc_loss = 0.033079552682435995
Trained batch 487 in epoch 0, gen_loss = 0.5797508669803377, disc_loss = 0.03302076085471571
Trained batch 488 in epoch 0, gen_loss = 0.5800039137433644, disc_loss = 0.032967247631913145
Trained batch 489 in epoch 0, gen_loss = 0.5797767623954889, disc_loss = 0.03290775647097058
Trained batch 490 in epoch 0, gen_loss = 0.5795012966799882, disc_loss = 0.03284768919415901
Trained batch 491 in epoch 0, gen_loss = 0.5791689770493081, disc_loss = 0.03278806237943993
Trained batch 492 in epoch 0, gen_loss = 0.5790563988637247, disc_loss = 0.03272759187215671
Trained batch 493 in epoch 0, gen_loss = 0.5789782187716681, disc_loss = 0.03266865029428644
Trained batch 494 in epoch 0, gen_loss = 0.5789146246332111, disc_loss = 0.03261109898501838
Trained batch 495 in epoch 0, gen_loss = 0.5787127288358827, disc_loss = 0.03255455768474701
Trained batch 496 in epoch 0, gen_loss = 0.5784414257681826, disc_loss = 0.032504124348370404
Trained batch 497 in epoch 0, gen_loss = 0.5782515220493677, disc_loss = 0.032448328902195005
Trained batch 498 in epoch 0, gen_loss = 0.5781488360646732, disc_loss = 0.03238954383600876
Trained batch 499 in epoch 0, gen_loss = 0.5783500055670738, disc_loss = 0.03233248110697605
Trained batch 500 in epoch 0, gen_loss = 0.5781000533741629, disc_loss = 0.032273732976450725
Trained batch 501 in epoch 0, gen_loss = 0.5779512052042076, disc_loss = 0.0322169935336778
Trained batch 502 in epoch 0, gen_loss = 0.5778069145399818, disc_loss = 0.032157965003749796
Trained batch 503 in epoch 0, gen_loss = 0.5778684687046778, disc_loss = 0.03210060115158003
Trained batch 504 in epoch 0, gen_loss = 0.5776391672025812, disc_loss = 0.032042539958141304
Trained batch 505 in epoch 0, gen_loss = 0.5775175986907227, disc_loss = 0.031986325055042376
Trained batch 506 in epoch 0, gen_loss = 0.5773968030597566, disc_loss = 0.031933221289809036
Trained batch 507 in epoch 0, gen_loss = 0.5771484596170778, disc_loss = 0.03188014705275831
Trained batch 508 in epoch 0, gen_loss = 0.5769744150647949, disc_loss = 0.031822582062064855
Trained batch 509 in epoch 0, gen_loss = 0.576675699914203, disc_loss = 0.03176580720841337
Trained batch 510 in epoch 0, gen_loss = 0.5763685399422207, disc_loss = 0.03170790134290317
Trained batch 511 in epoch 0, gen_loss = 0.5761986316065304, disc_loss = 0.031649951030658485
Trained batch 512 in epoch 0, gen_loss = 0.576091122499451, disc_loss = 0.03159236138959762
Trained batch 513 in epoch 0, gen_loss = 0.5758828826335618, disc_loss = 0.031534764809481405
Trained batch 514 in epoch 0, gen_loss = 0.5758054474603782, disc_loss = 0.0314792316874887
Trained batch 515 in epoch 0, gen_loss = 0.5754645518207735, disc_loss = 0.0314220924373958
Trained batch 516 in epoch 0, gen_loss = 0.5755138109569623, disc_loss = 0.031366974520461
Trained batch 517 in epoch 0, gen_loss = 0.575296555419226, disc_loss = 0.03131015834728486
Trained batch 518 in epoch 0, gen_loss = 0.5752199045840026, disc_loss = 0.031253865536666606
Trained batch 519 in epoch 0, gen_loss = 0.5749968502957087, disc_loss = 0.03119780271596168
Trained batch 520 in epoch 0, gen_loss = 0.5748510516307633, disc_loss = 0.031141226332734792
Trained batch 521 in epoch 0, gen_loss = 0.5748169427858916, disc_loss = 0.03108529882015789
Trained batch 522 in epoch 0, gen_loss = 0.5745377140564855, disc_loss = 0.031029562976026114
Trained batch 523 in epoch 0, gen_loss = 0.5743234017650589, disc_loss = 0.030973786180252424
Trained batch 524 in epoch 0, gen_loss = 0.5743478586560203, disc_loss = 0.030917663179071887
Trained batch 525 in epoch 0, gen_loss = 0.574080095771601, disc_loss = 0.030862011592180074
Trained batch 526 in epoch 0, gen_loss = 0.5739422340999292, disc_loss = 0.030806380498136412
Trained batch 527 in epoch 0, gen_loss = 0.5738685424580718, disc_loss = 0.030750806610078806
Trained batch 528 in epoch 0, gen_loss = 0.5740125356189245, disc_loss = 0.03069920683436321
Trained batch 529 in epoch 0, gen_loss = 0.5738029031821017, disc_loss = 0.030646592628579796
Trained batch 530 in epoch 0, gen_loss = 0.5737215205886719, disc_loss = 0.0305950343069531
Trained batch 531 in epoch 0, gen_loss = 0.5736885221492976, disc_loss = 0.030543482079454242
Trained batch 532 in epoch 0, gen_loss = 0.5734537302553094, disc_loss = 0.030489826487555514
Trained batch 533 in epoch 0, gen_loss = 0.5732247378495748, disc_loss = 0.030436455375473067
Trained batch 534 in epoch 0, gen_loss = 0.5731018255804187, disc_loss = 0.030383353502811673
Trained batch 535 in epoch 0, gen_loss = 0.5726965849301708, disc_loss = 0.030332587543812427
Trained batch 536 in epoch 0, gen_loss = 0.5724676981762579, disc_loss = 0.030281201663584875
Trained batch 537 in epoch 0, gen_loss = 0.5721999796238973, disc_loss = 0.030227980691572603
Trained batch 538 in epoch 0, gen_loss = 0.5720478850857447, disc_loss = 0.03017679181343881
Trained batch 539 in epoch 0, gen_loss = 0.5720619265128065, disc_loss = 0.03012622222131877
Trained batch 540 in epoch 0, gen_loss = 0.5719651399720839, disc_loss = 0.030074410387547258
Trained batch 541 in epoch 0, gen_loss = 0.5719050065067861, disc_loss = 0.0300241995523804
Trained batch 542 in epoch 0, gen_loss = 0.5715470728404395, disc_loss = 0.02997248024490197
Trained batch 543 in epoch 0, gen_loss = 0.5713958755573806, disc_loss = 0.029920207409946564
Trained batch 544 in epoch 0, gen_loss = 0.5713262229884436, disc_loss = 0.02986914825074614
Trained batch 545 in epoch 0, gen_loss = 0.5711147988235558, disc_loss = 0.029818762559656927
Trained batch 546 in epoch 0, gen_loss = 0.5709999660251346, disc_loss = 0.029767728377673334
Trained batch 547 in epoch 0, gen_loss = 0.5708364786675376, disc_loss = 0.029716355286803723
Trained batch 548 in epoch 0, gen_loss = 0.5709136491047663, disc_loss = 0.02966907136023506
Trained batch 549 in epoch 0, gen_loss = 0.5709209666468881, disc_loss = 0.029620050971650266
Trained batch 550 in epoch 0, gen_loss = 0.5710567428498865, disc_loss = 0.029570523511276946
Trained batch 551 in epoch 0, gen_loss = 0.5709093926825385, disc_loss = 0.02952058342044188
Trained batch 552 in epoch 0, gen_loss = 0.570848951719122, disc_loss = 0.02947155468942257
Trained batch 553 in epoch 0, gen_loss = 0.5708325235206728, disc_loss = 0.029421993196329998
Trained batch 554 in epoch 0, gen_loss = 0.5707951518866393, disc_loss = 0.02937193112196149
Trained batch 555 in epoch 0, gen_loss = 0.5705721717086627, disc_loss = 0.029321657906189267
Trained batch 556 in epoch 0, gen_loss = 0.5706579864988207, disc_loss = 0.029273801387908
Trained batch 557 in epoch 0, gen_loss = 0.5708076489228074, disc_loss = 0.02922578377016623
Trained batch 558 in epoch 0, gen_loss = 0.570539649196089, disc_loss = 0.02917748846052038
Trained batch 559 in epoch 0, gen_loss = 0.5703821339777537, disc_loss = 0.029128971411098194
Trained batch 560 in epoch 0, gen_loss = 0.5701787929696408, disc_loss = 0.029079963487561346
Trained batch 561 in epoch 0, gen_loss = 0.570181075152129, disc_loss = 0.02903168843818822
Trained batch 562 in epoch 0, gen_loss = 0.5700189857139892, disc_loss = 0.02898329681159389
Trained batch 563 in epoch 0, gen_loss = 0.5695876488660244, disc_loss = 0.028937452174690122
Trained batch 564 in epoch 0, gen_loss = 0.5693266129071733, disc_loss = 0.028893658977859053
Trained batch 565 in epoch 0, gen_loss = 0.569074892323767, disc_loss = 0.028847505160781814
Trained batch 566 in epoch 0, gen_loss = 0.5691097964264938, disc_loss = 0.028799959273624515
Trained batch 567 in epoch 0, gen_loss = 0.5689492515275176, disc_loss = 0.02875237075224991
Trained batch 568 in epoch 0, gen_loss = 0.5687626178109583, disc_loss = 0.028704592462560335
Trained batch 569 in epoch 0, gen_loss = 0.5687616782230244, disc_loss = 0.02865894418781656
Trained batch 570 in epoch 0, gen_loss = 0.5684512017487227, disc_loss = 0.028611794128792195
Trained batch 571 in epoch 0, gen_loss = 0.5682783689011227, disc_loss = 0.028565982298866904
Trained batch 572 in epoch 0, gen_loss = 0.5682101732877328, disc_loss = 0.02851965409755564
Trained batch 573 in epoch 0, gen_loss = 0.5681373791100671, disc_loss = 0.028473378717655946
Trained batch 574 in epoch 0, gen_loss = 0.5680367276461228, disc_loss = 0.028426929723633372
Trained batch 575 in epoch 0, gen_loss = 0.5680785850207839, disc_loss = 0.028380208097789565
Trained batch 576 in epoch 0, gen_loss = 0.5678395025651625, disc_loss = 0.028333599058155814
Trained batch 577 in epoch 0, gen_loss = 0.5675225990556928, disc_loss = 0.02828783456157171
Trained batch 578 in epoch 0, gen_loss = 0.5674038152105861, disc_loss = 0.028242953361328916
Trained batch 579 in epoch 0, gen_loss = 0.5671792594009433, disc_loss = 0.028198518042035145
Trained batch 580 in epoch 0, gen_loss = 0.5669366071023136, disc_loss = 0.028153642113184277
Trained batch 581 in epoch 0, gen_loss = 0.5669613297247805, disc_loss = 0.0281080994346976
Trained batch 582 in epoch 0, gen_loss = 0.5667638228647092, disc_loss = 0.02806206742153019
Trained batch 583 in epoch 0, gen_loss = 0.5666732929953157, disc_loss = 0.028016451994604622
Trained batch 584 in epoch 0, gen_loss = 0.5665762981797895, disc_loss = 0.02797161236954614
Trained batch 585 in epoch 0, gen_loss = 0.5665594965931499, disc_loss = 0.027927335642942516
Trained batch 586 in epoch 0, gen_loss = 0.5664506661221765, disc_loss = 0.02788259681779098
Trained batch 587 in epoch 0, gen_loss = 0.5664007439702546, disc_loss = 0.027837970788229485
Trained batch 588 in epoch 0, gen_loss = 0.566288929641348, disc_loss = 0.027793280002566177
Trained batch 589 in epoch 0, gen_loss = 0.5660509016554235, disc_loss = 0.02775077938903281
Trained batch 590 in epoch 0, gen_loss = 0.5659398259043492, disc_loss = 0.027706497076421335
Trained batch 591 in epoch 0, gen_loss = 0.5659033581614494, disc_loss = 0.0276623900627365
Trained batch 592 in epoch 0, gen_loss = 0.5656829776390004, disc_loss = 0.027618110158306376
Trained batch 593 in epoch 0, gen_loss = 0.5657004709496642, disc_loss = 0.027574412940777147
Trained batch 594 in epoch 0, gen_loss = 0.5655756888770256, disc_loss = 0.027531140453980436
Trained batch 595 in epoch 0, gen_loss = 0.565513778602917, disc_loss = 0.027488493128334353
Trained batch 596 in epoch 0, gen_loss = 0.5652887001508844, disc_loss = 0.02744490530445604
Trained batch 597 in epoch 0, gen_loss = 0.5649833376690695, disc_loss = 0.027401144550116514
Trained batch 598 in epoch 0, gen_loss = 0.5648412150711767, disc_loss = 0.02735737647794386
Trained batch 599 in epoch 0, gen_loss = 0.5647172942260901, disc_loss = 0.027315136641263962
Trained batch 600 in epoch 0, gen_loss = 0.5646204916193164, disc_loss = 0.027272219437650232
Trained batch 601 in epoch 0, gen_loss = 0.5643589915429239, disc_loss = 0.027229112138054287
Trained batch 602 in epoch 0, gen_loss = 0.5643085519077371, disc_loss = 0.027186614010061384
Trained batch 603 in epoch 0, gen_loss = 0.5640362376803594, disc_loss = 0.027143793348076487
Trained batch 604 in epoch 0, gen_loss = 0.5638826854465422, disc_loss = 0.027101109225749355
Trained batch 605 in epoch 0, gen_loss = 0.5637643297906756, disc_loss = 0.0270588999839266
Trained batch 606 in epoch 0, gen_loss = 0.5636742925133305, disc_loss = 0.027016773165926287
Trained batch 607 in epoch 0, gen_loss = 0.5636123757025129, disc_loss = 0.0269747565166893
Trained batch 608 in epoch 0, gen_loss = 0.5635462614703061, disc_loss = 0.026932878942574467
Trained batch 609 in epoch 0, gen_loss = 0.5634286986999825, disc_loss = 0.026890741473018023
Trained batch 610 in epoch 0, gen_loss = 0.5634151472787811, disc_loss = 0.02684985439189437
Trained batch 611 in epoch 0, gen_loss = 0.56319218611016, disc_loss = 0.026809841147897875
Trained batch 612 in epoch 0, gen_loss = 0.5630987521874768, disc_loss = 0.026769404242583967
Trained batch 613 in epoch 0, gen_loss = 0.5631348566434282, disc_loss = 0.02672820092567185
Trained batch 614 in epoch 0, gen_loss = 0.5629317343719606, disc_loss = 0.02668671828270625
Trained batch 615 in epoch 0, gen_loss = 0.5627267266345488, disc_loss = 0.026645278092095773
Trained batch 616 in epoch 0, gen_loss = 0.562596405972538, disc_loss = 0.026604094491996823
Trained batch 617 in epoch 0, gen_loss = 0.5625666840466095, disc_loss = 0.026563939185584005
Trained batch 618 in epoch 0, gen_loss = 0.5622854175494445, disc_loss = 0.026523218293838288
Trained batch 619 in epoch 0, gen_loss = 0.5621090367917092, disc_loss = 0.026483900454871718
Trained batch 620 in epoch 0, gen_loss = 0.561969902060458, disc_loss = 0.02644446510754094
Trained batch 621 in epoch 0, gen_loss = 0.5616622613173973, disc_loss = 0.02640447581757363
Trained batch 622 in epoch 0, gen_loss = 0.5615057359729112, disc_loss = 0.026364784454739987
Trained batch 623 in epoch 0, gen_loss = 0.5614018731105787, disc_loss = 0.02632600445698093
Trained batch 624 in epoch 0, gen_loss = 0.5612704381465912, disc_loss = 0.026286845233850182
Trained batch 625 in epoch 0, gen_loss = 0.5611074172650663, disc_loss = 0.026247354210376703
Trained batch 626 in epoch 0, gen_loss = 0.5611226065307142, disc_loss = 0.02620870956242756
Trained batch 627 in epoch 0, gen_loss = 0.5608139203232565, disc_loss = 0.026170841734270582
Trained batch 628 in epoch 0, gen_loss = 0.5606967864620288, disc_loss = 0.02613400116089779
Trained batch 629 in epoch 0, gen_loss = 0.5605003465735723, disc_loss = 0.026095939303622653
Trained batch 630 in epoch 0, gen_loss = 0.5604604804761436, disc_loss = 0.026056815073832798
Trained batch 631 in epoch 0, gen_loss = 0.5603933792702759, disc_loss = 0.026018175195543024
Trained batch 632 in epoch 0, gen_loss = 0.5602257369442197, disc_loss = 0.025979887917019887
Trained batch 633 in epoch 0, gen_loss = 0.5601488057559221, disc_loss = 0.025941605602570278
Trained batch 634 in epoch 0, gen_loss = 0.5601115612533148, disc_loss = 0.025904261407361726
Trained batch 635 in epoch 0, gen_loss = 0.5599814065784778, disc_loss = 0.025867029527383827
Trained batch 636 in epoch 0, gen_loss = 0.5599525798246662, disc_loss = 0.025829933885283173
Trained batch 637 in epoch 0, gen_loss = 0.559852142085476, disc_loss = 0.025792467782308922
Trained batch 638 in epoch 0, gen_loss = 0.5597852377544547, disc_loss = 0.025754271704573856
Trained batch 639 in epoch 0, gen_loss = 0.5598389281425625, disc_loss = 0.025717961886221018
Trained batch 640 in epoch 0, gen_loss = 0.5598226818670162, disc_loss = 0.025681075468759654
Trained batch 641 in epoch 0, gen_loss = 0.5596133851652205, disc_loss = 0.025643563562803947
Trained batch 642 in epoch 0, gen_loss = 0.5595743490692987, disc_loss = 0.02560643749083443
Trained batch 643 in epoch 0, gen_loss = 0.55936050803765, disc_loss = 0.025568859267369584
Trained batch 644 in epoch 0, gen_loss = 0.5591449013976164, disc_loss = 0.025531325894660614
Trained batch 645 in epoch 0, gen_loss = 0.5592218973319227, disc_loss = 0.02549438494644719
Trained batch 646 in epoch 0, gen_loss = 0.5591740724292383, disc_loss = 0.0254572701057091
Trained batch 647 in epoch 0, gen_loss = 0.5590147860808137, disc_loss = 0.025420390878463404
Trained batch 648 in epoch 0, gen_loss = 0.5589242373666338, disc_loss = 0.025385635959677605
Trained batch 649 in epoch 0, gen_loss = 0.5588719731110793, disc_loss = 0.02535170434598023
Trained batch 650 in epoch 0, gen_loss = 0.5588356092595101, disc_loss = 0.02531893372792499
Trained batch 651 in epoch 0, gen_loss = 0.5588486011774262, disc_loss = 0.025284809064144338
Trained batch 652 in epoch 0, gen_loss = 0.5588165967059172, disc_loss = 0.025249054938715928
Trained batch 653 in epoch 0, gen_loss = 0.5586266916370538, disc_loss = 0.0252128288511896
Trained batch 654 in epoch 0, gen_loss = 0.5585839852576947, disc_loss = 0.025176275879827164
Trained batch 655 in epoch 0, gen_loss = 0.5586713230373656, disc_loss = 0.02514286435112675
Trained batch 656 in epoch 0, gen_loss = 0.5584420860267303, disc_loss = 0.025108341616684716
Trained batch 657 in epoch 0, gen_loss = 0.5583621170201924, disc_loss = 0.025075863388658567
Trained batch 658 in epoch 0, gen_loss = 0.558080699854447, disc_loss = 0.025046447243920227
Trained batch 659 in epoch 0, gen_loss = 0.5582168611161636, disc_loss = 0.02501366140081011
Trained batch 660 in epoch 0, gen_loss = 0.55801363368258, disc_loss = 0.024978439316559308
Trained batch 661 in epoch 0, gen_loss = 0.5579236067854025, disc_loss = 0.024943802840909232
Trained batch 662 in epoch 0, gen_loss = 0.5578777138284249, disc_loss = 0.024910792516302852
Trained batch 663 in epoch 0, gen_loss = 0.5577666168830481, disc_loss = 0.024877312745694182
Trained batch 664 in epoch 0, gen_loss = 0.557564318673055, disc_loss = 0.02484595693290738
Trained batch 665 in epoch 0, gen_loss = 0.5576667573985394, disc_loss = 0.024819081609679928
Trained batch 666 in epoch 0, gen_loss = 0.5574428590609395, disc_loss = 0.02479323483912785
Trained batch 667 in epoch 0, gen_loss = 0.5571419680993001, disc_loss = 0.02476692638146652
Trained batch 668 in epoch 0, gen_loss = 0.5571148552791242, disc_loss = 0.02473775880593634
Trained batch 669 in epoch 0, gen_loss = 0.5568961355668395, disc_loss = 0.024706960046566577
Trained batch 670 in epoch 0, gen_loss = 0.5568526044776649, disc_loss = 0.02467641929524011
Trained batch 671 in epoch 0, gen_loss = 0.5565894304198169, disc_loss = 0.024648255612105242
Trained batch 672 in epoch 0, gen_loss = 0.5565088068482426, disc_loss = 0.024625766049879556
Trained batch 673 in epoch 0, gen_loss = 0.5564390662781209, disc_loss = 0.024599731290776514
Trained batch 674 in epoch 0, gen_loss = 0.5563559892000975, disc_loss = 0.02456896269245556
Trained batch 675 in epoch 0, gen_loss = 0.5564227947557466, disc_loss = 0.024535999718444564
Trained batch 676 in epoch 0, gen_loss = 0.5564238433911847, disc_loss = 0.02450339655506052
Trained batch 677 in epoch 0, gen_loss = 0.5562433185478931, disc_loss = 0.024471009455780585
Trained batch 678 in epoch 0, gen_loss = 0.5560558888627785, disc_loss = 0.024439205120993842
Trained batch 679 in epoch 0, gen_loss = 0.5559338879935881, disc_loss = 0.024408151850665864
Trained batch 680 in epoch 0, gen_loss = 0.5557954104811434, disc_loss = 0.02437492604941819
Trained batch 681 in epoch 0, gen_loss = 0.5556103789911242, disc_loss = 0.024341201249925185
Trained batch 682 in epoch 0, gen_loss = 0.555398872970313, disc_loss = 0.024308783026060328
Trained batch 683 in epoch 0, gen_loss = 0.555187128614961, disc_loss = 0.024276598632725077
Trained batch 684 in epoch 0, gen_loss = 0.5548618301422926, disc_loss = 0.024246559934146757
Trained batch 685 in epoch 0, gen_loss = 0.5547680460763742, disc_loss = 0.024216189967948977
Trained batch 686 in epoch 0, gen_loss = 0.5546110494948892, disc_loss = 0.024185363930247433
Trained batch 687 in epoch 0, gen_loss = 0.5544883226326038, disc_loss = 0.02415402297087577
Trained batch 688 in epoch 0, gen_loss = 0.5543012107318303, disc_loss = 0.02412206652920277
Trained batch 689 in epoch 0, gen_loss = 0.5543029999819354, disc_loss = 0.024089618892564127
Trained batch 690 in epoch 0, gen_loss = 0.5542595210969017, disc_loss = 0.024056859688621753
Trained batch 691 in epoch 0, gen_loss = 0.5540862965445987, disc_loss = 0.024024090544768808
Trained batch 692 in epoch 0, gen_loss = 0.5540933998632225, disc_loss = 0.023991861809844698
Trained batch 693 in epoch 0, gen_loss = 0.553957676595501, disc_loss = 0.023959113764865137
Trained batch 694 in epoch 0, gen_loss = 0.5539476652797177, disc_loss = 0.023927515247508403
Trained batch 695 in epoch 0, gen_loss = 0.5539514099558195, disc_loss = 0.02389622629294306
Trained batch 696 in epoch 0, gen_loss = 0.5538382138549169, disc_loss = 0.023864807135297234
Trained batch 697 in epoch 0, gen_loss = 0.5537096878092064, disc_loss = 0.023833283283754406
Trained batch 698 in epoch 0, gen_loss = 0.5535201438421514, disc_loss = 0.023801096224556485
Trained batch 699 in epoch 0, gen_loss = 0.5536243696297918, disc_loss = 0.02377136266940007
Trained batch 700 in epoch 0, gen_loss = 0.5535077768538715, disc_loss = 0.023743557945835787
Trained batch 701 in epoch 0, gen_loss = 0.5534990932102557, disc_loss = 0.023715508384848986
Trained batch 702 in epoch 0, gen_loss = 0.5533769315540537, disc_loss = 0.023686113856583146
Trained batch 703 in epoch 0, gen_loss = 0.5531449621035294, disc_loss = 0.023659362840507212
Trained batch 704 in epoch 0, gen_loss = 0.5529091147666282, disc_loss = 0.02362979392517781
Trained batch 705 in epoch 0, gen_loss = 0.5527515749502452, disc_loss = 0.023599839301169098
Trained batch 706 in epoch 0, gen_loss = 0.552557304311844, disc_loss = 0.023569624243821828
Trained batch 707 in epoch 0, gen_loss = 0.5524709675103258, disc_loss = 0.023541408367843058
Trained batch 708 in epoch 0, gen_loss = 0.5522899500321602, disc_loss = 0.023512207042085655
Trained batch 709 in epoch 0, gen_loss = 0.5521665322109007, disc_loss = 0.023481576051235093
Trained batch 710 in epoch 0, gen_loss = 0.5519716609714739, disc_loss = 0.023450482774882167
Trained batch 711 in epoch 0, gen_loss = 0.5517557171288501, disc_loss = 0.02342427280022543
Trained batch 712 in epoch 0, gen_loss = 0.551552604115862, disc_loss = 0.023400093478426275
Trained batch 713 in epoch 0, gen_loss = 0.551563842248182, disc_loss = 0.023380982266942494
Trained batch 714 in epoch 0, gen_loss = 0.5516058778012549, disc_loss = 0.023357669588412854
Trained batch 715 in epoch 0, gen_loss = 0.5515775486160923, disc_loss = 0.023329831551833276
Trained batch 716 in epoch 0, gen_loss = 0.5516194631780706, disc_loss = 0.023300634548678537
Trained batch 717 in epoch 0, gen_loss = 0.551482792518265, disc_loss = 0.02327192013351698
Trained batch 718 in epoch 0, gen_loss = 0.5513240612100327, disc_loss = 0.02324299891323877
Trained batch 719 in epoch 0, gen_loss = 0.5513246215052074, disc_loss = 0.023214884370746505
Trained batch 720 in epoch 0, gen_loss = 0.5511179113354994, disc_loss = 0.023187140480779245
Trained batch 721 in epoch 0, gen_loss = 0.5508840827482889, disc_loss = 0.02316118485921184
Trained batch 722 in epoch 0, gen_loss = 0.5506759715443, disc_loss = 0.023135860310158955
Trained batch 723 in epoch 0, gen_loss = 0.5508340854012506, disc_loss = 0.0231111222725022
Trained batch 724 in epoch 0, gen_loss = 0.550734525754534, disc_loss = 0.02308263872942791
Trained batch 725 in epoch 0, gen_loss = 0.5506151353853136, disc_loss = 0.023053021941811216
Trained batch 726 in epoch 0, gen_loss = 0.5504666775915941, disc_loss = 0.023023807768744547
Trained batch 727 in epoch 0, gen_loss = 0.5503583588115462, disc_loss = 0.022994283751289073
Trained batch 728 in epoch 0, gen_loss = 0.5502124877214759, disc_loss = 0.022964339572853258
Trained batch 729 in epoch 0, gen_loss = 0.5501270956372561, disc_loss = 0.022934498972648577
Trained batch 730 in epoch 0, gen_loss = 0.550006554570309, disc_loss = 0.02290483625626724
Trained batch 731 in epoch 0, gen_loss = 0.5498655327682287, disc_loss = 0.022875244201057913
Trained batch 732 in epoch 0, gen_loss = 0.5497222879817671, disc_loss = 0.022845535874042765
Trained batch 733 in epoch 0, gen_loss = 0.5494449316074803, disc_loss = 0.02281763119104747
Trained batch 734 in epoch 0, gen_loss = 0.5491886277588046, disc_loss = 0.022788704917481056
Trained batch 735 in epoch 0, gen_loss = 0.5491873878661705, disc_loss = 0.022760600478922362
Trained batch 736 in epoch 0, gen_loss = 0.5491732395326104, disc_loss = 0.02273192773244547
Trained batch 737 in epoch 0, gen_loss = 0.5492190979522095, disc_loss = 0.022703256426968417
Trained batch 738 in epoch 0, gen_loss = 0.5491773116733772, disc_loss = 0.02267428970954309
Trained batch 739 in epoch 0, gen_loss = 0.5490571570154783, disc_loss = 0.022644957630640224
Trained batch 740 in epoch 0, gen_loss = 0.5488865801036439, disc_loss = 0.02261560384480109
Trained batch 741 in epoch 0, gen_loss = 0.5487742779470197, disc_loss = 0.022587154065152115
Trained batch 742 in epoch 0, gen_loss = 0.5487503378500367, disc_loss = 0.022560914933417008
Trained batch 743 in epoch 0, gen_loss = 0.5487045611665454, disc_loss = 0.022535947119092358
Trained batch 744 in epoch 0, gen_loss = 0.5486253294368718, disc_loss = 0.02250936030919645
Trained batch 745 in epoch 0, gen_loss = 0.5485691882809749, disc_loss = 0.022481231656064086
Trained batch 746 in epoch 0, gen_loss = 0.5485910558636727, disc_loss = 0.022452702652133123
Trained batch 747 in epoch 0, gen_loss = 0.5485659531731019, disc_loss = 0.022424445029212422
Trained batch 748 in epoch 0, gen_loss = 0.5485342733054677, disc_loss = 0.02239687620595008
Trained batch 749 in epoch 0, gen_loss = 0.5484612906773885, disc_loss = 0.02236971600074321
Trained batch 750 in epoch 0, gen_loss = 0.5484844182683688, disc_loss = 0.02234183925361473
Trained batch 751 in epoch 0, gen_loss = 0.5484690935687816, disc_loss = 0.022313973565766726
Trained batch 752 in epoch 0, gen_loss = 0.5481891822055042, disc_loss = 0.022286514793046483
Trained batch 753 in epoch 0, gen_loss = 0.5481378528578528, disc_loss = 0.02225944280268659
Trained batch 754 in epoch 0, gen_loss = 0.5477832064723337, disc_loss = 0.022238932957021607
Trained batch 755 in epoch 0, gen_loss = 0.5475750429526208, disc_loss = 0.022216066050407275
Trained batch 756 in epoch 0, gen_loss = 0.5477011151405118, disc_loss = 0.022191237916313653
Trained batch 757 in epoch 0, gen_loss = 0.5476243236841816, disc_loss = 0.02216498667035144
Trained batch 758 in epoch 0, gen_loss = 0.5476507713750731, disc_loss = 0.022138071919936192
Trained batch 759 in epoch 0, gen_loss = 0.5473929232280506, disc_loss = 0.022111848959742164
Trained batch 760 in epoch 0, gen_loss = 0.5472060896642261, disc_loss = 0.022086489027636505
Trained batch 761 in epoch 0, gen_loss = 0.5470940658035554, disc_loss = 0.02206149868886512
Trained batch 762 in epoch 0, gen_loss = 0.5468666480456236, disc_loss = 0.02203629584113309
Trained batch 763 in epoch 0, gen_loss = 0.5468519669006632, disc_loss = 0.02201026315658831
Trained batch 764 in epoch 0, gen_loss = 0.5466618699575562, disc_loss = 0.02198312984615126
Trained batch 765 in epoch 0, gen_loss = 0.546642293156594, disc_loss = 0.021956457296877556
Trained batch 766 in epoch 0, gen_loss = 0.5464924809752418, disc_loss = 0.02192955157999435
Trained batch 767 in epoch 0, gen_loss = 0.5463118340897685, disc_loss = 0.02190233683071104
Trained batch 768 in epoch 0, gen_loss = 0.5462953812201714, disc_loss = 0.021875556053349118
Trained batch 769 in epoch 0, gen_loss = 0.5461866077277567, disc_loss = 0.021848842287708267
Trained batch 770 in epoch 0, gen_loss = 0.5461506153590948, disc_loss = 0.02182182715224264
Trained batch 771 in epoch 0, gen_loss = 0.5461916425916815, disc_loss = 0.021796243996184482
Trained batch 772 in epoch 0, gen_loss = 0.54613018086009, disc_loss = 0.021771429988066453
Trained batch 773 in epoch 0, gen_loss = 0.5459833471272959, disc_loss = 0.021746297631182923
Trained batch 774 in epoch 0, gen_loss = 0.546069701602382, disc_loss = 0.021720451865976136
Trained batch 775 in epoch 0, gen_loss = 0.5459103997143888, disc_loss = 0.021693791419876134
Trained batch 776 in epoch 0, gen_loss = 0.5458894256535951, disc_loss = 0.02166752556942408
Trained batch 777 in epoch 0, gen_loss = 0.545675932594927, disc_loss = 0.021641037537169266
Trained batch 778 in epoch 0, gen_loss = 0.5457267546225267, disc_loss = 0.02161546950590162
Trained batch 779 in epoch 0, gen_loss = 0.54570343823005, disc_loss = 0.021589684955035455
Trained batch 780 in epoch 0, gen_loss = 0.5456513165664428, disc_loss = 0.021563595357153055
Trained batch 781 in epoch 0, gen_loss = 0.5455457065873743, disc_loss = 0.021537171785131363
Trained batch 782 in epoch 0, gen_loss = 0.5453903128513127, disc_loss = 0.021510883534623316
Trained batch 783 in epoch 0, gen_loss = 0.5453444172685243, disc_loss = 0.021485128530838088
Trained batch 784 in epoch 0, gen_loss = 0.5453973590188725, disc_loss = 0.021459382462350855
Trained batch 785 in epoch 0, gen_loss = 0.5453607709201541, disc_loss = 0.02143448225086858
Trained batch 786 in epoch 0, gen_loss = 0.545347341664107, disc_loss = 0.021409611542812127
Trained batch 787 in epoch 0, gen_loss = 0.5452546913762988, disc_loss = 0.021384839925691627
Trained batch 788 in epoch 0, gen_loss = 0.5453294177925632, disc_loss = 0.02136230353057663
Trained batch 789 in epoch 0, gen_loss = 0.5451499549271185, disc_loss = 0.021339339583882285
Trained batch 790 in epoch 0, gen_loss = 0.5449469048440381, disc_loss = 0.021317049650347045
Trained batch 791 in epoch 0, gen_loss = 0.5448829561772973, disc_loss = 0.021292944724061243
Trained batch 792 in epoch 0, gen_loss = 0.5449350467245335, disc_loss = 0.021269671787909488
Trained batch 793 in epoch 0, gen_loss = 0.5447335773210982, disc_loss = 0.021245155155058173
Trained batch 794 in epoch 0, gen_loss = 0.5446008112070695, disc_loss = 0.021221054522847792
Trained batch 795 in epoch 0, gen_loss = 0.5445892930255464, disc_loss = 0.021195915370422588
Trained batch 796 in epoch 0, gen_loss = 0.544582434138809, disc_loss = 0.021170729344010757
Trained batch 797 in epoch 0, gen_loss = 0.5444559524008504, disc_loss = 0.021145304772996445
Trained batch 798 in epoch 0, gen_loss = 0.5443573942843903, disc_loss = 0.021120104462375167
Trained batch 799 in epoch 0, gen_loss = 0.5442532481998206, disc_loss = 0.021094702225382206
Trained batch 800 in epoch 0, gen_loss = 0.5442368696989043, disc_loss = 0.021069818460754584
Trained batch 801 in epoch 0, gen_loss = 0.5441006742362072, disc_loss = 0.021045099744286408
Trained batch 802 in epoch 0, gen_loss = 0.5441442523917107, disc_loss = 0.021020952198836502
Trained batch 803 in epoch 0, gen_loss = 0.5441265092115497, disc_loss = 0.020999193493715852
Trained batch 804 in epoch 0, gen_loss = 0.543944323581198, disc_loss = 0.020976175879914984
Trained batch 805 in epoch 0, gen_loss = 0.5438202202098246, disc_loss = 0.020952378205981604
Trained batch 806 in epoch 0, gen_loss = 0.5437920777844969, disc_loss = 0.020928043990120142
Trained batch 807 in epoch 0, gen_loss = 0.54378237342923, disc_loss = 0.020903923250803962
Trained batch 808 in epoch 0, gen_loss = 0.5436916546193866, disc_loss = 0.020879540007333507
Trained batch 809 in epoch 0, gen_loss = 0.5435703526308507, disc_loss = 0.02085482267631264
Trained batch 810 in epoch 0, gen_loss = 0.5433428273542007, disc_loss = 0.020830887771595117
Trained batch 811 in epoch 0, gen_loss = 0.5432905454206937, disc_loss = 0.02080766366994848
Trained batch 812 in epoch 0, gen_loss = 0.5433577543255148, disc_loss = 0.02078459771372364
Trained batch 813 in epoch 0, gen_loss = 0.5433694934083437, disc_loss = 0.02076132353512497
Trained batch 814 in epoch 0, gen_loss = 0.5433435840109374, disc_loss = 0.020738647213531372
Trained batch 815 in epoch 0, gen_loss = 0.5432287041901374, disc_loss = 0.020714978302552766
Trained batch 816 in epoch 0, gen_loss = 0.54299624025311, disc_loss = 0.020690826359438634
Trained batch 817 in epoch 0, gen_loss = 0.54290889706326, disc_loss = 0.020667404751151044
Trained batch 818 in epoch 0, gen_loss = 0.5429074629249736, disc_loss = 0.02064461377463548
Trained batch 819 in epoch 0, gen_loss = 0.5429571535165717, disc_loss = 0.020621306558349737
Trained batch 820 in epoch 0, gen_loss = 0.5428753596856446, disc_loss = 0.02059841660406082
Trained batch 821 in epoch 0, gen_loss = 0.5427665082497608, disc_loss = 0.020575578887029176
Trained batch 822 in epoch 0, gen_loss = 0.5426438396495573, disc_loss = 0.020552634278947164
Trained batch 823 in epoch 0, gen_loss = 0.5425203831378117, disc_loss = 0.020529126385775014
Trained batch 824 in epoch 0, gen_loss = 0.5424545112884406, disc_loss = 0.02050580673780518
Trained batch 825 in epoch 0, gen_loss = 0.5423453090482416, disc_loss = 0.020484769921924596
Trained batch 826 in epoch 0, gen_loss = 0.5424991603557836, disc_loss = 0.02046578251286562
Trained batch 827 in epoch 0, gen_loss = 0.5425256128256447, disc_loss = 0.02044347649035809
Trained batch 828 in epoch 0, gen_loss = 0.5424090673106715, disc_loss = 0.02041999805666421
Trained batch 829 in epoch 0, gen_loss = 0.5423440418688648, disc_loss = 0.020396709470891852
Trained batch 830 in epoch 0, gen_loss = 0.54234328206122, disc_loss = 0.020373530605349525
Trained batch 831 in epoch 0, gen_loss = 0.5424235524394765, disc_loss = 0.02035076652436356
Trained batch 832 in epoch 0, gen_loss = 0.5423161442182503, disc_loss = 0.02032832794382247
Trained batch 833 in epoch 0, gen_loss = 0.5421928705857526, disc_loss = 0.0203056058525923
Trained batch 834 in epoch 0, gen_loss = 0.5419995687678902, disc_loss = 0.02028293347147901
Trained batch 835 in epoch 0, gen_loss = 0.5419052518440776, disc_loss = 0.020260499155875206
Trained batch 836 in epoch 0, gen_loss = 0.5418654630403508, disc_loss = 0.020238030546377083
Trained batch 837 in epoch 0, gen_loss = 0.5419018718109495, disc_loss = 0.020216186008825097
Trained batch 838 in epoch 0, gen_loss = 0.5419093536677037, disc_loss = 0.02019340158340269
Trained batch 839 in epoch 0, gen_loss = 0.5418858229404404, disc_loss = 0.02017095635639548
Trained batch 840 in epoch 0, gen_loss = 0.5417584185793056, disc_loss = 0.020148196313297652
Trained batch 841 in epoch 0, gen_loss = 0.5415906894093738, disc_loss = 0.020125857290160355
Trained batch 842 in epoch 0, gen_loss = 0.5414072253243072, disc_loss = 0.020104160762285925
Trained batch 843 in epoch 0, gen_loss = 0.5412596740287627, disc_loss = 0.02008226271762879
Trained batch 844 in epoch 0, gen_loss = 0.5411875094887773, disc_loss = 0.0200602650080295
Trained batch 845 in epoch 0, gen_loss = 0.5411957503774205, disc_loss = 0.020038173033775492
Trained batch 846 in epoch 0, gen_loss = 0.5411470687938271, disc_loss = 0.02001698852784452
Trained batch 847 in epoch 0, gen_loss = 0.5411387066233833, disc_loss = 0.019997203612907674
Trained batch 848 in epoch 0, gen_loss = 0.5411948881806418, disc_loss = 0.019976578536443412
Trained batch 849 in epoch 0, gen_loss = 0.5410181974663454, disc_loss = 0.01995530097745359
Trained batch 850 in epoch 0, gen_loss = 0.5408975232642069, disc_loss = 0.019933099191981673
Trained batch 851 in epoch 0, gen_loss = 0.5407363399513451, disc_loss = 0.01991057428611965
Trained batch 852 in epoch 0, gen_loss = 0.5406990485202525, disc_loss = 0.019888275529431006
Trained batch 853 in epoch 0, gen_loss = 0.5406462215846819, disc_loss = 0.0198665385374301
Trained batch 854 in epoch 0, gen_loss = 0.5407153900603802, disc_loss = 0.019845896675210577
Trained batch 855 in epoch 0, gen_loss = 0.5407295177612349, disc_loss = 0.019825054811807154
Trained batch 856 in epoch 0, gen_loss = 0.5405916791743965, disc_loss = 0.01980395518239781
Trained batch 857 in epoch 0, gen_loss = 0.5405572293313233, disc_loss = 0.019782242596994473
Trained batch 858 in epoch 0, gen_loss = 0.5403409898974981, disc_loss = 0.019760832412570078
Trained batch 859 in epoch 0, gen_loss = 0.5403048548934072, disc_loss = 0.019740909664840222
Trained batch 860 in epoch 0, gen_loss = 0.5402213369650847, disc_loss = 0.0197214867242149
Trained batch 861 in epoch 0, gen_loss = 0.5402515410962072, disc_loss = 0.01970191891236272
Trained batch 862 in epoch 0, gen_loss = 0.5402097322849658, disc_loss = 0.01968190183833097
Trained batch 863 in epoch 0, gen_loss = 0.5401242368475154, disc_loss = 0.019660618558460824
Trained batch 864 in epoch 0, gen_loss = 0.5400125480800695, disc_loss = 0.019639437770231416
Trained batch 865 in epoch 0, gen_loss = 0.5399543646094981, disc_loss = 0.01961883774082882
Trained batch 866 in epoch 0, gen_loss = 0.5399455937203514, disc_loss = 0.019598627990611742
Trained batch 867 in epoch 0, gen_loss = 0.5400192460500151, disc_loss = 0.019578020851713097
Trained batch 868 in epoch 0, gen_loss = 0.5398960058310501, disc_loss = 0.01955700597330077
Trained batch 869 in epoch 0, gen_loss = 0.5398065881825042, disc_loss = 0.019535830733834646
Trained batch 870 in epoch 0, gen_loss = 0.5397270780001113, disc_loss = 0.019514695990573504
Trained batch 871 in epoch 0, gen_loss = 0.5396653142407399, disc_loss = 0.019493515998615835
Trained batch 872 in epoch 0, gen_loss = 0.5395484395999406, disc_loss = 0.019472487281166757
Trained batch 873 in epoch 0, gen_loss = 0.5395235712522773, disc_loss = 0.019451496892278902
Trained batch 874 in epoch 0, gen_loss = 0.5395412445068359, disc_loss = 0.019430703686483735
Trained batch 875 in epoch 0, gen_loss = 0.5395246307327323, disc_loss = 0.019409818435446615
Trained batch 876 in epoch 0, gen_loss = 0.5394784773497109, disc_loss = 0.01938881942286329
Trained batch 877 in epoch 0, gen_loss = 0.5395485957689872, disc_loss = 0.019368061544901345
Trained batch 878 in epoch 0, gen_loss = 0.5394790230554118, disc_loss = 0.019347376934376114
Trained batch 879 in epoch 0, gen_loss = 0.5394716765054247, disc_loss = 0.01932718867749579
Trained batch 880 in epoch 0, gen_loss = 0.5393117945662421, disc_loss = 0.01930643854247592
Trained batch 881 in epoch 0, gen_loss = 0.5392756216109745, disc_loss = 0.019285561034786926
Trained batch 882 in epoch 0, gen_loss = 0.5392116221134147, disc_loss = 0.019264747957355687
Trained batch 883 in epoch 0, gen_loss = 0.5390997114022393, disc_loss = 0.019243896011686124
Trained batch 884 in epoch 0, gen_loss = 0.5389858672847856, disc_loss = 0.0192230455360328
Trained batch 885 in epoch 0, gen_loss = 0.5388956258985851, disc_loss = 0.01920295321883457
Trained batch 886 in epoch 0, gen_loss = 0.5388740081394673, disc_loss = 0.019183587661016157
Trained batch 887 in epoch 0, gen_loss = 0.5389316422311036, disc_loss = 0.019164560136289824
Trained batch 888 in epoch 0, gen_loss = 0.5387924074187992, disc_loss = 0.0191447614296418
Trained batch 889 in epoch 0, gen_loss = 0.5388110963146339, disc_loss = 0.01912479975752616
Trained batch 890 in epoch 0, gen_loss = 0.5387990320170367, disc_loss = 0.019104513061268404
Trained batch 891 in epoch 0, gen_loss = 0.5386045683058388, disc_loss = 0.019084340234959474
Trained batch 892 in epoch 0, gen_loss = 0.5385110889210012, disc_loss = 0.019064345117293355
Trained batch 893 in epoch 0, gen_loss = 0.5384739659709952, disc_loss = 0.019044600883175847
Trained batch 894 in epoch 0, gen_loss = 0.5386084624841893, disc_loss = 0.0190256346389131
Trained batch 895 in epoch 0, gen_loss = 0.5385387535539589, disc_loss = 0.019007614897418534
Trained batch 896 in epoch 0, gen_loss = 0.5383969588223907, disc_loss = 0.018991553011260005
Trained batch 897 in epoch 0, gen_loss = 0.5383948206171426, disc_loss = 0.01897486025583671
Trained batch 898 in epoch 0, gen_loss = 0.5382694963021857, disc_loss = 0.018955687048909223
Trained batch 899 in epoch 0, gen_loss = 0.5382870928115315, disc_loss = 0.01893609849503264
Trained batch 900 in epoch 0, gen_loss = 0.5382912575404731, disc_loss = 0.0189178686136262
Trained batch 901 in epoch 0, gen_loss = 0.5382447609153397, disc_loss = 0.01889967217322794
Trained batch 902 in epoch 0, gen_loss = 0.5382023878139779, disc_loss = 0.01888115010827631
Trained batch 903 in epoch 0, gen_loss = 0.5381982103649494, disc_loss = 0.0188616549230976
Trained batch 904 in epoch 0, gen_loss = 0.5380364392673113, disc_loss = 0.018842236945426184
Trained batch 905 in epoch 0, gen_loss = 0.5380161993090417, disc_loss = 0.018823105260826416
Trained batch 906 in epoch 0, gen_loss = 0.5378700713929951, disc_loss = 0.018803196008686313
Trained batch 907 in epoch 0, gen_loss = 0.5378699247860699, disc_loss = 0.01878419873300306
Trained batch 908 in epoch 0, gen_loss = 0.5377175637466548, disc_loss = 0.018766317166358834
Trained batch 909 in epoch 0, gen_loss = 0.5376412129009163, disc_loss = 0.018748052645625627
Trained batch 910 in epoch 0, gen_loss = 0.537556687314738, disc_loss = 0.018729162422066573
Trained batch 911 in epoch 0, gen_loss = 0.5374444688490608, disc_loss = 0.01870992125272128
Trained batch 912 in epoch 0, gen_loss = 0.5375399303592961, disc_loss = 0.01869284627883392
Trained batch 913 in epoch 0, gen_loss = 0.5375427998912152, disc_loss = 0.018677258171603244
Trained batch 914 in epoch 0, gen_loss = 0.5373786767975228, disc_loss = 0.018661529486273705
Trained batch 915 in epoch 0, gen_loss = 0.5374171802180303, disc_loss = 0.01864397043902905
Testing Epoch 0
Training Epoch 1
Traceback (most recent call last):
  File "esrgan_bones.py", line 318, in <module>
    gen_hr = generator(imgs_lr)
  File "/work3/soeba/HALOS/halos/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "esrgan_bones.py", line 228, in forward
    trunk = self.trunk_conv(self.RRDB_trunk(fea))
  File "/work3/soeba/HALOS/halos/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/work3/soeba/HALOS/halos/lib/python3.8/site-packages/torch/nn/modules/container.py", line 139, in forward
    input = module(input)
  File "/work3/soeba/HALOS/halos/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "esrgan_bones.py", line 206, in forward
    out = self.RDB3(out)
  File "/work3/soeba/HALOS/halos/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "esrgan_bones.py", line 189, in forward
    x4 = self.lrelu(self.conv4(torch.cat((x, x1, x2, x3), 1)))
  File "/work3/soeba/HALOS/halos/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/work3/soeba/HALOS/halos/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 457, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/work3/soeba/HALOS/halos/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 453, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 31.75 GiB total capacity; 28.70 GiB already allocated; 3.69 MiB free; 30.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF